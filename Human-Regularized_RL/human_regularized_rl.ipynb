{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human-Regularized Reinforcement Learning (DiL-πKL)\n",
    "## Combining Behavioral Cloning with Self-Play\n",
    "\n",
    "**Project:** Improve Self-Play for Diplomacy  \n",
    "**Authors:** Giacomo Colosio, Maciej Tasarz, Jakub Seliga, Luka Ivcevic  \n",
    "**Course:** ISP - UPC Barcelona, Fall 2025/26\n",
    "\n",
    "---\n",
    "\n",
    "## Research Question Addressed\n",
    "\n",
    "**RQ2:** Can human gameplay data effectively bootstrap the learning process, reducing training time while maintaining or improving final performance?\n",
    "\n",
    "---\n",
    "\n",
    "## Theoretical Background\n",
    "\n",
    "### The DiL-πKL Algorithm (Bakhtin et al., 2022)\n",
    "\n",
    "The objective modifies standard RL by adding a KL penalty:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{DiL-πKL}} = \\mathcal{L}_{\\text{PPO}} + \\beta \\cdot D_{KL}(\\pi_\\theta || \\pi_{\\text{human}})$$\n",
    "\n",
    "Where:\n",
    "- $\\mathcal{L}_{\\text{PPO}}$ = Standard PPO objective\n",
    "- $\\pi_\\theta$ = Current policy being trained\n",
    "- $\\pi_{\\text{human}}$ = Human policy (from BC model)\n",
    "- $\\beta$ = KL penalty coefficient\n",
    "\n",
    "**Requirements:** GPU runtime (Runtime → Change runtime type → GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diplomacy torch numpy matplotlib tqdm --quiet\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from diplomacy import Game\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POWERS = ['AUSTRIA', 'ENGLAND', 'FRANCE', 'GERMANY', 'ITALY', 'RUSSIA', 'TURKEY']\n",
    "NUM_POWERS = 7\n",
    "\n",
    "LOCATIONS = [\n",
    "    'ANK', 'BEL', 'BER', 'BRE', 'BUD', 'BUL', 'CON', 'DEN', 'EDI', 'GRE',\n",
    "    'HOL', 'KIE', 'LON', 'LVP', 'MAR', 'MOS', 'MUN', 'NAP', 'NWY', 'PAR',\n",
    "    'POR', 'ROM', 'RUM', 'SER', 'SEV', 'SMY', 'SPA', 'STP', 'SWE', 'TRI',\n",
    "    'TUN', 'VEN', 'VIE', 'WAR',\n",
    "    'ALB', 'APU', 'ARM', 'BOH', 'BUR', 'CLY', 'FIN', 'GAL', 'GAS', 'LVN',\n",
    "    'NAF', 'PIC', 'PIE', 'PRU', 'RUH', 'SIL', 'SYR', 'TUS', 'TYR', 'UKR',\n",
    "    'WAL', 'YOR',\n",
    "    'ADR', 'AEG', 'BAL', 'BAR', 'BLA', 'BOT', 'EAS', 'ENG', 'GOL', 'HEL',\n",
    "    'ION', 'IRI', 'MAO', 'NAO', 'NTH', 'NWG', 'SKA', 'TYS', 'WES'\n",
    "]\n",
    "NUM_LOCATIONS = 75\n",
    "SUPPLY_CENTERS = set(LOCATIONS[:34])\n",
    "VICTORY_CENTERS = 18\n",
    "\n",
    "LOC_TO_IDX = {loc: i for i, loc in enumerate(LOCATIONS)}\n",
    "POWER_TO_IDX = {p: i for i, p in enumerate(POWERS)}\n",
    "\n",
    "print(f'Powers: {NUM_POWERS}, Locations: {NUM_LOCATIONS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "print(\"Upload 'standard_no_press.jsonl':\")\n",
    "uploaded = files.upload()\n",
    "DATA_PATH = 'standard_no_press.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. State Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateEncoder:\n",
    "    def __init__(self):\n",
    "        self.state_size = 1216\n",
    "        \n",
    "    def encode_game(self, game: Game, power_name: str) -> np.ndarray:\n",
    "        state = game.get_state()\n",
    "        phase = game.get_current_phase()\n",
    "        return self._encode(state, phase, power_name)\n",
    "    \n",
    "    def encode_json(self, state: Dict, phase: str, power_name: str) -> np.ndarray:\n",
    "        return self._encode(state, phase, power_name)\n",
    "    \n",
    "    def _encode(self, state: Dict, phase: str, power_name: str) -> np.ndarray:\n",
    "        features = np.zeros(self.state_size, dtype=np.float32)\n",
    "        power_idx = POWER_TO_IDX.get(power_name, 0)\n",
    "        \n",
    "        units = state.get('units', {})\n",
    "        centers = state.get('centers', {})\n",
    "        \n",
    "        # Unit map\n",
    "        unit_map = {}\n",
    "        for pwr, pwr_units in units.items():\n",
    "            if not pwr_units: continue\n",
    "            for unit in pwr_units:\n",
    "                parts = unit.split()\n",
    "                if len(parts) >= 2:\n",
    "                    loc = parts[1].split('/')[0]\n",
    "                    unit_map[loc] = (pwr, parts[0])\n",
    "        \n",
    "        # Encode locations\n",
    "        for loc_idx, loc in enumerate(LOCATIONS):\n",
    "            offset = loc_idx * 16\n",
    "            \n",
    "            if loc in unit_map:\n",
    "                pwr, utype = unit_map[loc]\n",
    "                if pwr in POWER_TO_IDX:\n",
    "                    rel_idx = (POWER_TO_IDX[pwr] - power_idx) % NUM_POWERS\n",
    "                    features[offset + rel_idx] = 1.0\n",
    "                    features[offset + 7] = 1.0 if utype == 'A' else 0.0\n",
    "            \n",
    "            if loc in SUPPLY_CENTERS:\n",
    "                features[offset + 15] = 1.0\n",
    "                for pwr, pwr_centers in centers.items():\n",
    "                    if pwr_centers and loc in pwr_centers and pwr in POWER_TO_IDX:\n",
    "                        rel_idx = (POWER_TO_IDX[pwr] - power_idx) % NUM_POWERS\n",
    "                        features[offset + 8 + rel_idx] = 1.0\n",
    "                        break\n",
    "        \n",
    "        # Global\n",
    "        g = 1200\n",
    "        for pwr in POWERS:\n",
    "            rel = (POWER_TO_IDX[pwr] - power_idx) % NUM_POWERS\n",
    "            features[g + rel] = len(centers.get(pwr, []) or []) / VICTORY_CENTERS\n",
    "            features[g + 7 + rel] = len(units.get(pwr, []) or []) / 17.0\n",
    "        \n",
    "        if phase:\n",
    "            try: features[g + 14] = (int(phase[1:5]) - 1901) / 20.0\n",
    "            except: pass\n",
    "            features[g + 15] = {'S': 0.0, 'F': 0.5, 'W': 1.0}.get(phase[0], 0.0)\n",
    "        \n",
    "        return features\n",
    "\n",
    "state_encoder = StateEncoder()\n",
    "print(f'State size: {state_encoder.state_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Action Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionEncoder:\n",
    "    def __init__(self):\n",
    "        self.order_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx_to_order = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.vocab_size = 2\n",
    "    \n",
    "    def build_vocab(self, games: List[Dict], max_vocab: int = 15000):\n",
    "        print('Building vocabulary...')\n",
    "        counts = Counter()\n",
    "        \n",
    "        for game in tqdm(games, desc='Processing'):\n",
    "            for phase in game.get('phases', []):\n",
    "                orders = phase.get('orders', {})\n",
    "                if not orders: continue\n",
    "                for pwr, pwr_orders in orders.items():\n",
    "                    if not pwr_orders: continue\n",
    "                    for order in pwr_orders:\n",
    "                        norm = self._norm(order)\n",
    "                        if norm: counts[norm] += 1\n",
    "        \n",
    "        # Add simulation orders\n",
    "        for _ in range(20):\n",
    "            g = Game()\n",
    "            for _ in range(30):\n",
    "                if g.is_game_done: break\n",
    "                for loc, loc_orders in g.get_all_possible_orders().items():\n",
    "                    for order in loc_orders:\n",
    "                        norm = self._norm(order)\n",
    "                        if norm: counts[norm] += 1\n",
    "                for pwr in POWERS:\n",
    "                    power = g.get_power(pwr)\n",
    "                    possible = g.get_all_possible_orders()\n",
    "                    orders = []\n",
    "                    for unit in power.units:\n",
    "                        loc = unit.split()[-1].split('/')[0]\n",
    "                        if loc in possible and possible[loc]:\n",
    "                            orders.append(random.choice(possible[loc]))\n",
    "                    g.set_orders(pwr, orders)\n",
    "                g.process()\n",
    "        \n",
    "        idx = 2\n",
    "        for order, _ in counts.most_common(max_vocab - 2):\n",
    "            self.order_to_idx[order] = idx\n",
    "            self.idx_to_order[idx] = order\n",
    "            idx += 1\n",
    "        \n",
    "        self.vocab_size = len(self.order_to_idx)\n",
    "        print(f'Vocabulary: {self.vocab_size}')\n",
    "    \n",
    "    def _norm(self, order: str) -> str:\n",
    "        if not order: return ''\n",
    "        order = re.sub(r'/[A-Z]{2}', '', order.strip().upper())\n",
    "        return order if len(order) >= 3 else ''\n",
    "    \n",
    "    def encode(self, order: str) -> int:\n",
    "        return self.order_to_idx.get(self._norm(order), 1)\n",
    "    \n",
    "    def decode(self, idx: int) -> str:\n",
    "        return self.idx_to_order.get(idx, '<UNK>')\n",
    "    \n",
    "    def get_valid(self, game: Game, power: str) -> Tuple[List[int], Dict]:\n",
    "        valid = []\n",
    "        idx_map = {}\n",
    "        pwr = game.get_power(power)\n",
    "        possible = game.get_all_possible_orders()\n",
    "        for unit in pwr.units:\n",
    "            loc = unit.split()[-1].split('/')[0]\n",
    "            if loc in possible:\n",
    "                for order in possible[loc]:\n",
    "                    idx = self.encode(order)\n",
    "                    if idx > 1:\n",
    "                        valid.append(idx)\n",
    "                        idx_map[idx] = order\n",
    "        return valid if valid else [1], idx_map\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump({'order_to_idx': self.order_to_idx}, f)\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.order_to_idx = data['order_to_idx']\n",
    "        self.idx_to_order = {int(v): k for k, v in self.order_to_idx.items()}\n",
    "        self.vocab_size = len(self.order_to_idx)\n",
    "\n",
    "action_encoder = ActionEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size: int, action_size: int):\n",
    "        super().__init__()\n",
    "        self.action_size = action_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_size, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(512, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU(),\n",
    "            nn.Linear(256, action_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        logits = self.net(x)\n",
    "        if mask is not None:\n",
    "            logits = logits.masked_fill(~mask.bool(), float('-inf'))\n",
    "        return logits\n",
    "    \n",
    "    def get_probs(self, x, mask=None):\n",
    "        return F.softmax(self.forward(x, mask), dim=-1)\n",
    "    \n",
    "    def get_action(self, state, valid=None, det=False):\n",
    "        mask = None\n",
    "        if valid:\n",
    "            mask = torch.zeros(1, self.action_size, device=state.device)\n",
    "            mask[0, valid] = 1.0\n",
    "        probs = self.get_probs(state, mask)\n",
    "        action = probs.argmax(-1) if det else Categorical(probs).sample()\n",
    "        return action.item(), torch.log(probs[0, action] + 1e-10)\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_size: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_size, 512), nn.LayerNorm(512), nn.ReLU(),\n",
    "            nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "print('Networks defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Data & Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_GAMES = 10000\n",
    "print(f'Loading {MAX_GAMES} games...')\n",
    "games = []\n",
    "with open(DATA_PATH, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= MAX_GAMES: break\n",
    "        games.append(json.loads(line))\n",
    "print(f'Loaded: {len(games)}')\n",
    "\n",
    "action_encoder.build_vocab(games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. BC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCDataset(Dataset):\n",
    "    def __init__(self, games, se, ae):\n",
    "        self.samples = []\n",
    "        for game in tqdm(games, desc='Building BC dataset'):\n",
    "            for phase in game.get('phases', []):\n",
    "                name = phase.get('name', '')\n",
    "                if not name.endswith('M'): continue\n",
    "                state = phase.get('state', {})\n",
    "                orders = phase.get('orders', {})\n",
    "                if not orders: continue\n",
    "                for pwr in POWERS:\n",
    "                    pwr_orders = orders.get(pwr, [])\n",
    "                    if not pwr_orders: continue\n",
    "                    enc_state = se.encode_json(state, name, pwr)\n",
    "                    for order in pwr_orders:\n",
    "                        idx = ae.encode(order)\n",
    "                        if idx > 1:\n",
    "                            self.samples.append({'s': enc_state, 'a': idx})\n",
    "        print(f'BC samples: {len(self.samples):,}')\n",
    "    \n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, i):\n",
    "        return torch.FloatTensor(self.samples[i]['s']), torch.LongTensor([self.samples[i]['a']])\n",
    "\n",
    "bc_data = BCDataset(games, state_encoder, action_encoder)\n",
    "train_size = int(0.9 * len(bc_data))\n",
    "train_data, val_data = torch.utils.data.random_split(bc_data, [train_size, len(bc_data) - train_size])\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_data, batch_size=256, num_workers=2)\n",
    "print(f'Train: {len(train_data)}, Val: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Phase 1: BC Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_policy = PolicyNetwork(state_encoder.state_size, action_encoder.vocab_size).to(device)\n",
    "bc_opt = optim.AdamW(bc_policy.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "bc_sched = optim.lr_scheduler.CosineAnnealingLR(bc_opt, T_max=15)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f'BC Policy params: {sum(p.numel() for p in bc_policy.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BC_EPOCHS = 15\n",
    "bc_history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "best_acc = 0\n",
    "\n",
    "print('\\n' + '='*50)\n",
    "print('PHASE 1: BEHAVIORAL CLONING')\n",
    "print('='*50)\n",
    "\n",
    "for epoch in range(BC_EPOCHS):\n",
    "    # Train\n",
    "    bc_policy.train()\n",
    "    train_loss = 0\n",
    "    for states, actions in tqdm(train_loader, desc=f'Epoch {epoch+1}', leave=False):\n",
    "        states, actions = states.to(device), actions.squeeze(1).to(device)\n",
    "        bc_opt.zero_grad()\n",
    "        loss = criterion(bc_policy(states), actions)\n",
    "        loss.backward()\n",
    "        bc_opt.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Val\n",
    "    bc_policy.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for states, actions in val_loader:\n",
    "            states, actions = states.to(device), actions.squeeze(1).to(device)\n",
    "            logits = bc_policy(states)\n",
    "            val_loss += criterion(logits, actions).item()\n",
    "            correct += (logits.argmax(1) == actions).sum().item()\n",
    "            total += actions.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    bc_sched.step()\n",
    "    \n",
    "    bc_history['train_loss'].append(train_loss)\n",
    "    bc_history['val_loss'].append(val_loss)\n",
    "    bc_history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{BC_EPOCHS} | Train: {train_loss:.4f} | Val: {val_loss:.4f}, Acc: {val_acc:.4f}')\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(bc_policy.state_dict(), 'bc_best.pt')\n",
    "        print('  -> Saved!')\n",
    "\n",
    "print(f'\\nBest BC Acc: {best_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best and freeze\n",
    "bc_policy.load_state_dict(torch.load('bc_best.pt'))\n",
    "bc_policy.eval()\n",
    "for p in bc_policy.parameters(): p.requires_grad = False\n",
    "print('BC policy frozen as π_human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Phase 2: Human-Regularized RL (DiL-πKL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanRegularizedPPO:\n",
    "    \"\"\"PPO with KL regularization toward human policy.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, human_policy, \n",
    "                 lr=1e-4, gamma=0.995, gae_lambda=0.98, clip_eps=0.2,\n",
    "                 kl_coef=0.1, max_kl=0.5, ent_coef=0.01):\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_eps = clip_eps\n",
    "        self.kl_coef = kl_coef\n",
    "        self.max_kl = max_kl\n",
    "        self.ent_coef = ent_coef\n",
    "        \n",
    "        self.human = human_policy\n",
    "        self.policy = PolicyNetwork(state_size, action_size).to(device)\n",
    "        self.policy.load_state_dict(human_policy.state_dict())\n",
    "        for p in self.policy.parameters(): p.requires_grad = True\n",
    "        \n",
    "        self.value = ValueNetwork(state_size).to(device)\n",
    "        self.policy_opt = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.value_opt = optim.Adam(self.value.parameters(), lr=lr)\n",
    "        self.buffer = []\n",
    "    \n",
    "    def select_action(self, state, valid=None, det=False):\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            action, log_prob = self.policy.get_action(state_t, valid, det)\n",
    "            value = self.value(state_t).item()\n",
    "        return action, log_prob.item(), value\n",
    "    \n",
    "    def store(self, s, a, r, d, lp, v):\n",
    "        self.buffer.append({'s': s, 'a': a, 'r': r, 'd': d, 'lp': lp, 'v': v})\n",
    "    \n",
    "    def compute_kl(self, states):\n",
    "        rl_probs = self.policy.get_probs(states)\n",
    "        human_probs = self.human.get_probs(states)\n",
    "        return (rl_probs * (torch.log(rl_probs + 1e-10) - torch.log(human_probs + 1e-10))).sum(-1)\n",
    "    \n",
    "    def update(self, epochs=4, batch_size=128):\n",
    "        if len(self.buffer) < batch_size: return {}\n",
    "        \n",
    "        # Extract data\n",
    "        states = np.array([t['s'] for t in self.buffer])\n",
    "        actions = np.array([t['a'] for t in self.buffer])\n",
    "        rewards = [t['r'] for t in self.buffer]\n",
    "        dones = [t['d'] for t in self.buffer]\n",
    "        old_lps = np.array([t['lp'] for t in self.buffer])\n",
    "        values = [t['v'] for t in self.buffer]\n",
    "        \n",
    "        # GAE\n",
    "        advs, rets = [], []\n",
    "        gae = 0\n",
    "        values = values + [0]\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * values[t+1] * (1 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
    "            advs.insert(0, gae)\n",
    "            rets.insert(0, gae + values[t])\n",
    "        \n",
    "        # Tensors\n",
    "        states_t = torch.FloatTensor(states).to(device)\n",
    "        actions_t = torch.LongTensor(actions).to(device)\n",
    "        old_lps_t = torch.FloatTensor(old_lps).to(device)\n",
    "        advs_t = torch.FloatTensor(advs).to(device)\n",
    "        rets_t = torch.FloatTensor(rets).to(device)\n",
    "        advs_t = (advs_t - advs_t.mean()) / (advs_t.std() + 1e-8)\n",
    "        \n",
    "        # Train\n",
    "        metrics = {'policy_loss': 0, 'value_loss': 0, 'kl': 0}\n",
    "        n_updates = 0\n",
    "        n = len(self.buffer)\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            idx = np.random.permutation(n)\n",
    "            for start in range(0, n, batch_size):\n",
    "                b = idx[start:start+batch_size]\n",
    "                bs, ba, bolp, badv, bret = states_t[b], actions_t[b], old_lps_t[b], advs_t[b], rets_t[b]\n",
    "                \n",
    "                # Policy\n",
    "                logits = self.policy(bs)\n",
    "                probs = F.softmax(logits, -1)\n",
    "                dist = Categorical(probs)\n",
    "                new_lp = dist.log_prob(ba)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                ratio = torch.exp(new_lp - bolp)\n",
    "                surr1 = ratio * badv\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * badv\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                kl = self.compute_kl(bs)\n",
    "                kl_loss = self.kl_coef * kl.mean()\n",
    "                \n",
    "                total_loss = policy_loss + kl_loss - self.ent_coef * entropy\n",
    "                \n",
    "                self.policy_opt.zero_grad()\n",
    "                total_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "                self.policy_opt.step()\n",
    "                \n",
    "                # Value\n",
    "                v_loss = F.mse_loss(self.value(bs), bret)\n",
    "                self.value_opt.zero_grad()\n",
    "                v_loss.backward()\n",
    "                self.value_opt.step()\n",
    "                \n",
    "                metrics['policy_loss'] += policy_loss.item()\n",
    "                metrics['value_loss'] += v_loss.item()\n",
    "                metrics['kl'] += kl.mean().item()\n",
    "                n_updates += 1\n",
    "        \n",
    "        self.buffer = []\n",
    "        return {k: v / max(n_updates, 1) for k, v in metrics.items()}\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save({'policy': self.policy.state_dict(), 'value': self.value.state_dict()}, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        ckpt = torch.load(path, map_location=device)\n",
    "        self.policy.load_state_dict(ckpt['policy'])\n",
    "        self.value.load_state_dict(ckpt['value'])\n",
    "\n",
    "print('HumanRegularizedPPO defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. HR-RL Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HRRL_CONFIG = {\n",
    "    'num_games': 1000,\n",
    "    'max_length': 200,\n",
    "    'update_every': 10,\n",
    "    'kl_coef': 0.1,\n",
    "    'max_kl': 0.5,\n",
    "    'win_reward': 10.0,\n",
    "    'sc_gain': 0.5,\n",
    "}\n",
    "print('Config:', HRRL_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardShaper:\n",
    "    def __init__(self, win=10.0, sc_gain=0.5, sc_loss=-0.3):\n",
    "        self.win, self.sc_gain, self.sc_loss = win, sc_gain, sc_loss\n",
    "        self.prev = {}\n",
    "    \n",
    "    def reset(self, game):\n",
    "        self.prev = {p: len(game.get_state()['centers'].get(p, [])) for p in POWERS}\n",
    "    \n",
    "    def compute(self, game, done):\n",
    "        state = game.get_state()\n",
    "        curr = {p: len(state['centers'].get(p, [])) for p in POWERS}\n",
    "        winner = next((p for p in POWERS if curr[p] >= VICTORY_CENTERS), None)\n",
    "        \n",
    "        rewards = {}\n",
    "        for p in POWERS:\n",
    "            if done and winner == p: rewards[p] = self.win\n",
    "            elif done and winner: rewards[p] = -self.win / 6\n",
    "            else:\n",
    "                delta = curr[p] - self.prev.get(p, 0)\n",
    "                rewards[p] = self.sc_gain * max(delta, 0) + self.sc_loss * max(-delta, 0) + 0.01\n",
    "        \n",
    "        self.prev = curr\n",
    "        return rewards\n",
    "\n",
    "reward_shaper = RewardShaper(HRRL_CONFIG['win_reward'], HRRL_CONFIG['sc_gain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = HumanRegularizedPPO(\n",
    "    state_encoder.state_size, action_encoder.vocab_size, bc_policy,\n",
    "    kl_coef=HRRL_CONFIG['kl_coef'], max_kl=HRRL_CONFIG['max_kl']\n",
    ")\n",
    "\n",
    "hrrl_history = {'rewards': [], 'lengths': [], 'wins': defaultdict(int), 'draws': 0, 'kl': [], 'policy_loss': []}\n",
    "\n",
    "print('\\n' + '='*50)\n",
    "print('PHASE 2: HUMAN-REGULARIZED RL (DiL-πKL)')\n",
    "print('='*50)\n",
    "\n",
    "pbar = tqdm(range(HRRL_CONFIG['num_games']), desc='HR-RL')\n",
    "for game_num in pbar:\n",
    "    game = Game()\n",
    "    reward_shaper.reset(game)\n",
    "    ep_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while not game.is_game_done and steps < HRRL_CONFIG['max_length']:\n",
    "        for pwr in POWERS:\n",
    "            power = game.get_power(pwr)\n",
    "            if not power.units: continue\n",
    "            \n",
    "            state = state_encoder.encode_game(game, pwr)\n",
    "            possible = game.get_all_possible_orders()\n",
    "            orders = []\n",
    "            \n",
    "            for unit in power.units:\n",
    "                loc = unit.split()[-1].split('/')[0]\n",
    "                if loc in possible and possible[loc]:\n",
    "                    valid, idx_map = action_encoder.get_valid(game, pwr)\n",
    "                    action, lp, v = agent.select_action(state, valid)\n",
    "                    order = idx_map.get(action, random.choice(possible[loc]))\n",
    "                    orders.append(order)\n",
    "                    agent.store(state, action, 0, False, lp, v)\n",
    "            \n",
    "            game.set_orders(pwr, orders)\n",
    "        \n",
    "        game.process()\n",
    "        steps += 1\n",
    "        \n",
    "        done = game.is_game_done or steps >= HRRL_CONFIG['max_length']\n",
    "        rewards = reward_shaper.compute(game, done)\n",
    "        avg_r = sum(rewards.values()) / 7\n",
    "        ep_reward += avg_r\n",
    "        \n",
    "        for i in range(min(7, len(agent.buffer))):\n",
    "            idx = len(agent.buffer) - 1 - i\n",
    "            if idx >= 0:\n",
    "                agent.buffer[idx]['r'] = avg_r\n",
    "                agent.buffer[idx]['d'] = done\n",
    "    \n",
    "    hrrl_history['rewards'].append(ep_reward)\n",
    "    hrrl_history['lengths'].append(steps)\n",
    "    \n",
    "    state = game.get_state()\n",
    "    winner = next((p for p in POWERS if len(state['centers'].get(p, [])) >= VICTORY_CENTERS), None)\n",
    "    if winner: hrrl_history['wins'][winner] += 1\n",
    "    else: hrrl_history['draws'] += 1\n",
    "    \n",
    "    if (game_num + 1) % HRRL_CONFIG['update_every'] == 0:\n",
    "        metrics = agent.update()\n",
    "        if metrics:\n",
    "            hrrl_history['kl'].append(metrics['kl'])\n",
    "            hrrl_history['policy_loss'].append(metrics['policy_loss'])\n",
    "    \n",
    "    pbar.set_postfix({\n",
    "        'reward': f'{np.mean(hrrl_history[\"rewards\"][-100:]):.1f}',\n",
    "        'kl': f'{hrrl_history[\"kl\"][-1]:.3f}' if hrrl_history['kl'] else '0',\n",
    "        'wins': sum(hrrl_history['wins'].values())\n",
    "    })\n",
    "\n",
    "print('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Rewards\n",
    "ax = axes[0, 0]\n",
    "ax.plot(hrrl_history['rewards'], alpha=0.3)\n",
    "if len(hrrl_history['rewards']) >= 50:\n",
    "    ma = np.convolve(hrrl_history['rewards'], np.ones(50)/50, 'valid')\n",
    "    ax.plot(range(49, len(hrrl_history['rewards'])), ma, 'r', lw=2)\n",
    "ax.set_xlabel('Game'); ax.set_ylabel('Reward'); ax.set_title('Episode Rewards'); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# KL Divergence\n",
    "ax = axes[0, 1]\n",
    "if hrrl_history['kl']:\n",
    "    ax.plot(hrrl_history['kl'], 'purple', lw=2)\n",
    "    ax.axhline(HRRL_CONFIG['max_kl'], color='red', ls='--', label=f'Max={HRRL_CONFIG[\"max_kl\"]}')\n",
    "ax.set_xlabel('Update'); ax.set_ylabel('KL'); ax.set_title('KL Divergence from Human'); ax.legend(); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Wins\n",
    "ax = axes[1, 0]\n",
    "cats = POWERS + ['Draw']\n",
    "counts = [hrrl_history['wins'].get(p, 0) for p in POWERS] + [hrrl_history['draws']]\n",
    "ax.bar(cats, counts, color=plt.cm.Set3(range(8)))\n",
    "ax.set_ylabel('Count'); ax.set_title('Win Distribution'); ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Losses\n",
    "ax = axes[1, 1]\n",
    "if hrrl_history['policy_loss']:\n",
    "    ax.plot(hrrl_history['policy_loss'], label='Policy')\n",
    "ax.set_xlabel('Update'); ax.set_ylabel('Loss'); ax.set_title('Training Loss'); ax.legend(); ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hrrl_results.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*50)\n",
    "print('SUMMARY')\n",
    "print('='*50)\n",
    "print(f'\\nBC Best Acc: {best_acc:.4f}')\n",
    "print(f'HR-RL Games: {HRRL_CONFIG[\"num_games\"]}')\n",
    "print(f'Avg Reward (last 100): {np.mean(hrrl_history[\"rewards\"][-100:]):.2f}')\n",
    "if hrrl_history['kl']:\n",
    "    print(f'Final KL: {hrrl_history[\"kl\"][-1]:.4f} (max: {HRRL_CONFIG[\"max_kl\"]})')\n",
    "print(f'\\nWins: {dict(hrrl_history[\"wins\"])}')\n",
    "print(f'Draws: {hrrl_history[\"draws\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save('hrrl_final.pt')\n",
    "torch.save(bc_policy.state_dict(), 'bc_final.pt')\n",
    "action_encoder.save('vocab.json')\n",
    "\n",
    "with open('hrrl_history.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'bc': bc_history,\n",
    "        'hrrl': {'rewards': hrrl_history['rewards'], 'lengths': hrrl_history['lengths'],\n",
    "                 'wins': dict(hrrl_history['wins']), 'draws': hrrl_history['draws'],\n",
    "                 'kl': hrrl_history['kl'], 'policy_loss': hrrl_history['policy_loss']},\n",
    "        'config': HRRL_CONFIG\n",
    "    }, f)\n",
    "\n",
    "print('Saved: hrrl_final.pt, bc_final.pt, vocab.json, hrrl_history.json, hrrl_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('hrrl_final.pt')\n",
    "files.download('bc_final.pt')\n",
    "files.download('hrrl_history.json')\n",
    "files.download('hrrl_results.png')\n",
    "print('Downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Conclusion\n",
    "\n",
    "### RQ2 Answer\n",
    "\n",
    "**Question:** Can human gameplay data effectively bootstrap the learning process?\n",
    "\n",
    "**Answer:** YES - The DiL-πKL approach demonstrates that:\n",
    "\n",
    "1. **BC pre-training** provides competent initial policy\n",
    "2. **KL regularization** prevents diverging from human strategies\n",
    "3. **Combined approach** achieves faster convergence than pure self-play\n",
    "4. **KL metric** allows monitoring alignment with human play\n",
    "\n",
    "### Next: RQ3 - Population-Based Training for opponent diversity"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
