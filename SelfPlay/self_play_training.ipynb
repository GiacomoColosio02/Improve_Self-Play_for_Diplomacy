{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Play Reinforcement Learning for No-Press Diplomacy\n",
    "\n",
    "**Project:** Improve Self-Play for Diplomacy  \n",
    "**Authors:** Giacomo Colosio, Maciej Tasarz, Jakub Seliga, Luka Ivcevic  \n",
    "**Course:** ISP - UPC Barcelona, Fall 2025/26\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements **Self-Play Reinforcement Learning** for No-Press Diplomacy following:\n",
    "\n",
    "1. **Silver et al. (2017)** - AlphaGo Zero: Tabula rasa self-play\n",
    "2. **Bakhtin et al. (2021)** - DORA: Double Oracle RL for Diplomacy\n",
    "3. **Bakhtin et al. (2022)** - Diplodocus: Human-regularized self-play\n",
    "\n",
    "### Architecture\n",
    "- **Policy Network**: Predicts action probabilities π(a|s)\n",
    "- **Value Network**: Estimates state value V(s)\n",
    "- **Training**: PPO (Proximal Policy Optimization) with self-play\n",
    "\n",
    "### Research Questions Addressed\n",
    "- **RQ1**: Quantify overfitting in pure self-play\n",
    "- **RQ2**: Establish baseline for comparison with human-regularized RL\n",
    "\n",
    "**Requirements:** GPU runtime recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install diplomacy environment\n",
    "!pip install diplomacy torch numpy matplotlib tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "from collections import deque, defaultdict, namedtuple\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Game Constants & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diplomacy Constants\n",
    "POWERS = ['AUSTRIA', 'ENGLAND', 'FRANCE', 'GERMANY', 'ITALY', 'RUSSIA', 'TURKEY']\n",
    "NUM_POWERS = len(POWERS)\n",
    "\n",
    "# All 75 locations on standard map\n",
    "LOCATIONS = [\n",
    "    # Supply centers (34)\n",
    "    'ANK', 'BEL', 'BER', 'BRE', 'BUD', 'BUL', 'CON', 'DEN', 'EDI', 'GRE',\n",
    "    'HOL', 'KIE', 'LON', 'LVP', 'MAR', 'MOS', 'MUN', 'NAP', 'NWY', 'PAR',\n",
    "    'POR', 'ROM', 'RUM', 'SER', 'SEV', 'SMY', 'SPA', 'STP', 'SWE', 'TRI',\n",
    "    'TUN', 'VEN', 'VIE', 'WAR',\n",
    "    # Non-supply center land (22)\n",
    "    'ALB', 'APU', 'ARM', 'BOH', 'BUR', 'CLY', 'FIN', 'GAL', 'GAS', 'LVN',\n",
    "    'NAF', 'PIC', 'PIE', 'PRU', 'RUH', 'SIL', 'SYR', 'TUS', 'TYR', 'UKR',\n",
    "    'WAL', 'YOR',\n",
    "    # Sea zones (19)\n",
    "    'ADR', 'AEG', 'BAL', 'BAR', 'BLA', 'BOT', 'EAS', 'ENG', 'GOL', 'HEL',\n",
    "    'ION', 'IRI', 'MAO', 'NAO', 'NTH', 'NWG', 'SKA', 'TYS', 'WES'\n",
    "]\n",
    "NUM_LOCATIONS = len(LOCATIONS)\n",
    "SUPPLY_CENTERS = LOCATIONS[:34]\n",
    "VICTORY_THRESHOLD = 18  # SCs needed to win\n",
    "\n",
    "# Location indices\n",
    "LOC_TO_IDX = {loc: i for i, loc in enumerate(LOCATIONS)}\n",
    "IDX_TO_LOC = {i: loc for i, loc in enumerate(LOCATIONS)}\n",
    "\n",
    "# Starting positions\n",
    "STARTING_UNITS = {\n",
    "    'AUSTRIA': ['A VIE', 'A BUD', 'F TRI'],\n",
    "    'ENGLAND': ['F LON', 'F EDI', 'A LVP'],\n",
    "    'FRANCE': ['F BRE', 'A PAR', 'A MAR'],\n",
    "    'GERMANY': ['F KIE', 'A BER', 'A MUN'],\n",
    "    'ITALY': ['F NAP', 'A ROM', 'A VEN'],\n",
    "    'RUSSIA': ['F STP/SC', 'A MOS', 'A WAR', 'F SEV'],\n",
    "    'TURKEY': ['F ANK', 'A CON', 'A SMY']\n",
    "}\n",
    "\n",
    "STARTING_CENTERS = {\n",
    "    'AUSTRIA': ['VIE', 'BUD', 'TRI'],\n",
    "    'ENGLAND': ['LON', 'EDI', 'LVP'],\n",
    "    'FRANCE': ['BRE', 'PAR', 'MAR'],\n",
    "    'GERMANY': ['KIE', 'BER', 'MUN'],\n",
    "    'ITALY': ['NAP', 'ROM', 'VEN'],\n",
    "    'RUSSIA': ['STP', 'MOS', 'WAR', 'SEV'],\n",
    "    'TURKEY': ['ANK', 'CON', 'SMY']\n",
    "}\n",
    "\n",
    "print(f'Powers: {NUM_POWERS}')\n",
    "print(f'Locations: {NUM_LOCATIONS}')\n",
    "print(f'Supply Centers: {len(SUPPLY_CENTERS)}')\n",
    "print(f'Victory requires: {VICTORY_THRESHOLD} SCs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacency map (simplified - key connections)\n",
    "ADJACENCIES = {\n",
    "    'VIE': ['BOH', 'GAL', 'BUD', 'TRI', 'TYR'],\n",
    "    'BUD': ['VIE', 'GAL', 'RUM', 'SER', 'TRI'],\n",
    "    'TRI': ['VIE', 'BUD', 'SER', 'ALB', 'ADR', 'VEN', 'TYR'],\n",
    "    'LON': ['WAL', 'YOR', 'NTH', 'ENG'],\n",
    "    'EDI': ['YOR', 'NTH', 'NWG', 'CLY', 'LVP'],\n",
    "    'LVP': ['EDI', 'CLY', 'NAO', 'IRI', 'WAL', 'YOR'],\n",
    "    'BRE': ['PIC', 'PAR', 'GAS', 'MAO', 'ENG'],\n",
    "    'PAR': ['PIC', 'BUR', 'GAS', 'BRE'],\n",
    "    'MAR': ['SPA', 'GAS', 'BUR', 'PIE', 'GOL'],\n",
    "    'KIE': ['HOL', 'RUH', 'MUN', 'BER', 'BAL', 'HEL', 'DEN'],\n",
    "    'BER': ['KIE', 'MUN', 'SIL', 'PRU', 'BAL'],\n",
    "    'MUN': ['KIE', 'BER', 'SIL', 'BOH', 'TYR', 'BUR', 'RUH'],\n",
    "    'NAP': ['ROM', 'APU', 'ION', 'TYS'],\n",
    "    'ROM': ['NAP', 'APU', 'VEN', 'TUS', 'TYS'],\n",
    "    'VEN': ['ROM', 'APU', 'TRI', 'TYR', 'PIE', 'TUS', 'ADR'],\n",
    "    'MOS': ['STP', 'LVN', 'WAR', 'UKR', 'SEV'],\n",
    "    'WAR': ['MOS', 'LVN', 'PRU', 'SIL', 'GAL', 'UKR'],\n",
    "    'SEV': ['MOS', 'UKR', 'RUM', 'BLA', 'ARM'],\n",
    "    'STP': ['MOS', 'LVN', 'FIN', 'NWY', 'BAR', 'BOT'],\n",
    "    'ANK': ['CON', 'SMY', 'ARM', 'BLA'],\n",
    "    'CON': ['ANK', 'SMY', 'BUL', 'BLA', 'AEG'],\n",
    "    'SMY': ['ANK', 'CON', 'ARM', 'SYR', 'AEG', 'EAS'],\n",
    "    # Add more as needed...\n",
    "}\n",
    "\n",
    "# Fill missing adjacencies with empty lists\n",
    "for loc in LOCATIONS:\n",
    "    if loc not in ADJACENCIES:\n",
    "        ADJACENCIES[loc] = []\n",
    "\n",
    "print(f'Adjacencies defined for {len([k for k,v in ADJACENCIES.items() if v])} locations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. State Encoder (887-dimensional as per Bakhtin et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiplomacyStateEncoder:\n",
    "    \"\"\"\n",
    "    Encodes Diplomacy state into fixed-size vector.\n",
    "    Based on Bakhtin et al. (2022) encoding scheme.\n",
    "    \n",
    "    Features per location (75 locations):\n",
    "        - Unit presence: 7 powers × 2 unit types = 14\n",
    "        - SC ownership: 7 powers + neutral = 8\n",
    "        - Dislodged units: 7 powers × 2 = 14 (retreat phases)\n",
    "    Subtotal: 36 features × 75 locations = 2700 (we simplify to ~1200)\n",
    "    \n",
    "    Global features:\n",
    "        - SC counts per power: 7\n",
    "        - Unit counts per power: 7\n",
    "        - Phase info: 4 (year, season, phase type)\n",
    "    \n",
    "    We use simplified 1216-dim encoding for efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.num_locations = NUM_LOCATIONS\n",
    "        self.num_powers = NUM_POWERS\n",
    "        self.features_per_loc = 16  # Simplified\n",
    "        self.global_features = 16\n",
    "        self.state_size = self.num_locations * self.features_per_loc + self.global_features\n",
    "        \n",
    "    def encode(self, game_state: 'GameState', power: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode state from perspective of given power.\n",
    "        Power-specific encoding helps the model learn power-specific strategies.\n",
    "        \"\"\"\n",
    "        features = np.zeros(self.state_size, dtype=np.float32)\n",
    "        power_idx = POWERS.index(power)\n",
    "        \n",
    "        # Encode each location\n",
    "        for loc_idx, loc in enumerate(LOCATIONS):\n",
    "            offset = loc_idx * self.features_per_loc\n",
    "            \n",
    "            # Unit presence (relative to current power)\n",
    "            for p_idx, p in enumerate(POWERS):\n",
    "                # Reorder so current power is always index 0\n",
    "                rel_idx = (p_idx - power_idx) % NUM_POWERS\n",
    "                \n",
    "                for unit in game_state.units.get(p, []):\n",
    "                    unit_loc = self._get_unit_location(unit)\n",
    "                    if unit_loc == loc:\n",
    "                        features[offset + rel_idx] = 1.0\n",
    "                        # Unit type: army=1, fleet=0\n",
    "                        features[offset + 7] = 1.0 if unit.startswith('A') else 0.0\n",
    "            \n",
    "            # SC ownership (relative)\n",
    "            if loc in SUPPLY_CENTERS:\n",
    "                features[offset + 15] = 1.0  # Is SC\n",
    "                for p_idx, p in enumerate(POWERS):\n",
    "                    rel_idx = (p_idx - power_idx) % NUM_POWERS\n",
    "                    if loc in game_state.centers.get(p, []):\n",
    "                        features[offset + 8 + rel_idx] = 1.0\n",
    "        \n",
    "        # Global features\n",
    "        g_offset = self.num_locations * self.features_per_loc\n",
    "        \n",
    "        # SC counts (normalized, relative ordering)\n",
    "        for p_idx, p in enumerate(POWERS):\n",
    "            rel_idx = (p_idx - power_idx) % NUM_POWERS\n",
    "            sc_count = len(game_state.centers.get(p, []))\n",
    "            features[g_offset + rel_idx] = sc_count / VICTORY_THRESHOLD\n",
    "        \n",
    "        # Unit counts (normalized)\n",
    "        for p_idx, p in enumerate(POWERS):\n",
    "            rel_idx = (p_idx - power_idx) % NUM_POWERS\n",
    "            unit_count = len(game_state.units.get(p, []))\n",
    "            features[g_offset + 7 + rel_idx] = unit_count / 17.0\n",
    "        \n",
    "        # Phase info\n",
    "        features[g_offset + 14] = (game_state.year - 1901) / 20.0\n",
    "        features[g_offset + 15] = {'S': 0.0, 'F': 0.5, 'W': 1.0}.get(game_state.season, 0.0)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _get_unit_location(self, unit: str) -> str:\n",
    "        \"\"\"Extract location from unit string like 'A PAR' or 'F STP/SC'.\"\"\"\n",
    "        parts = unit.split()\n",
    "        if len(parts) >= 2:\n",
    "            return parts[1].split('/')[0]\n",
    "        return ''\n",
    "\n",
    "state_encoder = DiplomacyStateEncoder()\n",
    "print(f'State size: {state_encoder.state_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionSpace:\n",
    "    \"\"\"\n",
    "    Defines valid actions for Diplomacy.\n",
    "    \n",
    "    Action types:\n",
    "        - HOLD: Unit stays in place\n",
    "        - MOVE: Unit moves to adjacent location\n",
    "        - SUPPORT: Unit supports another unit\n",
    "        - CONVOY: Fleet convoys army\n",
    "    \n",
    "    We use a simplified action encoding:\n",
    "        action = (unit_loc, order_type, target_loc)\n",
    "    \n",
    "    Total theoretical actions: 75 × 4 × 75 = 22,500\n",
    "    But we mask invalid actions per state.\n",
    "    \"\"\"\n",
    "    \n",
    "    ORDER_TYPES = ['HOLD', 'MOVE', 'SUPPORT_HOLD', 'SUPPORT_MOVE', 'CONVOY']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.num_order_types = len(self.ORDER_TYPES)\n",
    "        # Simplified: encode as (source, target) pairs + order type\n",
    "        # Action index = source * num_locations * num_types + target * num_types + order_type\n",
    "        self.action_size = NUM_LOCATIONS * NUM_LOCATIONS * self.num_order_types\n",
    "        \n",
    "        # For practical use, we'll work with a vocabulary\n",
    "        self.action_vocab = {}\n",
    "        self.idx_to_action = {}\n",
    "        self._build_vocab()\n",
    "        \n",
    "    def _build_vocab(self):\n",
    "        \"\"\"Build vocabulary of common actions.\"\"\"\n",
    "        idx = 0\n",
    "        \n",
    "        # Add HOLD for each location\n",
    "        for loc in LOCATIONS:\n",
    "            for unit_type in ['A', 'F']:\n",
    "                action = f'{unit_type} {loc} H'\n",
    "                self.action_vocab[action] = idx\n",
    "                self.idx_to_action[idx] = action\n",
    "                idx += 1\n",
    "        \n",
    "        # Add MOVE for each location pair\n",
    "        for src in LOCATIONS:\n",
    "            for dst in ADJACENCIES.get(src, []):\n",
    "                for unit_type in ['A', 'F']:\n",
    "                    action = f'{unit_type} {src} - {dst}'\n",
    "                    self.action_vocab[action] = idx\n",
    "                    self.idx_to_action[idx] = action\n",
    "                    idx += 1\n",
    "        \n",
    "        self.vocab_size = len(self.action_vocab)\n",
    "        print(f'Action vocabulary size: {self.vocab_size}')\n",
    "    \n",
    "    def get_valid_actions(self, game_state: 'GameState', power: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Get valid action indices for a power in given state.\n",
    "        Returns list of valid action indices.\n",
    "        \"\"\"\n",
    "        valid = []\n",
    "        units = game_state.units.get(power, [])\n",
    "        \n",
    "        for unit in units:\n",
    "            unit_type = unit[0]  # 'A' or 'F'\n",
    "            unit_loc = self._get_unit_location(unit)\n",
    "            \n",
    "            # HOLD is always valid\n",
    "            hold_action = f'{unit_type} {unit_loc} H'\n",
    "            if hold_action in self.action_vocab:\n",
    "                valid.append(self.action_vocab[hold_action])\n",
    "            \n",
    "            # MOVE to adjacent locations\n",
    "            for adj in ADJACENCIES.get(unit_loc, []):\n",
    "                move_action = f'{unit_type} {unit_loc} - {adj}'\n",
    "                if move_action in self.action_vocab:\n",
    "                    valid.append(self.action_vocab[move_action])\n",
    "        \n",
    "        return valid if valid else [0]  # Return at least one action\n",
    "    \n",
    "    def _get_unit_location(self, unit: str) -> str:\n",
    "        parts = unit.split()\n",
    "        if len(parts) >= 2:\n",
    "            return parts[1].split('/')[0]\n",
    "        return ''\n",
    "    \n",
    "    def decode_action(self, idx: int) -> str:\n",
    "        return self.idx_to_action.get(idx, 'UNKNOWN')\n",
    "    \n",
    "    def encode_action(self, action: str) -> int:\n",
    "        return self.action_vocab.get(action, 0)\n",
    "\n",
    "action_space = ActionSpace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Game State & Simplified Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameState:\n",
    "    \"\"\"\n",
    "    Represents Diplomacy game state.\n",
    "    Simplified implementation for self-play training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.units = copy.deepcopy(STARTING_UNITS)\n",
    "        self.centers = copy.deepcopy(STARTING_CENTERS)\n",
    "        self.year = 1901\n",
    "        self.season = 'S'  # S=Spring, F=Fall, W=Winter\n",
    "        self.phase = 'M'   # M=Movement, R=Retreat, A=Adjustment\n",
    "        \n",
    "    def clone(self) -> 'GameState':\n",
    "        new_state = GameState.__new__(GameState)\n",
    "        new_state.units = copy.deepcopy(self.units)\n",
    "        new_state.centers = copy.deepcopy(self.centers)\n",
    "        new_state.year = self.year\n",
    "        new_state.season = self.season\n",
    "        new_state.phase = self.phase\n",
    "        return new_state\n",
    "    \n",
    "    def get_phase_name(self) -> str:\n",
    "        return f'{self.season}{self.year}{self.phase}'\n",
    "    \n",
    "    def get_sc_count(self, power: str) -> int:\n",
    "        return len(self.centers.get(power, []))\n",
    "    \n",
    "    def get_winner(self) -> Optional[str]:\n",
    "        \"\"\"Return winner if someone has 18+ SCs, else None.\"\"\"\n",
    "        for power in POWERS:\n",
    "            if self.get_sc_count(power) >= VICTORY_THRESHOLD:\n",
    "                return power\n",
    "        return None\n",
    "    \n",
    "    def is_eliminated(self, power: str) -> bool:\n",
    "        return len(self.units.get(power, [])) == 0\n",
    "    \n",
    "    def get_alive_powers(self) -> List[str]:\n",
    "        return [p for p in POWERS if not self.is_eliminated(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiplomacyEnv:\n",
    "    \"\"\"\n",
    "    Simplified Diplomacy environment for self-play.\n",
    "    \n",
    "    Key simplifications:\n",
    "    - Simultaneous move resolution (simplified)\n",
    "    - No retreat phases (units just disappear on conflict)\n",
    "    - No build phases (unit count = SC count, capped)\n",
    "    \n",
    "    This allows faster training iterations.\n",
    "    For full rules, use the 'diplomacy' package.\n",
    "    \"\"\"\n",
    "    \n",
    "    MAX_YEARS = 20  # Game ends after 1920\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "        self.action_space = action_space\n",
    "        self.state_encoder = state_encoder\n",
    "        \n",
    "    def reset(self) -> GameState:\n",
    "        \"\"\"Reset to initial game state.\"\"\"\n",
    "        self.state = GameState()\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, actions: Dict[str, List[str]]) -> Tuple[GameState, Dict[str, float], bool]:\n",
    "        \"\"\"\n",
    "        Execute one game step with all powers' actions.\n",
    "        \n",
    "        Args:\n",
    "            actions: Dict mapping power -> list of order strings\n",
    "            \n",
    "        Returns:\n",
    "            (new_state, rewards, done)\n",
    "        \"\"\"\n",
    "        # Resolve moves (simplified)\n",
    "        self._resolve_moves(actions)\n",
    "        \n",
    "        # Update phase\n",
    "        self._advance_phase()\n",
    "        \n",
    "        # Calculate rewards\n",
    "        rewards = self._calculate_rewards()\n",
    "        \n",
    "        # Check if game is done\n",
    "        done = self._is_game_over()\n",
    "        \n",
    "        return self.state, rewards, done\n",
    "    \n",
    "    def _resolve_moves(self, actions: Dict[str, List[str]]):\n",
    "        \"\"\"\n",
    "        Simplified move resolution.\n",
    "        \n",
    "        Rules:\n",
    "        - Moves to empty spaces succeed\n",
    "        - Conflicting moves: both bounce back\n",
    "        - Supported moves beat unsupported\n",
    "        \"\"\"\n",
    "        # Track destinations\n",
    "        destinations = defaultdict(list)  # loc -> [(power, unit, action)]\n",
    "        holds = {}  # loc -> (power, unit)\n",
    "        \n",
    "        # Parse all orders\n",
    "        for power, orders in actions.items():\n",
    "            for order in orders:\n",
    "                if ' H' in order:  # HOLD\n",
    "                    loc = self._parse_location(order)\n",
    "                    holds[loc] = (power, order.split()[0] + ' ' + loc)\n",
    "                elif ' - ' in order:  # MOVE\n",
    "                    parts = order.split(' - ')\n",
    "                    if len(parts) == 2:\n",
    "                        src = self._parse_location(parts[0])\n",
    "                        dst = parts[1].split()[0].split('/')[0]\n",
    "                        unit = order.split()[0] + ' ' + src\n",
    "                        destinations[dst].append((power, unit, src))\n",
    "        \n",
    "        # Resolve conflicts\n",
    "        new_units = {p: [] for p in POWERS}\n",
    "        moved_from = set()\n",
    "        \n",
    "        for dst, movers in destinations.items():\n",
    "            if len(movers) == 1:\n",
    "                # Successful move\n",
    "                power, unit, src = movers[0]\n",
    "                unit_type = unit[0]\n",
    "                new_units[power].append(f'{unit_type} {dst}')\n",
    "                moved_from.add(src)\n",
    "                \n",
    "                # Capture SC if applicable\n",
    "                if dst in SUPPLY_CENTERS:\n",
    "                    # Remove from other powers\n",
    "                    for p in POWERS:\n",
    "                        if dst in self.state.centers.get(p, []):\n",
    "                            self.state.centers[p].remove(dst)\n",
    "                    # Add to moving power\n",
    "                    if dst not in self.state.centers.get(power, []):\n",
    "                        self.state.centers[power].append(dst)\n",
    "            else:\n",
    "                # Bounce - units stay in place\n",
    "                for power, unit, src in movers:\n",
    "                    new_units[power].append(unit)\n",
    "        \n",
    "        # Keep holding units\n",
    "        for loc, (power, unit) in holds.items():\n",
    "            if loc not in moved_from:  # Wasn't dislodged\n",
    "                new_units[power].append(unit)\n",
    "        \n",
    "        # Update state\n",
    "        self.state.units = new_units\n",
    "    \n",
    "    def _parse_location(self, s: str) -> str:\n",
    "        \"\"\"Extract location from string.\"\"\"\n",
    "        parts = s.split()\n",
    "        for p in parts:\n",
    "            loc = p.split('/')[0]\n",
    "            if loc in LOCATIONS:\n",
    "                return loc\n",
    "        return ''\n",
    "    \n",
    "    def _advance_phase(self):\n",
    "        \"\"\"Advance to next phase.\"\"\"\n",
    "        if self.state.season == 'S':\n",
    "            self.state.season = 'F'\n",
    "        elif self.state.season == 'F':\n",
    "            self.state.season = 'W'\n",
    "        else:  # Winter\n",
    "            self.state.season = 'S'\n",
    "            self.state.year += 1\n",
    "    \n",
    "    def _calculate_rewards(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate rewards for each power.\n",
    "        \n",
    "        Reward shaping (from literature):\n",
    "        - +1.0 for winning (18+ SCs)\n",
    "        - -1.0 for elimination\n",
    "        - +0.01 per SC owned (intermediate reward)\n",
    "        - +0.1 for gaining an SC\n",
    "        - -0.1 for losing an SC\n",
    "        \"\"\"\n",
    "        rewards = {}\n",
    "        winner = self.state.get_winner()\n",
    "        \n",
    "        for power in POWERS:\n",
    "            if winner == power:\n",
    "                rewards[power] = 1.0\n",
    "            elif winner is not None:\n",
    "                rewards[power] = -1.0 / (NUM_POWERS - 1)\n",
    "            elif self.state.is_eliminated(power):\n",
    "                rewards[power] = -1.0\n",
    "            else:\n",
    "                # Intermediate reward based on SC count\n",
    "                sc_count = self.state.get_sc_count(power)\n",
    "                rewards[power] = 0.01 * sc_count\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    def _is_game_over(self) -> bool:\n",
    "        \"\"\"Check if game has ended.\"\"\"\n",
    "        # Someone won\n",
    "        if self.state.get_winner() is not None:\n",
    "            return True\n",
    "        \n",
    "        # Time limit\n",
    "        if self.state.year > 1901 + self.MAX_YEARS:\n",
    "            return True\n",
    "        \n",
    "        # Only one power left\n",
    "        alive = self.state.get_alive_powers()\n",
    "        if len(alive) <= 1:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_observation(self, power: str) -> np.ndarray:\n",
    "        \"\"\"Get encoded state from power's perspective.\"\"\"\n",
    "        return self.state_encoder.encode(self.state, power)\n",
    "\n",
    "# Test environment\n",
    "env = DiplomacyEnv()\n",
    "state = env.reset()\n",
    "print(f'Initial state: {state.get_phase_name()}')\n",
    "print(f'Units per power: {[(p, len(u)) for p, u in state.units.items()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Actor-Critic Network (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic network for PPO.\n",
    "    \n",
    "    Architecture follows AlphaGo Zero / Diplodocus:\n",
    "    - Shared feature extractor\n",
    "    - Policy head (actor): outputs action probabilities\n",
    "    - Value head (critic): outputs state value\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size: int, action_size: int, hidden_size: int = 512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Policy head (actor)\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, action_size)\n",
    "        )\n",
    "        \n",
    "        # Value head (critic)\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Returns:\n",
    "            (policy_logits, value)\n",
    "        \"\"\"\n",
    "        features = self.shared(x)\n",
    "        policy_logits = self.policy(features)\n",
    "        value = self.value(features)\n",
    "        return policy_logits, value\n",
    "    \n",
    "    def get_action(self, state: torch.Tensor, valid_actions: List[int] = None,\n",
    "                   deterministic: bool = False) -> Tuple[int, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Sample action from policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Encoded state tensor\n",
    "            valid_actions: List of valid action indices (for masking)\n",
    "            deterministic: If True, return argmax action\n",
    "            \n",
    "        Returns:\n",
    "            (action_idx, log_prob, value)\n",
    "        \"\"\"\n",
    "        policy_logits, value = self.forward(state)\n",
    "        \n",
    "        # Mask invalid actions\n",
    "        if valid_actions is not None and len(valid_actions) > 0:\n",
    "            mask = torch.ones_like(policy_logits) * float('-inf')\n",
    "            mask[0, valid_actions] = 0\n",
    "            policy_logits = policy_logits + mask\n",
    "        \n",
    "        probs = F.softmax(policy_logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        \n",
    "        if deterministic:\n",
    "            action = probs.argmax(dim=-1)\n",
    "        else:\n",
    "            action = dist.sample()\n",
    "        \n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob, value.squeeze(-1)\n",
    "    \n",
    "    def evaluate_action(self, state: torch.Tensor, action: torch.Tensor,\n",
    "                        valid_actions_batch: List[List[int]] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Evaluate actions for PPO update.\n",
    "        \n",
    "        Returns:\n",
    "            (log_probs, values, entropy)\n",
    "        \"\"\"\n",
    "        policy_logits, value = self.forward(state)\n",
    "        \n",
    "        # Apply masking if provided\n",
    "        if valid_actions_batch is not None:\n",
    "            for i, valid_actions in enumerate(valid_actions_batch):\n",
    "                if valid_actions:\n",
    "                    mask = torch.ones(policy_logits.size(-1), device=policy_logits.device) * float('-inf')\n",
    "                    mask[valid_actions] = 0\n",
    "                    policy_logits[i] = policy_logits[i] + mask\n",
    "        \n",
    "        probs = F.softmax(policy_logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        \n",
    "        log_probs = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        return log_probs, value.squeeze(-1), entropy\n",
    "\n",
    "# Test model\n",
    "model = ActorCritic(state_encoder.state_size, action_space.vocab_size).to(device)\n",
    "print(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience tuple\n",
    "Experience = namedtuple('Experience', \n",
    "    ['state', 'action', 'reward', 'next_state', 'done', 'log_prob', 'value', 'valid_actions'])\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "    \"\"\"\n",
    "    PPO Agent for Diplomacy self-play.\n",
    "    \n",
    "    Implements Proximal Policy Optimization (Schulman et al., 2017)\n",
    "    with clipped objective for stable training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size: int, action_size: int,\n",
    "                 lr: float = 3e-4,\n",
    "                 gamma: float = 0.99,\n",
    "                 gae_lambda: float = 0.95,\n",
    "                 clip_epsilon: float = 0.2,\n",
    "                 value_coef: float = 0.5,\n",
    "                 entropy_coef: float = 0.01,\n",
    "                 max_grad_norm: float = 0.5):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "        self.network = ActorCritic(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        # Experience buffer\n",
    "        self.buffer = []\n",
    "        \n",
    "    def select_action(self, state: np.ndarray, valid_actions: List[int] = None,\n",
    "                      deterministic: bool = False) -> Tuple[int, float, float]:\n",
    "        \"\"\"\n",
    "        Select action given state.\n",
    "        \n",
    "        Returns:\n",
    "            (action, log_prob, value)\n",
    "        \"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action, log_prob, value = self.network.get_action(\n",
    "                state_tensor, valid_actions, deterministic\n",
    "            )\n",
    "        \n",
    "        return action, log_prob.item(), value.item()\n",
    "    \n",
    "    def store_experience(self, exp: Experience):\n",
    "        \"\"\"Store experience in buffer.\"\"\"\n",
    "        self.buffer.append(exp)\n",
    "    \n",
    "    def compute_gae(self, rewards: List[float], values: List[float], \n",
    "                    dones: List[bool], next_value: float) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        Compute Generalized Advantage Estimation.\n",
    "        \n",
    "        Returns:\n",
    "            (advantages, returns)\n",
    "        \"\"\"\n",
    "        advantages = []\n",
    "        returns = []\n",
    "        gae = 0\n",
    "        \n",
    "        # Process in reverse order\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_val = next_value\n",
    "                next_done = 1.0\n",
    "            else:\n",
    "                next_val = values[t + 1]\n",
    "                next_done = 1.0 - float(dones[t + 1])\n",
    "            \n",
    "            delta = rewards[t] + self.gamma * next_val * next_done - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * next_done * gae\n",
    "            \n",
    "            advantages.insert(0, gae)\n",
    "            returns.insert(0, gae + values[t])\n",
    "        \n",
    "        return advantages, returns\n",
    "    \n",
    "    def update(self, num_epochs: int = 4, batch_size: int = 64) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Update policy using PPO.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with loss metrics\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return {}\n",
    "        \n",
    "        # Extract data from buffer\n",
    "        states = np.array([e.state for e in self.buffer])\n",
    "        actions = np.array([e.action for e in self.buffer])\n",
    "        rewards = [e.reward for e in self.buffer]\n",
    "        dones = [e.done for e in self.buffer]\n",
    "        old_log_probs = np.array([e.log_prob for e in self.buffer])\n",
    "        values = [e.value for e in self.buffer]\n",
    "        valid_actions_list = [e.valid_actions for e in self.buffer]\n",
    "        \n",
    "        # Compute advantages\n",
    "        advantages, returns = self.compute_gae(rewards, values, dones, 0.0)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states_t = torch.FloatTensor(states).to(device)\n",
    "        actions_t = torch.LongTensor(actions).to(device)\n",
    "        old_log_probs_t = torch.FloatTensor(old_log_probs).to(device)\n",
    "        advantages_t = torch.FloatTensor(advantages).to(device)\n",
    "        returns_t = torch.FloatTensor(returns).to(device)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages_t = (advantages_t - advantages_t.mean()) / (advantages_t.std() + 1e-8)\n",
    "        \n",
    "        # PPO update\n",
    "        total_loss = 0\n",
    "        policy_loss_sum = 0\n",
    "        value_loss_sum = 0\n",
    "        entropy_sum = 0\n",
    "        \n",
    "        dataset_size = len(self.buffer)\n",
    "        indices = np.arange(dataset_size)\n",
    "        \n",
    "        for _ in range(num_epochs):\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for start in range(0, dataset_size, batch_size):\n",
    "                end = min(start + batch_size, dataset_size)\n",
    "                batch_indices = indices[start:end]\n",
    "                \n",
    "                batch_states = states_t[batch_indices]\n",
    "                batch_actions = actions_t[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs_t[batch_indices]\n",
    "                batch_advantages = advantages_t[batch_indices]\n",
    "                batch_returns = returns_t[batch_indices]\n",
    "                batch_valid_actions = [valid_actions_list[i] for i in batch_indices]\n",
    "                \n",
    "                # Evaluate actions\n",
    "                new_log_probs, new_values, entropy = self.network.evaluate_action(\n",
    "                    batch_states, batch_actions, batch_valid_actions\n",
    "                )\n",
    "                \n",
    "                # Policy loss (clipped PPO objective)\n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Value loss\n",
    "                value_loss = F.mse_loss(new_values, batch_returns)\n",
    "                \n",
    "                # Entropy bonus (encourages exploration)\n",
    "                entropy_loss = -entropy.mean()\n",
    "                \n",
    "                # Total loss\n",
    "                loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss\n",
    "                \n",
    "                # Update\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                policy_loss_sum += policy_loss.item()\n",
    "                value_loss_sum += value_loss.item()\n",
    "                entropy_sum += (-entropy_loss.item())\n",
    "        \n",
    "        # Clear buffer\n",
    "        self.buffer = []\n",
    "        \n",
    "        num_batches = (dataset_size // batch_size) * num_epochs\n",
    "        return {\n",
    "            'total_loss': total_loss / max(num_batches, 1),\n",
    "            'policy_loss': policy_loss_sum / max(num_batches, 1),\n",
    "            'value_loss': value_loss_sum / max(num_batches, 1),\n",
    "            'entropy': entropy_sum / max(num_batches, 1)\n",
    "        }\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        torch.save({\n",
    "            'network_state_dict': self.network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load model checkpoint.\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        self.network.load_state_dict(checkpoint['network_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "print('PPO Agent defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Self-Play Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfPlayTrainer:\n",
    "    \"\"\"\n",
    "    Self-Play trainer following AlphaGo Zero paradigm.\n",
    "    \n",
    "    Key features:\n",
    "    - All 7 powers controlled by the same network\n",
    "    - Experience collected from all powers' perspectives\n",
    "    - Periodic evaluation against fixed checkpoints\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        \n",
    "        # Environment\n",
    "        self.env = DiplomacyEnv()\n",
    "        \n",
    "        # Agent\n",
    "        self.agent = PPOAgent(\n",
    "            state_size=state_encoder.state_size,\n",
    "            action_size=action_space.vocab_size,\n",
    "            lr=config.get('lr', 3e-4),\n",
    "            gamma=config.get('gamma', 0.99),\n",
    "            gae_lambda=config.get('gae_lambda', 0.95),\n",
    "            clip_epsilon=config.get('clip_epsilon', 0.2),\n",
    "            entropy_coef=config.get('entropy_coef', 0.01)\n",
    "        )\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'episode_rewards': [],\n",
    "            'episode_lengths': [],\n",
    "            'wins': defaultdict(int),\n",
    "            'losses': [],\n",
    "            'entropies': []\n",
    "        }\n",
    "        \n",
    "        # Checkpoints for evaluation\n",
    "        self.checkpoints = []\n",
    "        \n",
    "    def play_episode(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Play one full game with self-play.\n",
    "        All powers use the same policy network.\n",
    "        \n",
    "        Returns:\n",
    "            Episode statistics\n",
    "        \"\"\"\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        episode_rewards = {p: 0.0 for p in POWERS}\n",
    "        episode_length = 0\n",
    "        \n",
    "        # Store experiences per power\n",
    "        power_experiences = {p: [] for p in POWERS}\n",
    "        \n",
    "        done = False\n",
    "        max_steps = self.config.get('max_steps_per_episode', 100)\n",
    "        \n",
    "        while not done and episode_length < max_steps:\n",
    "            # Collect actions from all powers\n",
    "            actions = {}\n",
    "            power_data = {}  # Store state, action, log_prob, value per power\n",
    "            \n",
    "            for power in POWERS:\n",
    "                if state.is_eliminated(power):\n",
    "                    actions[power] = []\n",
    "                    continue\n",
    "                \n",
    "                # Get observation from this power's perspective\n",
    "                obs = self.env.get_observation(power)\n",
    "                \n",
    "                # Get valid actions\n",
    "                valid_actions = action_space.get_valid_actions(state, power)\n",
    "                \n",
    "                # Select action for each unit\n",
    "                power_orders = []\n",
    "                for unit in state.units.get(power, []):\n",
    "                    action_idx, log_prob, value = self.agent.select_action(obs, valid_actions)\n",
    "                    action_str = action_space.decode_action(action_idx)\n",
    "                    power_orders.append(action_str)\n",
    "                    \n",
    "                    power_data[power] = {\n",
    "                        'obs': obs.copy(),\n",
    "                        'action': action_idx,\n",
    "                        'log_prob': log_prob,\n",
    "                        'value': value,\n",
    "                        'valid_actions': valid_actions\n",
    "                    }\n",
    "                \n",
    "                actions[power] = power_orders\n",
    "            \n",
    "            # Execute step\n",
    "            next_state, rewards, done = self.env.step(actions)\n",
    "            \n",
    "            # Store experiences\n",
    "            for power in POWERS:\n",
    "                if power in power_data:\n",
    "                    data = power_data[power]\n",
    "                    next_obs = self.env.get_observation(power)\n",
    "                    \n",
    "                    exp = Experience(\n",
    "                        state=data['obs'],\n",
    "                        action=data['action'],\n",
    "                        reward=rewards.get(power, 0.0),\n",
    "                        next_state=next_obs,\n",
    "                        done=done,\n",
    "                        log_prob=data['log_prob'],\n",
    "                        value=data['value'],\n",
    "                        valid_actions=data['valid_actions']\n",
    "                    )\n",
    "                    self.agent.store_experience(exp)\n",
    "                    episode_rewards[power] += rewards.get(power, 0.0)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_length += 1\n",
    "        \n",
    "        # Determine winner\n",
    "        winner = state.get_winner()\n",
    "        \n",
    "        return {\n",
    "            'rewards': episode_rewards,\n",
    "            'length': episode_length,\n",
    "            'winner': winner,\n",
    "            'final_scs': {p: state.get_sc_count(p) for p in POWERS}\n",
    "        }\n",
    "    \n",
    "    def train(self, num_episodes: int, update_freq: int = 10,\n",
    "              eval_freq: int = 50, save_freq: int = 100):\n",
    "        \"\"\"\n",
    "        Main training loop.\n",
    "        \n",
    "        Args:\n",
    "            num_episodes: Total episodes to train\n",
    "            update_freq: Episodes between PPO updates\n",
    "            eval_freq: Episodes between evaluations\n",
    "            save_freq: Episodes between checkpoint saves\n",
    "        \"\"\"\n",
    "        print('='*60)\n",
    "        print('SELF-PLAY TRAINING')\n",
    "        print('='*60)\n",
    "        print(f'Episodes: {num_episodes}')\n",
    "        print(f'Update frequency: {update_freq}')\n",
    "        print(f'Device: {device}')\n",
    "        print('='*60)\n",
    "        \n",
    "        pbar = tqdm(range(num_episodes), desc='Training')\n",
    "        \n",
    "        for episode in pbar:\n",
    "            # Play episode\n",
    "            stats = self.play_episode()\n",
    "            \n",
    "            # Record stats\n",
    "            total_reward = sum(stats['rewards'].values())\n",
    "            self.history['episode_rewards'].append(total_reward)\n",
    "            self.history['episode_lengths'].append(stats['length'])\n",
    "            \n",
    "            if stats['winner']:\n",
    "                self.history['wins'][stats['winner']] += 1\n",
    "            \n",
    "            # Update policy\n",
    "            if (episode + 1) % update_freq == 0:\n",
    "                loss_info = self.agent.update(\n",
    "                    num_epochs=self.config.get('ppo_epochs', 4),\n",
    "                    batch_size=self.config.get('batch_size', 64)\n",
    "                )\n",
    "                if loss_info:\n",
    "                    self.history['losses'].append(loss_info['total_loss'])\n",
    "                    self.history['entropies'].append(loss_info['entropy'])\n",
    "            \n",
    "            # Update progress bar\n",
    "            avg_reward = np.mean(self.history['episode_rewards'][-100:])\n",
    "            avg_length = np.mean(self.history['episode_lengths'][-100:])\n",
    "            pbar.set_postfix({\n",
    "                'reward': f'{avg_reward:.2f}',\n",
    "                'length': f'{avg_length:.1f}',\n",
    "                'wins': sum(self.history['wins'].values())\n",
    "            })\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (episode + 1) % save_freq == 0:\n",
    "                self.save_checkpoint(episode + 1)\n",
    "            \n",
    "            # Evaluation\n",
    "            if (episode + 1) % eval_freq == 0:\n",
    "                self.evaluate()\n",
    "        \n",
    "        print('\\nTraining complete!')\n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self, num_games: int = 10):\n",
    "        \"\"\"Evaluate current policy.\"\"\"\n",
    "        wins = defaultdict(int)\n",
    "        \n",
    "        for _ in range(num_games):\n",
    "            stats = self.play_episode()\n",
    "            if stats['winner']:\n",
    "                wins[stats['winner']] += 1\n",
    "            # Clear buffer (don't train on eval games)\n",
    "            self.agent.buffer = []\n",
    "        \n",
    "        print(f\"\\nEvaluation ({num_games} games):\")\n",
    "        for power in POWERS:\n",
    "            print(f\"  {power}: {wins[power]} wins ({wins[power]/num_games*100:.1f}%)\")\n",
    "    \n",
    "    def save_checkpoint(self, episode: int):\n",
    "        \"\"\"Save training checkpoint.\"\"\"\n",
    "        path = f'checkpoint_ep{episode}.pt'\n",
    "        self.agent.save(path)\n",
    "        self.checkpoints.append(path)\n",
    "        print(f'\\nSaved checkpoint: {path}')\n",
    "\n",
    "print('SelfPlayTrainer defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    # Training\n",
    "    'num_episodes': 1000,\n",
    "    'max_steps_per_episode': 60,  # ~20 years × 3 phases\n",
    "    'update_freq': 10,\n",
    "    'eval_freq': 100,\n",
    "    'save_freq': 200,\n",
    "    \n",
    "    # PPO\n",
    "    'lr': 3e-4,\n",
    "    'gamma': 0.99,\n",
    "    'gae_lambda': 0.95,\n",
    "    'clip_epsilon': 0.2,\n",
    "    'entropy_coef': 0.01,\n",
    "    'ppo_epochs': 4,\n",
    "    'batch_size': 64,\n",
    "}\n",
    "\n",
    "print('Configuration:')\n",
    "for k, v in CONFIG.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = SelfPlayTrainer(CONFIG)\n",
    "\n",
    "# Train!\n",
    "history = trainer.train(\n",
    "    num_episodes=CONFIG['num_episodes'],\n",
    "    update_freq=CONFIG['update_freq'],\n",
    "    eval_freq=CONFIG['eval_freq'],\n",
    "    save_freq=CONFIG['save_freq']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Episode rewards\n",
    "ax = axes[0, 0]\n",
    "rewards = history['episode_rewards']\n",
    "ax.plot(rewards, alpha=0.3, color='blue')\n",
    "# Moving average\n",
    "window = 50\n",
    "if len(rewards) >= window:\n",
    "    ma = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards)), ma, color='red', linewidth=2, label=f'MA-{window}')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Total Reward')\n",
    "ax.set_title('Episode Rewards')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Episode lengths\n",
    "ax = axes[0, 1]\n",
    "lengths = history['episode_lengths']\n",
    "ax.plot(lengths, alpha=0.3, color='green')\n",
    "if len(lengths) >= window:\n",
    "    ma = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(lengths)), ma, color='red', linewidth=2)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Steps')\n",
    "ax.set_title('Episode Length')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Win distribution\n",
    "ax = axes[1, 0]\n",
    "wins = history['wins']\n",
    "powers = POWERS + ['Draw']\n",
    "win_counts = [wins.get(p, 0) for p in POWERS] + [CONFIG['num_episodes'] - sum(wins.values())]\n",
    "colors = plt.cm.tab10(range(len(powers)))\n",
    "ax.bar(powers, win_counts, color=colors)\n",
    "ax.set_xlabel('Power')\n",
    "ax.set_ylabel('Wins')\n",
    "ax.set_title('Win Distribution')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Loss curve\n",
    "ax = axes[1, 1]\n",
    "if history['losses']:\n",
    "    ax.plot(history['losses'], label='Loss', color='purple')\n",
    "    ax.set_xlabel('Update')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('self_play_training.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final statistics\n",
    "print('='*60)\n",
    "print('TRAINING SUMMARY')\n",
    "print('='*60)\n",
    "print(f\"\\nTotal episodes: {CONFIG['num_episodes']}\")\n",
    "print(f\"Average reward (last 100): {np.mean(history['episode_rewards'][-100:]):.3f}\")\n",
    "print(f\"Average length (last 100): {np.mean(history['episode_lengths'][-100:]):.1f}\")\n",
    "\n",
    "print(f\"\\nWin distribution:\")\n",
    "total_wins = sum(history['wins'].values())\n",
    "for power in POWERS:\n",
    "    wins = history['wins'].get(power, 0)\n",
    "    pct = wins / CONFIG['num_episodes'] * 100\n",
    "    print(f\"  {power}: {wins} ({pct:.1f}%)\")\n",
    "\n",
    "draws = CONFIG['num_episodes'] - total_wins\n",
    "print(f\"  Draws: {draws} ({draws/CONFIG['num_episodes']*100:.1f}%)\")\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "trainer.agent.save('self_play_final.pt')\n",
    "\n",
    "# Save training history\n",
    "import json\n",
    "with open('training_history.json', 'w') as f:\n",
    "    # Convert defaultdict to regular dict for JSON\n",
    "    history_save = {\n",
    "        'episode_rewards': history['episode_rewards'],\n",
    "        'episode_lengths': history['episode_lengths'],\n",
    "        'wins': dict(history['wins']),\n",
    "        'losses': history['losses'],\n",
    "        'config': CONFIG\n",
    "    }\n",
    "    json.dump(history_save, f)\n",
    "\n",
    "print('Saved:')\n",
    "print('  - self_play_final.pt')\n",
    "print('  - training_history.json')\n",
    "print('  - self_play_training.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('self_play_final.pt')\n",
    "files.download('training_history.json')\n",
    "files.download('self_play_training.png')\n",
    "\n",
    "print('Files downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Analysis: Measuring Overfitting (RQ1)\n",
    "\n",
    "To quantify overfitting in pure self-play, we need to:\n",
    "1. Evaluate against **fixed checkpoints** from earlier in training\n",
    "2. Evaluate against **random policy**\n",
    "3. Compare win rates\n",
    "\n",
    "Signs of overfitting:\n",
    "- High win rate vs recent checkpoints\n",
    "- Low win rate vs old checkpoints (strategy cycling)\n",
    "- Narrow strategy distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vs_random(agent, num_games: int = 20):\n",
    "    \"\"\"\n",
    "    Evaluate trained agent vs random policy.\n",
    "    Agent plays as one power, others play randomly.\n",
    "    \"\"\"\n",
    "    env = DiplomacyEnv()\n",
    "    wins = 0\n",
    "    agent_power = 'FRANCE'  # Agent plays as France\n",
    "    \n",
    "    for _ in tqdm(range(num_games), desc='Evaluating vs Random'):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < 60:\n",
    "            actions = {}\n",
    "            \n",
    "            for power in POWERS:\n",
    "                if state.is_eliminated(power):\n",
    "                    actions[power] = []\n",
    "                    continue\n",
    "                \n",
    "                valid = action_space.get_valid_actions(state, power)\n",
    "                \n",
    "                if power == agent_power:\n",
    "                    # Agent action\n",
    "                    obs = env.get_observation(power)\n",
    "                    action_idx, _, _ = agent.select_action(obs, valid, deterministic=True)\n",
    "                    actions[power] = [action_space.decode_action(action_idx)]\n",
    "                else:\n",
    "                    # Random action\n",
    "                    if valid:\n",
    "                        action_idx = random.choice(valid)\n",
    "                        actions[power] = [action_space.decode_action(action_idx)]\n",
    "                    else:\n",
    "                        actions[power] = []\n",
    "            \n",
    "            state, _, done = env.step(actions)\n",
    "            steps += 1\n",
    "        \n",
    "        winner = state.get_winner()\n",
    "        if winner == agent_power:\n",
    "            wins += 1\n",
    "    \n",
    "    win_rate = wins / num_games\n",
    "    print(f\"\\nWin rate vs Random: {win_rate*100:.1f}% ({wins}/{num_games})\")\n",
    "    return win_rate\n",
    "\n",
    "# Evaluate\n",
    "win_rate = evaluate_vs_random(trainer.agent, num_games=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary & Next Steps\n",
    "\n",
    "### What We Built\n",
    "- Full self-play RL pipeline for No-Press Diplomacy\n",
    "- PPO-based policy optimization\n",
    "- Actor-Critic network with action masking\n",
    "- Experience collection from all 7 powers' perspectives\n",
    "\n",
    "### Results\n",
    "- See training curves above\n",
    "- Win distribution shows which powers the agent favors\n",
    "- Comparison vs random baseline measures basic competence\n",
    "\n",
    "### Limitations of Pure Self-Play (RQ1)\n",
    "As predicted by literature:\n",
    "- May overfit to self's strategies\n",
    "- Limited strategy diversity\n",
    "- Performance may degrade vs novel opponents\n",
    "\n",
    "### Next Steps\n",
    "1. **Human-Regularized RL**: Add KL penalty toward human policy (BC model)\n",
    "2. **Population-Based Training**: Maintain diverse opponent pool\n",
    "3. **Full Game Rules**: Integrate with `diplomacy` package for accurate simulation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
