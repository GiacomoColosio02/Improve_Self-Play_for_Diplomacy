{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Play Reinforcement Learning for No-Press Diplomacy\n",
    "## Using Official Diplomacy Package\n",
    "\n",
    "**Project:** Improve Self-Play for Diplomacy  \n",
    "**Authors:** Giacomo Colosio, Maciej Tasarz, Jakub Seliga, Luka Ivcevic  \n",
    "**Course:** ISP - UPC Barcelona, Fall 2025/26\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "- **Silver et al. (2017)** - AlphaGo Zero: Mastering Go without human knowledge\n",
    "- **Bakhtin et al. (2021)** - DORA: Double Oracle RL for Diplomacy  \n",
    "- **Bakhtin et al. (2022)** - No-Press Diplomacy from Scratch with Human-Regularized RL\n",
    "- **Paquette et al. (2019)** - No-Press Diplomacy: Modeling Multi-Agent Gameplay\n",
    "\n",
    "### This Notebook\n",
    "- Uses official `diplomacy` package for accurate game simulation\n",
    "- Implements PPO (Proximal Policy Optimization) for policy learning\n",
    "- Actor-Critic architecture with proper state/action encoding\n",
    "- Addresses RQ1: Quantify overfitting in pure self-play\n",
    "\n",
    "**Requirements:** GPU runtime (Runtime → Change runtime type → GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install diplomacy torch numpy matplotlib tqdm tensorboard --quiet\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import copy\n",
    "from collections import defaultdict, deque, namedtuple\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Diplomacy package\n",
    "from diplomacy import Game\n",
    "from diplomacy.utils.export import to_saved_game_format\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Game Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Diplomacy constants\n",
    "POWERS = ['AUSTRIA', 'ENGLAND', 'FRANCE', 'GERMANY', 'ITALY', 'RUSSIA', 'TURKEY']\n",
    "NUM_POWERS = 7\n",
    "\n",
    "# All 75 provinces on the standard map\n",
    "LOCATIONS = [\n",
    "    # 34 Supply Centers\n",
    "    'ANK', 'BEL', 'BER', 'BRE', 'BUD', 'BUL', 'CON', 'DEN', 'EDI', 'GRE',\n",
    "    'HOL', 'KIE', 'LON', 'LVP', 'MAR', 'MOS', 'MUN', 'NAP', 'NWY', 'PAR',\n",
    "    'POR', 'ROM', 'RUM', 'SER', 'SEV', 'SMY', 'SPA', 'STP', 'SWE', 'TRI',\n",
    "    'TUN', 'VEN', 'VIE', 'WAR',\n",
    "    # 22 Non-SC land provinces\n",
    "    'ALB', 'APU', 'ARM', 'BOH', 'BUR', 'CLY', 'FIN', 'GAL', 'GAS', 'LVN',\n",
    "    'NAF', 'PIC', 'PIE', 'PRU', 'RUH', 'SIL', 'SYR', 'TUS', 'TYR', 'UKR',\n",
    "    'WAL', 'YOR',\n",
    "    # 19 Sea provinces\n",
    "    'ADR', 'AEG', 'BAL', 'BAR', 'BLA', 'BOT', 'EAS', 'ENG', 'GOL', 'HEL',\n",
    "    'ION', 'IRI', 'MAO', 'NAO', 'NTH', 'NWG', 'SKA', 'TYS', 'WES'\n",
    "]\n",
    "NUM_LOCATIONS = 75\n",
    "\n",
    "SUPPLY_CENTERS = set(LOCATIONS[:34])\n",
    "VICTORY_CENTERS = 18\n",
    "\n",
    "# Mappings\n",
    "LOC_TO_IDX = {loc: i for i, loc in enumerate(LOCATIONS)}\n",
    "IDX_TO_LOC = {i: loc for i, loc in enumerate(LOCATIONS)}\n",
    "POWER_TO_IDX = {p: i for i, p in enumerate(POWERS)}\n",
    "IDX_TO_POWER = {i: p for i, p in enumerate(POWERS)}\n",
    "\n",
    "print(f'Powers: {NUM_POWERS}')\n",
    "print(f'Locations: {NUM_LOCATIONS}')\n",
    "print(f'Supply Centers: {len(SUPPLY_CENTERS)}')\n",
    "print(f'Victory requires: {VICTORY_CENTERS} SCs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. State Encoder\n",
    "\n",
    "Based on Bakhtin et al. (2022) encoding scheme:\n",
    "- Board state encoding per location\n",
    "- Relative encoding from each power's perspective\n",
    "- Season and year information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiplomacyStateEncoder:\n",
    "    \"\"\"\n",
    "    Encodes Diplomacy game state into fixed-size tensor.\n",
    "    \n",
    "    Features per location (75 locations × 35 features = 2625):\n",
    "        - Unit presence: 7 powers × 2 unit types = 14 (one-hot)\n",
    "        - Unit can move here: 7 powers = 7 (binary)\n",
    "        - SC ownership: 8 (7 powers + neutral)\n",
    "        - Dislodged unit: 7 powers (for retreat phases)\n",
    "        - Area type: 3 (land, sea, coast)\n",
    "        - Is supply center: 1\n",
    "    \n",
    "    Global features (29):\n",
    "        - SC count per power: 7 (normalized)\n",
    "        - Unit count per power: 7 (normalized)\n",
    "        - Build/disband count per power: 7\n",
    "        - Season: 3 (one-hot: spring, fall, winter)\n",
    "        - Year: 1 (normalized)\n",
    "        - Phase type: 3 (movement, retreat, adjustment)\n",
    "        - Current power: 1 (index, normalized)\n",
    "    \n",
    "    Total: ~2654 features (we use 2048 with compression)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, compressed_size: int = 2048):\n",
    "        self.compressed_size = compressed_size\n",
    "        self.raw_loc_features = 23  # Features per location\n",
    "        self.raw_global_features = 29\n",
    "        self.raw_size = NUM_LOCATIONS * self.raw_loc_features + self.raw_global_features\n",
    "        \n",
    "        # Compression layer (learned during training)\n",
    "        self.use_compression = compressed_size < self.raw_size\n",
    "        self.state_size = compressed_size if self.use_compression else self.raw_size\n",
    "        \n",
    "    def encode(self, game: Game, power_name: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode game state from the perspective of power_name.\n",
    "        \n",
    "        Args:\n",
    "            game: Diplomacy Game object\n",
    "            power_name: Power whose perspective to encode from\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of shape (state_size,)\n",
    "        \"\"\"\n",
    "        features = np.zeros(self.raw_size, dtype=np.float32)\n",
    "        power_idx = POWER_TO_IDX[power_name]\n",
    "        \n",
    "        # Get game state\n",
    "        state = game.get_state()\n",
    "        units = state['units']\n",
    "        centers = state['centers']\n",
    "        \n",
    "        # Parse current phase\n",
    "        phase_name = game.get_current_phase()\n",
    "        year = self._parse_year(phase_name)\n",
    "        season = self._parse_season(phase_name)\n",
    "        phase_type = self._parse_phase_type(phase_name)\n",
    "        \n",
    "        # Build unit location map\n",
    "        unit_locations = {}  # loc -> (power, unit_type)\n",
    "        for pwr, pwr_units in units.items():\n",
    "            for unit in pwr_units:\n",
    "                unit_type, loc = self._parse_unit(unit)\n",
    "                if loc:\n",
    "                    unit_locations[loc] = (pwr, unit_type)\n",
    "        \n",
    "        # Encode each location\n",
    "        for loc_idx, loc in enumerate(LOCATIONS):\n",
    "            offset = loc_idx * self.raw_loc_features\n",
    "            \n",
    "            # Unit presence (relative to current power)\n",
    "            if loc in unit_locations:\n",
    "                pwr, unit_type = unit_locations[loc]\n",
    "                rel_pwr_idx = (POWER_TO_IDX[pwr] - power_idx) % NUM_POWERS\n",
    "                \n",
    "                if unit_type == 'A':\n",
    "                    features[offset + rel_pwr_idx] = 1.0  # Army\n",
    "                else:\n",
    "                    features[offset + NUM_POWERS + rel_pwr_idx] = 1.0  # Fleet\n",
    "            \n",
    "            # SC ownership (relative)\n",
    "            sc_offset = offset + 14\n",
    "            if loc in SUPPLY_CENTERS:\n",
    "                features[offset + 22] = 1.0  # Is SC\n",
    "                owned = False\n",
    "                for pwr, pwr_centers in centers.items():\n",
    "                    if loc in pwr_centers:\n",
    "                        rel_pwr_idx = (POWER_TO_IDX[pwr] - power_idx) % NUM_POWERS\n",
    "                        features[sc_offset + rel_pwr_idx] = 1.0\n",
    "                        owned = True\n",
    "                        break\n",
    "                if not owned:\n",
    "                    features[sc_offset + 7] = 1.0  # Neutral\n",
    "        \n",
    "        # Global features\n",
    "        g_offset = NUM_LOCATIONS * self.raw_loc_features\n",
    "        \n",
    "        # SC counts (normalized, relative order)\n",
    "        for pwr in POWERS:\n",
    "            rel_idx = (POWER_TO_IDX[pwr] - power_idx) % NUM_POWERS\n",
    "            sc_count = len(centers.get(pwr, []))\n",
    "            features[g_offset + rel_idx] = sc_count / VICTORY_CENTERS\n",
    "        \n",
    "        # Unit counts (normalized)\n",
    "        for pwr in POWERS:\n",
    "            rel_idx = (POWER_TO_IDX[pwr] - power_idx) % NUM_POWERS\n",
    "            unit_count = len(units.get(pwr, []))\n",
    "            features[g_offset + 7 + rel_idx] = unit_count / 17.0\n",
    "        \n",
    "        # Build counts\n",
    "        for pwr in POWERS:\n",
    "            rel_idx = (POWER_TO_IDX[pwr] - power_idx) % NUM_POWERS\n",
    "            sc_count = len(centers.get(pwr, []))\n",
    "            unit_count = len(units.get(pwr, []))\n",
    "            build_count = sc_count - unit_count\n",
    "            features[g_offset + 14 + rel_idx] = np.clip(build_count / 5.0, -1, 1)\n",
    "        \n",
    "        # Season (one-hot)\n",
    "        season_offset = g_offset + 21\n",
    "        if season == 'S':\n",
    "            features[season_offset] = 1.0\n",
    "        elif season == 'F':\n",
    "            features[season_offset + 1] = 1.0\n",
    "        else:\n",
    "            features[season_offset + 2] = 1.0\n",
    "        \n",
    "        # Year (normalized)\n",
    "        features[g_offset + 24] = (year - 1901) / 20.0\n",
    "        \n",
    "        # Phase type (one-hot)\n",
    "        phase_offset = g_offset + 25\n",
    "        if phase_type == 'M':\n",
    "            features[phase_offset] = 1.0\n",
    "        elif phase_type == 'R':\n",
    "            features[phase_offset + 1] = 1.0\n",
    "        else:\n",
    "            features[phase_offset + 2] = 1.0\n",
    "        \n",
    "        # Current power index\n",
    "        features[g_offset + 28] = power_idx / (NUM_POWERS - 1)\n",
    "        \n",
    "        return features[:self.state_size] if not self.use_compression else features[:self.raw_size]\n",
    "    \n",
    "    def _parse_unit(self, unit: str) -> Tuple[str, str]:\n",
    "        \"\"\"Parse unit string like 'A PAR' or 'F STP/SC'.\"\"\"\n",
    "        parts = unit.split()\n",
    "        if len(parts) >= 2:\n",
    "            unit_type = parts[0]\n",
    "            loc = parts[1].split('/')[0]\n",
    "            return unit_type, loc\n",
    "        return '', ''\n",
    "    \n",
    "    def _parse_year(self, phase: str) -> int:\n",
    "        try:\n",
    "            return int(phase[1:5])\n",
    "        except:\n",
    "            return 1901\n",
    "    \n",
    "    def _parse_season(self, phase: str) -> str:\n",
    "        return phase[0] if phase else 'S'\n",
    "    \n",
    "    def _parse_phase_type(self, phase: str) -> str:\n",
    "        return phase[-1] if phase else 'M'\n",
    "\n",
    "# Test encoder\n",
    "state_encoder = DiplomacyStateEncoder(compressed_size=1754)\n",
    "print(f'Raw state size: {state_encoder.raw_size}')\n",
    "print(f'Used state size: {state_encoder.state_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Action Encoder\n",
    "\n",
    "Handles encoding/decoding of Diplomacy orders using the official game's action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiplomacyActionEncoder:\n",
    "    \"\"\"\n",
    "    Encodes Diplomacy orders to indices and vice versa.\n",
    "    \n",
    "    Uses the official diplomacy package to get valid orders,\n",
    "    then maps them to a vocabulary for the neural network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.order_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx_to_order = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.vocab_size = 2\n",
    "        self._build_base_vocab()\n",
    "        \n",
    "    def _build_base_vocab(self):\n",
    "        \"\"\"\n",
    "        Build vocabulary from possible orders.\n",
    "        We generate a comprehensive set covering most game situations.\n",
    "        \"\"\"\n",
    "        idx = 2\n",
    "        \n",
    "        # Generate orders by playing sample games\n",
    "        orders_seen = set()\n",
    "        \n",
    "        # Play random games to collect order vocabulary\n",
    "        for _ in range(50):\n",
    "            game = Game()\n",
    "            for _ in range(30):  # 30 phases max\n",
    "                if game.is_game_done:\n",
    "                    break\n",
    "                \n",
    "                # Get all possible orders for all powers\n",
    "                for power_name in POWERS:\n",
    "                    possible = game.get_all_possible_orders()\n",
    "                    for loc, loc_orders in possible.items():\n",
    "                        for order in loc_orders:\n",
    "                            orders_seen.add(order)\n",
    "                \n",
    "                # Submit random orders\n",
    "                for power_name in POWERS:\n",
    "                    power = game.get_power(power_name)\n",
    "                    possible = game.get_all_possible_orders()\n",
    "                    orders = []\n",
    "                    for unit in power.units:\n",
    "                        loc = unit.split()[-1].split('/')[0]\n",
    "                        if loc in possible and possible[loc]:\n",
    "                            orders.append(random.choice(possible[loc]))\n",
    "                    game.set_orders(power_name, orders)\n",
    "                \n",
    "                game.process()\n",
    "        \n",
    "        # Add all seen orders to vocabulary\n",
    "        for order in sorted(orders_seen):\n",
    "            if order not in self.order_to_idx:\n",
    "                self.order_to_idx[order] = idx\n",
    "                self.idx_to_order[idx] = order\n",
    "                idx += 1\n",
    "        \n",
    "        self.vocab_size = len(self.order_to_idx)\n",
    "        print(f'Built vocabulary with {self.vocab_size} orders')\n",
    "    \n",
    "    def encode(self, order: str) -> int:\n",
    "        \"\"\"Encode order string to index.\"\"\"\n",
    "        return self.order_to_idx.get(order, 1)  # 1 = UNK\n",
    "    \n",
    "    def decode(self, idx: int) -> str:\n",
    "        \"\"\"Decode index to order string.\"\"\"\n",
    "        return self.idx_to_order.get(idx, '<UNK>')\n",
    "    \n",
    "    def get_valid_action_mask(self, game: Game, power_name: str) -> Tuple[List[int], Dict[int, str]]:\n",
    "        \"\"\"\n",
    "        Get mask of valid actions for a power.\n",
    "        \n",
    "        Returns:\n",
    "            (list of valid action indices, mapping from idx to order)\n",
    "        \"\"\"\n",
    "        valid_indices = []\n",
    "        idx_to_order = {}\n",
    "        \n",
    "        power = game.get_power(power_name)\n",
    "        possible = game.get_all_possible_orders()\n",
    "        \n",
    "        for unit in power.units:\n",
    "            loc = unit.split()[-1].split('/')[0]\n",
    "            if loc in possible:\n",
    "                for order in possible[loc]:\n",
    "                    idx = self.encode(order)\n",
    "                    if idx > 1:  # Not PAD or UNK\n",
    "                        valid_indices.append(idx)\n",
    "                        idx_to_order[idx] = order\n",
    "        \n",
    "        # If no valid actions, return hold-equivalent\n",
    "        if not valid_indices:\n",
    "            valid_indices = [1]\n",
    "            \n",
    "        return valid_indices, idx_to_order\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save vocabulary to file.\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump({\n",
    "                'order_to_idx': self.order_to_idx,\n",
    "                'vocab_size': self.vocab_size\n",
    "            }, f)\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load vocabulary from file.\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.order_to_idx = data['order_to_idx']\n",
    "        self.idx_to_order = {int(v): k for k, v in self.order_to_idx.items()}\n",
    "        self.vocab_size = data['vocab_size']\n",
    "\n",
    "# Build action encoder\n",
    "print('Building action vocabulary (this may take a minute)...')\n",
    "action_encoder = DiplomacyActionEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Actor-Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic network for PPO.\n",
    "    \n",
    "    Architecture:\n",
    "    - Shared MLP backbone for feature extraction\n",
    "    - Policy head (Actor): outputs action logits\n",
    "    - Value head (Critic): outputs state value V(s)\n",
    "    \n",
    "    Based on architectures from:\n",
    "    - Schulman et al. (2017) - PPO\n",
    "    - Bakhtin et al. (2022) - Diplodocus\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size: int, action_size: int, \n",
    "                 hidden_sizes: List[int] = [1024, 512, 256]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Shared backbone\n",
    "        layers = []\n",
    "        prev_size = state_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.LayerNorm(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "        \n",
    "        # Policy head (Actor)\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(hidden_sizes[-1], hidden_sizes[-1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[-1], action_size)\n",
    "        )\n",
    "        \n",
    "        # Value head (Critic)\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(hidden_sizes[-1], hidden_sizes[-1] // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[-1] // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.orthogonal_(module.weight, gain=np.sqrt(2))\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        \n",
    "        # Smaller init for output layers\n",
    "        nn.init.orthogonal_(self.policy_head[-1].weight, gain=0.01)\n",
    "        nn.init.orthogonal_(self.value_head[-1].weight, gain=1.0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, \n",
    "                action_mask: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: State tensor (batch_size, state_size)\n",
    "            action_mask: Binary mask for valid actions (batch_size, action_size)\n",
    "            \n",
    "        Returns:\n",
    "            (policy_logits, value)\n",
    "        \"\"\"\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Policy logits\n",
    "        logits = self.policy_head(features)\n",
    "        \n",
    "        # Apply action mask if provided\n",
    "        if action_mask is not None:\n",
    "            # Set invalid actions to very negative value\n",
    "            logits = logits.masked_fill(~action_mask.bool(), float('-inf'))\n",
    "        \n",
    "        # Value\n",
    "        value = self.value_head(features).squeeze(-1)\n",
    "        \n",
    "        return logits, value\n",
    "    \n",
    "    def get_action_and_value(self, state: torch.Tensor,\n",
    "                              valid_actions: List[int] = None,\n",
    "                              deterministic: bool = False) -> Tuple[int, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get action, log prob, entropy, and value for a state.\n",
    "        \n",
    "        Args:\n",
    "            state: State tensor (1, state_size)\n",
    "            valid_actions: List of valid action indices\n",
    "            deterministic: If True, return argmax action\n",
    "            \n",
    "        Returns:\n",
    "            (action, log_prob, entropy, value)\n",
    "        \"\"\"\n",
    "        # Create action mask\n",
    "        action_mask = None\n",
    "        if valid_actions is not None:\n",
    "            action_mask = torch.zeros(1, self.action_size, device=state.device)\n",
    "            action_mask[0, valid_actions] = 1.0\n",
    "        \n",
    "        logits, value = self.forward(state, action_mask)\n",
    "        \n",
    "        # Get probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        \n",
    "        if deterministic:\n",
    "            action = probs.argmax(dim=-1)\n",
    "        else:\n",
    "            action = dist.sample()\n",
    "        \n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        return action.item(), log_prob, entropy, value\n",
    "    \n",
    "    def evaluate_actions(self, states: torch.Tensor, \n",
    "                         actions: torch.Tensor,\n",
    "                         action_masks: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Evaluate actions for PPO update.\n",
    "        \n",
    "        Returns:\n",
    "            (log_probs, entropy, values)\n",
    "        \"\"\"\n",
    "        logits, values = self.forward(states, action_masks)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        \n",
    "        log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        return log_probs, entropy, values\n",
    "\n",
    "# Test network\n",
    "test_net = ActorCriticNetwork(state_encoder.state_size, action_encoder.vocab_size)\n",
    "print(f'Network parameters: {sum(p.numel() for p in test_net.parameters()):,}')\n",
    "del test_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experience Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    \"\"\"\n",
    "    Buffer for storing rollout experiences.\n",
    "    Handles GAE computation and batch generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.action_masks = []\n",
    "        \n",
    "    def add(self, state, action, reward, done, log_prob, value, action_mask=None):\n",
    "        \"\"\"Add experience to buffer.\"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "        self.action_masks.append(action_mask)\n",
    "    \n",
    "    def compute_returns_and_advantages(self, last_value: float, \n",
    "                                        gamma: float = 0.99, \n",
    "                                        gae_lambda: float = 0.95) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute returns and GAE advantages.\n",
    "        \n",
    "        Args:\n",
    "            last_value: Value estimate for final state\n",
    "            gamma: Discount factor\n",
    "            gae_lambda: GAE lambda parameter\n",
    "            \n",
    "        Returns:\n",
    "            (returns, advantages)\n",
    "        \"\"\"\n",
    "        rewards = np.array(self.rewards)\n",
    "        values = np.array(self.values + [last_value])\n",
    "        dones = np.array(self.dones + [True])\n",
    "        \n",
    "        # GAE computation\n",
    "        advantages = np.zeros_like(rewards)\n",
    "        last_gae = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            next_non_terminal = 1.0 - dones[t + 1]\n",
    "            delta = rewards[t] + gamma * values[t + 1] * next_non_terminal - values[t]\n",
    "            last_gae = delta + gamma * gae_lambda * next_non_terminal * last_gae\n",
    "            advantages[t] = last_gae\n",
    "        \n",
    "        returns = advantages + np.array(self.values)\n",
    "        \n",
    "        return returns, advantages\n",
    "    \n",
    "    def get_batches(self, batch_size: int, returns: np.ndarray, \n",
    "                    advantages: np.ndarray) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Generate mini-batches for training.\n",
    "        \"\"\"\n",
    "        n_samples = len(self.states)\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        batches = []\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = min(start + batch_size, n_samples)\n",
    "            batch_indices = indices[start:end]\n",
    "            \n",
    "            batch = {\n",
    "                'states': np.array([self.states[i] for i in batch_indices]),\n",
    "                'actions': np.array([self.actions[i] for i in batch_indices]),\n",
    "                'log_probs': np.array([self.log_probs[i] for i in batch_indices]),\n",
    "                'returns': returns[batch_indices],\n",
    "                'advantages': advantages[batch_indices],\n",
    "            }\n",
    "            batches.append(batch)\n",
    "        \n",
    "        return batches\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear buffer.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.action_masks = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "print('RolloutBuffer defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PPO Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    \"\"\"\n",
    "    PPO Agent implementation.\n",
    "    \n",
    "    Based on:\n",
    "    - Schulman et al. (2017) - Proximal Policy Optimization\n",
    "    - Implementation details from OpenAI baselines\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size: int, action_size: int,\n",
    "                 lr: float = 2.5e-4,\n",
    "                 gamma: float = 0.99,\n",
    "                 gae_lambda: float = 0.95,\n",
    "                 clip_epsilon: float = 0.2,\n",
    "                 value_coef: float = 0.5,\n",
    "                 entropy_coef: float = 0.01,\n",
    "                 max_grad_norm: float = 0.5,\n",
    "                 target_kl: float = 0.03):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.target_kl = target_kl\n",
    "        \n",
    "        # Network\n",
    "        self.network = ActorCriticNetwork(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr, eps=1e-5)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = None  # Set externally if needed\n",
    "        \n",
    "        # Buffer\n",
    "        self.buffer = RolloutBuffer()\n",
    "        \n",
    "        # Stats\n",
    "        self.train_step = 0\n",
    "        \n",
    "    def select_action(self, state: np.ndarray, \n",
    "                      valid_actions: List[int] = None,\n",
    "                      deterministic: bool = False) -> Tuple[int, float, float]:\n",
    "        \"\"\"\n",
    "        Select action given state.\n",
    "        \n",
    "        Returns:\n",
    "            (action_idx, log_prob, value)\n",
    "        \"\"\"\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action, log_prob, entropy, value = self.network.get_action_and_value(\n",
    "                state_t, valid_actions, deterministic\n",
    "            )\n",
    "        \n",
    "        return action, log_prob.item(), value.item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, done, log_prob, value, action_mask=None):\n",
    "        \"\"\"Store transition in buffer.\"\"\"\n",
    "        self.buffer.add(state, action, reward, done, log_prob, value, action_mask)\n",
    "    \n",
    "    def update(self, num_epochs: int = 4, batch_size: int = 64) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Update policy using PPO.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of training metrics\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return {}\n",
    "        \n",
    "        # Compute returns and advantages\n",
    "        returns, advantages = self.buffer.compute_returns_and_advantages(\n",
    "            last_value=0.0,\n",
    "            gamma=self.gamma,\n",
    "            gae_lambda=self.gae_lambda\n",
    "        )\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Training metrics\n",
    "        total_policy_loss = 0\n",
    "        total_value_loss = 0\n",
    "        total_entropy = 0\n",
    "        total_kl = 0\n",
    "        num_updates = 0\n",
    "        \n",
    "        # PPO epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            batches = self.buffer.get_batches(batch_size, returns, advantages)\n",
    "            \n",
    "            for batch in batches:\n",
    "                states_t = torch.FloatTensor(batch['states']).to(device)\n",
    "                actions_t = torch.LongTensor(batch['actions']).to(device)\n",
    "                old_log_probs_t = torch.FloatTensor(batch['log_probs']).to(device)\n",
    "                returns_t = torch.FloatTensor(batch['returns']).to(device)\n",
    "                advantages_t = torch.FloatTensor(batch['advantages']).to(device)\n",
    "                \n",
    "                # Evaluate actions\n",
    "                new_log_probs, entropy, values = self.network.evaluate_actions(\n",
    "                    states_t, actions_t\n",
    "                )\n",
    "                \n",
    "                # Policy loss (clipped PPO objective)\n",
    "                ratio = torch.exp(new_log_probs - old_log_probs_t)\n",
    "                surr1 = ratio * advantages_t\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages_t\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Value loss (clipped)\n",
    "                value_loss = F.mse_loss(values, returns_t)\n",
    "                \n",
    "                # Entropy bonus\n",
    "                entropy_loss = -entropy.mean()\n",
    "                \n",
    "                # Total loss\n",
    "                loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss\n",
    "                \n",
    "                # Update\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Track metrics\n",
    "                total_policy_loss += policy_loss.item()\n",
    "                total_value_loss += value_loss.item()\n",
    "                total_entropy += entropy.mean().item()\n",
    "                \n",
    "                # Approximate KL divergence\n",
    "                with torch.no_grad():\n",
    "                    kl = (old_log_probs_t - new_log_probs).mean().item()\n",
    "                    total_kl += kl\n",
    "                \n",
    "                num_updates += 1\n",
    "            \n",
    "            # Early stopping if KL divergence too high\n",
    "            if total_kl / max(num_updates, 1) > self.target_kl:\n",
    "                break\n",
    "        \n",
    "        # Clear buffer\n",
    "        self.buffer.clear()\n",
    "        self.train_step += 1\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': total_policy_loss / max(num_updates, 1),\n",
    "            'value_loss': total_value_loss / max(num_updates, 1),\n",
    "            'entropy': total_entropy / max(num_updates, 1),\n",
    "            'kl': total_kl / max(num_updates, 1),\n",
    "            'num_updates': num_updates\n",
    "        }\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save agent checkpoint.\"\"\"\n",
    "        torch.save({\n",
    "            'network_state_dict': self.network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'train_step': self.train_step\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load agent checkpoint.\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        self.network.load_state_dict(checkpoint['network_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.train_step = checkpoint.get('train_step', 0)\n",
    "\n",
    "print('PPOAgent defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reward Shaping\n",
    "\n",
    "Critical for learning! Based on Bakhtin et al. (2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardShaper:\n",
    "    \"\"\"\n",
    "    Computes shaped rewards for Diplomacy.\n",
    "    \n",
    "    Based on Bakhtin et al. (2022) reward design:\n",
    "    - Final outcome reward (win/draw/loss)\n",
    "    - Supply center delta reward\n",
    "    - Survival bonus\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 win_reward: float = 1.0,\n",
    "                 draw_reward: float = 0.0,\n",
    "                 loss_reward: float = -1.0,\n",
    "                 elimination_reward: float = -1.0,\n",
    "                 sc_gain_reward: float = 0.1,\n",
    "                 sc_loss_reward: float = -0.1,\n",
    "                 survival_reward: float = 0.01):\n",
    "        \n",
    "        self.win_reward = win_reward\n",
    "        self.draw_reward = draw_reward\n",
    "        self.loss_reward = loss_reward\n",
    "        self.elimination_reward = elimination_reward\n",
    "        self.sc_gain_reward = sc_gain_reward\n",
    "        self.sc_loss_reward = sc_loss_reward\n",
    "        self.survival_reward = survival_reward\n",
    "        \n",
    "        # Track SC counts for delta computation\n",
    "        self.prev_sc_counts = {}\n",
    "    \n",
    "    def reset(self, game: Game):\n",
    "        \"\"\"Reset tracker for new game.\"\"\"\n",
    "        state = game.get_state()\n",
    "        self.prev_sc_counts = {\n",
    "            power: len(state['centers'].get(power, []))\n",
    "            for power in POWERS\n",
    "        }\n",
    "    \n",
    "    def compute_rewards(self, game: Game, done: bool) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute rewards for all powers.\n",
    "        \n",
    "        Args:\n",
    "            game: Current game state\n",
    "            done: Whether game is finished\n",
    "            \n",
    "        Returns:\n",
    "            Dict mapping power name to reward\n",
    "        \"\"\"\n",
    "        rewards = {power: 0.0 for power in POWERS}\n",
    "        state = game.get_state()\n",
    "        centers = state['centers']\n",
    "        units = state['units']\n",
    "        \n",
    "        # Current SC counts\n",
    "        current_sc_counts = {\n",
    "            power: len(centers.get(power, []))\n",
    "            for power in POWERS\n",
    "        }\n",
    "        \n",
    "        # Find winner (if any)\n",
    "        winner = None\n",
    "        for power in POWERS:\n",
    "            if current_sc_counts[power] >= VICTORY_CENTERS:\n",
    "                winner = power\n",
    "                break\n",
    "        \n",
    "        for power in POWERS:\n",
    "            # Check elimination\n",
    "            is_eliminated = len(units.get(power, [])) == 0 and current_sc_counts[power] == 0\n",
    "            \n",
    "            if done:\n",
    "                # Final rewards\n",
    "                if winner == power:\n",
    "                    rewards[power] = self.win_reward\n",
    "                elif winner is not None:\n",
    "                    rewards[power] = self.loss_reward\n",
    "                elif is_eliminated:\n",
    "                    rewards[power] = self.elimination_reward\n",
    "                else:\n",
    "                    # Draw - reward based on relative position\n",
    "                    total_scs = sum(current_sc_counts.values())\n",
    "                    if total_scs > 0:\n",
    "                        share = current_sc_counts[power] / total_scs\n",
    "                        rewards[power] = self.draw_reward + share * 0.5\n",
    "            else:\n",
    "                # Intermediate rewards\n",
    "                if is_eliminated:\n",
    "                    rewards[power] = self.elimination_reward\n",
    "                else:\n",
    "                    # SC delta reward\n",
    "                    sc_delta = current_sc_counts[power] - self.prev_sc_counts.get(power, 0)\n",
    "                    if sc_delta > 0:\n",
    "                        rewards[power] += self.sc_gain_reward * sc_delta\n",
    "                    elif sc_delta < 0:\n",
    "                        rewards[power] += self.sc_loss_reward * abs(sc_delta)\n",
    "                    \n",
    "                    # Survival bonus\n",
    "                    rewards[power] += self.survival_reward\n",
    "        \n",
    "        # Update previous SC counts\n",
    "        self.prev_sc_counts = current_sc_counts.copy()\n",
    "        \n",
    "        return rewards\n",
    "\n",
    "print('RewardShaper defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Self-Play Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfPlayTrainer:\n",
    "    \"\"\"\n",
    "    Self-Play training loop for Diplomacy.\n",
    "    \n",
    "    All 7 powers are controlled by the same policy network.\n",
    "    Experience is collected from all powers' perspectives.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        \n",
    "        # Components\n",
    "        self.state_encoder = state_encoder\n",
    "        self.action_encoder = action_encoder\n",
    "        self.reward_shaper = RewardShaper(\n",
    "            win_reward=config.get('win_reward', 1.0),\n",
    "            sc_gain_reward=config.get('sc_gain_reward', 0.1),\n",
    "            sc_loss_reward=config.get('sc_loss_reward', -0.1),\n",
    "            survival_reward=config.get('survival_reward', 0.02)\n",
    "        )\n",
    "        \n",
    "        # Agent\n",
    "        self.agent = PPOAgent(\n",
    "            state_size=self.state_encoder.state_size,\n",
    "            action_size=self.action_encoder.vocab_size,\n",
    "            lr=config.get('lr', 2.5e-4),\n",
    "            gamma=config.get('gamma', 0.99),\n",
    "            gae_lambda=config.get('gae_lambda', 0.95),\n",
    "            clip_epsilon=config.get('clip_epsilon', 0.2),\n",
    "            entropy_coef=config.get('entropy_coef', 0.01)\n",
    "        )\n",
    "        \n",
    "        # History\n",
    "        self.history = {\n",
    "            'episode_rewards': [],\n",
    "            'episode_lengths': [],\n",
    "            'wins': defaultdict(int),\n",
    "            'draws': 0,\n",
    "            'sc_counts': [],\n",
    "            'policy_loss': [],\n",
    "            'value_loss': [],\n",
    "            'entropy': []\n",
    "        }\n",
    "        \n",
    "        # Tensorboard\n",
    "        self.writer = SummaryWriter(log_dir=f'runs/selfplay_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}')\n",
    "    \n",
    "    def play_game(self, deterministic: bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Play one complete game with self-play.\n",
    "        \n",
    "        Returns:\n",
    "            Game statistics\n",
    "        \"\"\"\n",
    "        game = Game()\n",
    "        self.reward_shaper.reset(game)\n",
    "        \n",
    "        episode_rewards = {p: 0.0 for p in POWERS}\n",
    "        steps = 0\n",
    "        max_steps = self.config.get('max_game_length', 100)\n",
    "        \n",
    "        while not game.is_game_done and steps < max_steps:\n",
    "            current_phase = game.get_current_phase()\n",
    "            \n",
    "            # Collect orders from all powers\n",
    "            for power_name in POWERS:\n",
    "                power = game.get_power(power_name)\n",
    "                \n",
    "                # Skip if no units or eliminated\n",
    "                if not power.units:\n",
    "                    continue\n",
    "                \n",
    "                # Get state encoding\n",
    "                state = self.state_encoder.encode(game, power_name)\n",
    "                \n",
    "                # Get valid actions for each unit\n",
    "                orders = []\n",
    "                possible_orders = game.get_all_possible_orders()\n",
    "                \n",
    "                for unit in power.units:\n",
    "                    # Get unit location\n",
    "                    unit_loc = unit.split()[-1].split('/')[0]\n",
    "                    \n",
    "                    # Get valid orders for this unit\n",
    "                    if unit_loc in possible_orders and possible_orders[unit_loc]:\n",
    "                        unit_orders = possible_orders[unit_loc]\n",
    "                        \n",
    "                        # Encode valid actions\n",
    "                        valid_indices = []\n",
    "                        idx_to_order = {}\n",
    "                        for order in unit_orders:\n",
    "                            idx = self.action_encoder.encode(order)\n",
    "                            if idx > 1:  # Not PAD or UNK\n",
    "                                valid_indices.append(idx)\n",
    "                                idx_to_order[idx] = order\n",
    "                        \n",
    "                        if valid_indices:\n",
    "                            # Select action\n",
    "                            action_idx, log_prob, value = self.agent.select_action(\n",
    "                                state, valid_indices, deterministic\n",
    "                            )\n",
    "                            \n",
    "                            # Get order string\n",
    "                            if action_idx in idx_to_order:\n",
    "                                order = idx_to_order[action_idx]\n",
    "                            else:\n",
    "                                order = random.choice(unit_orders)\n",
    "                                action_idx = self.action_encoder.encode(order)\n",
    "                                log_prob = 0.0\n",
    "                            \n",
    "                            orders.append(order)\n",
    "                            \n",
    "                            # Store experience (we'll add reward after processing)\n",
    "                            self.agent.buffer.add(\n",
    "                                state=state,\n",
    "                                action=action_idx,\n",
    "                                reward=0.0,  # Placeholder\n",
    "                                done=False,\n",
    "                                log_prob=log_prob,\n",
    "                                value=value\n",
    "                            )\n",
    "                        else:\n",
    "                            # Fallback to random\n",
    "                            orders.append(random.choice(unit_orders))\n",
    "                    else:\n",
    "                        # No valid orders - unit holds\n",
    "                        pass\n",
    "                \n",
    "                # Submit orders\n",
    "                game.set_orders(power_name, orders)\n",
    "            \n",
    "            # Process the phase\n",
    "            game.process()\n",
    "            steps += 1\n",
    "            \n",
    "            # Compute rewards\n",
    "            done = game.is_game_done or steps >= max_steps\n",
    "            rewards = self.reward_shaper.compute_rewards(game, done)\n",
    "            \n",
    "            # Update rewards in buffer (for most recent experiences)\n",
    "            # This is a simplification - in practice we'd track per-power\n",
    "            for power_name, reward in rewards.items():\n",
    "                episode_rewards[power_name] += reward\n",
    "            \n",
    "            # Update last experiences with rewards\n",
    "            avg_reward = sum(rewards.values()) / len(rewards)\n",
    "            for i in range(min(7, len(self.agent.buffer))):\n",
    "                idx = len(self.agent.buffer) - 1 - i\n",
    "                if idx >= 0:\n",
    "                    self.agent.buffer.rewards[idx] = avg_reward\n",
    "                    self.agent.buffer.dones[idx] = done\n",
    "        \n",
    "        # Determine winner\n",
    "        winner = None\n",
    "        final_scs = {}\n",
    "        state = game.get_state()\n",
    "        for power_name in POWERS:\n",
    "            sc_count = len(state['centers'].get(power_name, []))\n",
    "            final_scs[power_name] = sc_count\n",
    "            if sc_count >= VICTORY_CENTERS:\n",
    "                winner = power_name\n",
    "        \n",
    "        return {\n",
    "            'winner': winner,\n",
    "            'steps': steps,\n",
    "            'rewards': episode_rewards,\n",
    "            'final_scs': final_scs,\n",
    "            'phase': game.get_current_phase()\n",
    "        }\n",
    "    \n",
    "    def train(self, num_games: int, \n",
    "              update_every: int = 5,\n",
    "              eval_every: int = 50,\n",
    "              save_every: int = 100):\n",
    "        \"\"\"\n",
    "        Main training loop.\n",
    "        \n",
    "        Args:\n",
    "            num_games: Total games to play\n",
    "            update_every: Games between policy updates\n",
    "            eval_every: Games between evaluations\n",
    "            save_every: Games between checkpoint saves\n",
    "        \"\"\"\n",
    "        print('='*60)\n",
    "        print('SELF-PLAY TRAINING')\n",
    "        print('='*60)\n",
    "        print(f'Games: {num_games}')\n",
    "        print(f'Update every: {update_every} games')\n",
    "        print(f'Device: {device}')\n",
    "        print('='*60 + '\\n')\n",
    "        \n",
    "        pbar = tqdm(range(num_games), desc='Training')\n",
    "        \n",
    "        for game_num in pbar:\n",
    "            # Play game\n",
    "            stats = self.play_game()\n",
    "            \n",
    "            # Record stats\n",
    "            total_reward = sum(stats['rewards'].values())\n",
    "            self.history['episode_rewards'].append(total_reward)\n",
    "            self.history['episode_lengths'].append(stats['steps'])\n",
    "            self.history['sc_counts'].append(stats['final_scs'])\n",
    "            \n",
    "            if stats['winner']:\n",
    "                self.history['wins'][stats['winner']] += 1\n",
    "            else:\n",
    "                self.history['draws'] += 1\n",
    "            \n",
    "            # Log to tensorboard\n",
    "            self.writer.add_scalar('Reward/Total', total_reward, game_num)\n",
    "            self.writer.add_scalar('Game/Length', stats['steps'], game_num)\n",
    "            \n",
    "            # Update policy\n",
    "            if (game_num + 1) % update_every == 0 and len(self.agent.buffer) > 0:\n",
    "                metrics = self.agent.update(\n",
    "                    num_epochs=self.config.get('ppo_epochs', 4),\n",
    "                    batch_size=self.config.get('batch_size', 64)\n",
    "                )\n",
    "                \n",
    "                if metrics:\n",
    "                    self.history['policy_loss'].append(metrics['policy_loss'])\n",
    "                    self.history['value_loss'].append(metrics['value_loss'])\n",
    "                    self.history['entropy'].append(metrics['entropy'])\n",
    "                    \n",
    "                    self.writer.add_scalar('Loss/Policy', metrics['policy_loss'], game_num)\n",
    "                    self.writer.add_scalar('Loss/Value', metrics['value_loss'], game_num)\n",
    "                    self.writer.add_scalar('Loss/Entropy', metrics['entropy'], game_num)\n",
    "            \n",
    "            # Update progress bar\n",
    "            recent_rewards = self.history['episode_rewards'][-100:]\n",
    "            recent_lengths = self.history['episode_lengths'][-100:]\n",
    "            total_wins = sum(self.history['wins'].values())\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'reward': f'{np.mean(recent_rewards):.2f}',\n",
    "                'length': f'{np.mean(recent_lengths):.1f}',\n",
    "                'wins': total_wins,\n",
    "                'draws': self.history['draws']\n",
    "            })\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (game_num + 1) % save_every == 0:\n",
    "                self.save_checkpoint(game_num + 1)\n",
    "            \n",
    "            # Evaluation\n",
    "            if (game_num + 1) % eval_every == 0:\n",
    "                self.evaluate(num_games=10)\n",
    "        \n",
    "        self.writer.close()\n",
    "        print('\\nTraining complete!')\n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self, num_games: int = 10):\n",
    "        \"\"\"Evaluate current policy with deterministic actions.\"\"\"\n",
    "        print(f'\\n--- Evaluation ({num_games} games) ---')\n",
    "        \n",
    "        eval_wins = defaultdict(int)\n",
    "        eval_scs = defaultdict(list)\n",
    "        \n",
    "        for _ in range(num_games):\n",
    "            stats = self.play_game(deterministic=True)\n",
    "            self.agent.buffer.clear()  # Don't train on eval games\n",
    "            \n",
    "            if stats['winner']:\n",
    "                eval_wins[stats['winner']] += 1\n",
    "            \n",
    "            for power, scs in stats['final_scs'].items():\n",
    "                eval_scs[power].append(scs)\n",
    "        \n",
    "        # Print results\n",
    "        for power in POWERS:\n",
    "            wins = eval_wins[power]\n",
    "            avg_sc = np.mean(eval_scs[power]) if eval_scs[power] else 0\n",
    "            print(f'  {power}: {wins} wins, avg {avg_sc:.1f} SCs')\n",
    "        \n",
    "        draws = num_games - sum(eval_wins.values())\n",
    "        print(f'  Draws: {draws}')\n",
    "        print()\n",
    "    \n",
    "    def save_checkpoint(self, game_num: int):\n",
    "        \"\"\"Save training checkpoint.\"\"\"\n",
    "        path = f'selfplay_checkpoint_{game_num}.pt'\n",
    "        self.agent.save(path)\n",
    "        print(f'\\nCheckpoint saved: {path}')\n",
    "\n",
    "print('SelfPlayTrainer defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Training\n",
    "    'num_games': 500,  # Start with 500, increase for better results\n",
    "    'max_game_length': 100,  # Max phases per game\n",
    "    'update_every': 5,  # Games between PPO updates\n",
    "    'eval_every': 100,  # Games between evaluations\n",
    "    'save_every': 100,  # Games between saves\n",
    "    \n",
    "    # PPO hyperparameters\n",
    "    'lr': 2.5e-4,\n",
    "    'gamma': 0.99,\n",
    "    'gae_lambda': 0.95,\n",
    "    'clip_epsilon': 0.2,\n",
    "    'entropy_coef': 0.01,\n",
    "    'ppo_epochs': 4,\n",
    "    'batch_size': 64,\n",
    "    \n",
    "    # Reward shaping\n",
    "    'win_reward': 1.0,\n",
    "    'sc_gain_reward': 0.1,\n",
    "    'sc_loss_reward': -0.1,\n",
    "    'survival_reward': 0.02,\n",
    "}\n",
    "\n",
    "print('Training Configuration:')\n",
    "print('-' * 40)\n",
    "for k, v in CONFIG.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Run Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = SelfPlayTrainer(CONFIG)\n",
    "\n",
    "# Train!\n",
    "history = trainer.train(\n",
    "    num_games=CONFIG['num_games'],\n",
    "    update_every=CONFIG['update_every'],\n",
    "    eval_every=CONFIG['eval_every'],\n",
    "    save_every=CONFIG['save_every']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Results & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Episode Rewards\n",
    "ax = axes[0, 0]\n",
    "rewards = history['episode_rewards']\n",
    "ax.plot(rewards, alpha=0.3, color='blue', label='Raw')\n",
    "window = 50\n",
    "if len(rewards) >= window:\n",
    "    ma = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards)), ma, color='red', linewidth=2, label=f'MA-{window}')\n",
    "ax.set_xlabel('Game')\n",
    "ax.set_ylabel('Total Reward')\n",
    "ax.set_title('Episode Rewards')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Game Lengths\n",
    "ax = axes[0, 1]\n",
    "lengths = history['episode_lengths']\n",
    "ax.plot(lengths, alpha=0.3, color='green', label='Raw')\n",
    "if len(lengths) >= window:\n",
    "    ma = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(lengths)), ma, color='red', linewidth=2, label=f'MA-{window}')\n",
    "ax.set_xlabel('Game')\n",
    "ax.set_ylabel('Phases')\n",
    "ax.set_title('Game Length')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Win Distribution\n",
    "ax = axes[1, 0]\n",
    "wins = history['wins']\n",
    "categories = POWERS + ['Draw']\n",
    "counts = [wins.get(p, 0) for p in POWERS] + [history['draws']]\n",
    "colors = plt.cm.Set3(range(len(categories)))\n",
    "bars = ax.bar(categories, counts, color=colors, edgecolor='black')\n",
    "ax.set_xlabel('Outcome')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Win Distribution')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars, counts):\n",
    "    if count > 0:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                str(count), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 4. Training Losses\n",
    "ax = axes[1, 1]\n",
    "if history['policy_loss']:\n",
    "    ax.plot(history['policy_loss'], label='Policy Loss', color='purple')\n",
    "    ax.plot(history['value_loss'], label='Value Loss', color='orange')\n",
    "    ax.set_xlabel('Update')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training Losses')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('selfplay_training_results.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final statistics\n",
    "print('='*60)\n",
    "print('TRAINING SUMMARY')\n",
    "print('='*60)\n",
    "\n",
    "print(f\"\\nTotal games: {CONFIG['num_games']}\")\n",
    "print(f\"Average reward (last 100): {np.mean(history['episode_rewards'][-100:]):.3f}\")\n",
    "print(f\"Average length (last 100): {np.mean(history['episode_lengths'][-100:]):.1f} phases\")\n",
    "\n",
    "print(f\"\\nWin Distribution:\")\n",
    "total_wins = sum(history['wins'].values())\n",
    "for power in POWERS:\n",
    "    count = history['wins'].get(power, 0)\n",
    "    pct = count / CONFIG['num_games'] * 100\n",
    "    print(f\"  {power}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"  Draws: {history['draws']} ({history['draws']/CONFIG['num_games']*100:.1f}%)\")\n",
    "\n",
    "# Win rate (non-draw games)\n",
    "if total_wins > 0:\n",
    "    print(f\"\\nWin rate among decisive games: {total_wins/(total_wins+history['draws'])*100:.1f}%\")\n",
    "\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Evaluate vs Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vs_random(trainer, agent_power: str = 'FRANCE', num_games: int = 20):\n",
    "    \"\"\"\n",
    "    Evaluate trained agent vs random opponents.\n",
    "    Agent plays as one power, all others play randomly.\n",
    "    \"\"\"\n",
    "    print(f'\\nEvaluating {agent_power} vs Random ({num_games} games)...')\n",
    "    \n",
    "    wins = 0\n",
    "    total_scs = []\n",
    "    \n",
    "    for game_idx in tqdm(range(num_games), desc='Eval vs Random'):\n",
    "        game = Game()\n",
    "        steps = 0\n",
    "        \n",
    "        while not game.is_game_done and steps < 100:\n",
    "            possible_orders = game.get_all_possible_orders()\n",
    "            \n",
    "            for power_name in POWERS:\n",
    "                power = game.get_power(power_name)\n",
    "                if not power.units:\n",
    "                    continue\n",
    "                \n",
    "                orders = []\n",
    "                \n",
    "                if power_name == agent_power:\n",
    "                    # Agent's turn - use trained policy\n",
    "                    state = trainer.state_encoder.encode(game, power_name)\n",
    "                    \n",
    "                    for unit in power.units:\n",
    "                        unit_loc = unit.split()[-1].split('/')[0]\n",
    "                        if unit_loc in possible_orders and possible_orders[unit_loc]:\n",
    "                            unit_orders = possible_orders[unit_loc]\n",
    "                            \n",
    "                            # Get valid indices\n",
    "                            valid_indices = []\n",
    "                            idx_to_order = {}\n",
    "                            for order in unit_orders:\n",
    "                                idx = trainer.action_encoder.encode(order)\n",
    "                                if idx > 1:\n",
    "                                    valid_indices.append(idx)\n",
    "                                    idx_to_order[idx] = order\n",
    "                            \n",
    "                            if valid_indices:\n",
    "                                action_idx, _, _ = trainer.agent.select_action(\n",
    "                                    state, valid_indices, deterministic=True\n",
    "                                )\n",
    "                                if action_idx in idx_to_order:\n",
    "                                    orders.append(idx_to_order[action_idx])\n",
    "                                else:\n",
    "                                    orders.append(random.choice(unit_orders))\n",
    "                            else:\n",
    "                                orders.append(random.choice(unit_orders))\n",
    "                else:\n",
    "                    # Random opponent\n",
    "                    for unit in power.units:\n",
    "                        unit_loc = unit.split()[-1].split('/')[0]\n",
    "                        if unit_loc in possible_orders and possible_orders[unit_loc]:\n",
    "                            orders.append(random.choice(possible_orders[unit_loc]))\n",
    "                \n",
    "                game.set_orders(power_name, orders)\n",
    "            \n",
    "            game.process()\n",
    "            steps += 1\n",
    "        \n",
    "        # Check result\n",
    "        state = game.get_state()\n",
    "        agent_scs = len(state['centers'].get(agent_power, []))\n",
    "        total_scs.append(agent_scs)\n",
    "        \n",
    "        # Check for win\n",
    "        if agent_scs >= VICTORY_CENTERS:\n",
    "            wins += 1\n",
    "        else:\n",
    "            # Check if agent has most SCs\n",
    "            max_scs = max(len(state['centers'].get(p, [])) for p in POWERS)\n",
    "            if agent_scs == max_scs and agent_scs > 0:\n",
    "                # Count as win if tied for first\n",
    "                pass\n",
    "    \n",
    "    win_rate = wins / num_games\n",
    "    avg_scs = np.mean(total_scs)\n",
    "    \n",
    "    print(f'\\nResults:')\n",
    "    print(f'  Win rate: {win_rate*100:.1f}% ({wins}/{num_games})')\n",
    "    print(f'  Average SCs: {avg_scs:.1f}')\n",
    "    \n",
    "    return win_rate, avg_scs\n",
    "\n",
    "# Run evaluation\n",
    "win_rate, avg_scs = evaluate_vs_random(trainer, agent_power='FRANCE', num_games=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Final Model & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "trainer.agent.save('selfplay_final_model.pt')\n",
    "\n",
    "# Save action encoder vocabulary\n",
    "trainer.action_encoder.save('action_vocab.json')\n",
    "\n",
    "# Save training history\n",
    "history_save = {\n",
    "    'episode_rewards': history['episode_rewards'],\n",
    "    'episode_lengths': history['episode_lengths'],\n",
    "    'wins': dict(history['wins']),\n",
    "    'draws': history['draws'],\n",
    "    'policy_loss': history['policy_loss'],\n",
    "    'value_loss': history['value_loss'],\n",
    "    'entropy': history['entropy'],\n",
    "    'config': CONFIG,\n",
    "    'eval_vs_random': {\n",
    "        'win_rate': win_rate,\n",
    "        'avg_scs': avg_scs\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('training_history.json', 'w') as f:\n",
    "    json.dump(history_save, f, indent=2)\n",
    "\n",
    "print('Saved files:')\n",
    "print('  - selfplay_final_model.pt')\n",
    "print('  - action_vocab.json')\n",
    "print('  - training_history.json')\n",
    "print('  - selfplay_training_results.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download('selfplay_final_model.pt')\n",
    "files.download('action_vocab.json')\n",
    "files.download('training_history.json')\n",
    "files.download('selfplay_training_results.png')\n",
    "\n",
    "print('\\nFiles downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Summary & Next Steps\n",
    "\n",
    "### What We Built\n",
    "- Complete self-play RL pipeline using official `diplomacy` package\n",
    "- PPO algorithm with proper state encoding and action masking\n",
    "- Reward shaping for faster learning\n",
    "- Evaluation framework vs random baseline\n",
    "\n",
    "### Key Results\n",
    "- Win distribution across powers\n",
    "- Learning curves (rewards, game length, losses)\n",
    "- Performance vs random opponents\n",
    "\n",
    "### Addressing RQ1: Overfitting in Pure Self-Play\n",
    "To fully quantify overfitting:\n",
    "1. Evaluate against **checkpoints from earlier training**\n",
    "2. Evaluate against **diverse opponent policies**\n",
    "3. Check for **strategy collapse** (narrow strategy distribution)\n",
    "\n",
    "### Next Steps\n",
    "1. **Human-Regularized RL (RQ2)**: Add KL divergence penalty toward BC policy\n",
    "2. **Population-Based Training (RQ3)**: Train against diverse opponents\n",
    "3. **Full Evaluation**: Compare self-play vs human-regularized vs population-based"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {"name": "python",
"version": "3.10.0"
}
},
"nbformat": 4,
"nbformat_minor": 0
}
