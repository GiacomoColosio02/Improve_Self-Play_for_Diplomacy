{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population-Based Training for No-Press Diplomacy\n",
    "## Training Against Diverse Opponents for Robust Generalization\n",
    "\n",
    "**Project:** Improve Self-Play for Diplomacy  \n",
    "**Authors:** Giacomo Colosio, Maciej Tasarz, Jakub Seliga, Luka Ivcevic  \n",
    "**Course:** ISP - UPC Barcelona, Fall 2025/26\n",
    "\n",
    "---\n",
    "\n",
    "## Research Question (RQ3)\n",
    "\n",
    "**Does exposing the agent to a diverse population of opponents during training improve robustness and generalization?**\n",
    "\n",
    "---\n",
    "\n",
    "## Why Population-Based Training?\n",
    "\n",
    "### The Problem with Pure Self-Play\n",
    "\n",
    "When training against only itself, an agent:\n",
    "- Develops narrow strategies that work only against itself\n",
    "- Fails against novel strategies it never encountered\n",
    "- Experiences \"strategy collapse\" - converging to exploitable equilibria\n",
    "\n",
    "### The Solution\n",
    "\n",
    "Train against a **diverse population** of opponents:\n",
    "\n",
    "| Opponent Type | Purpose |\n",
    "|---------------|----------|\n",
    "| Random | Baseline, prevents catastrophic failures |\n",
    "| BC (Human-like) | Exposes to human strategies |\n",
    "| Past Checkpoints | Prevents forgetting old strategies |\n",
    "| Current Self | Continues improvement |\n",
    "\n",
    "### Prioritized Fictitious Self-Play (PFSP)\n",
    "\n",
    "We sample opponents with probability:\n",
    "\n",
    "$$P(\\text{opponent}_i) \\propto (1 - \\text{win\\_rate}_i)^p$$\n",
    "\n",
    "This focuses training on opponents we struggle against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diplomacy torch numpy matplotlib tqdm --quiet\n",
    "print('Installation complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "from diplomacy import Game\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POWERS = ['AUSTRIA', 'ENGLAND', 'FRANCE', 'GERMANY', 'ITALY', 'RUSSIA', 'TURKEY']\n",
    "NUM_POWERS = 7\n",
    "LOCATIONS = ['ANK','BEL','BER','BRE','BUD','BUL','CON','DEN','EDI','GRE','HOL','KIE','LON','LVP','MAR','MOS','MUN','NAP','NWY','PAR','POR','ROM','RUM','SER','SEV','SMY','SPA','STP','SWE','TRI','TUN','VEN','VIE','WAR','ALB','APU','ARM','BOH','BUR','CLY','FIN','GAL','GAS','LVN','NAF','PIC','PIE','PRU','RUH','SIL','SYR','TUS','TYR','UKR','WAL','YOR','ADR','AEG','BAL','BAR','BLA','BOT','EAS','ENG','GOL','HEL','ION','IRI','MAO','NAO','NTH','NWG','SKA','TYS','WES']\n",
    "SUPPLY_CENTERS = set(LOCATIONS[:34])\n",
    "VICTORY_CENTERS = 18\n",
    "POWER_TO_IDX = {p: i for i, p in enumerate(POWERS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "print(\"Upload 'standard_no_press.jsonl':\")\n",
    "uploaded = files.upload()\n",
    "DATA_PATH = 'standard_no_press.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateEncoder:\n",
    "    def __init__(self): self.state_size = 1216\n",
    "    def encode_game(self, game, power): return self._encode(game.get_state(), game.get_current_phase(), power)\n",
    "    def encode_json(self, state, phase, power): return self._encode(state, phase, power)\n",
    "    def _encode(self, state, phase, power):\n",
    "        f = np.zeros(self.state_size, dtype=np.float32)\n",
    "        pi = POWER_TO_IDX.get(power, 0)\n",
    "        units, centers = state.get('units', {}), state.get('centers', {})\n",
    "        um = {}\n",
    "        for p, pu in units.items():\n",
    "            if pu:\n",
    "                for u in pu:\n",
    "                    parts = u.split()\n",
    "                    if len(parts) >= 2: um[parts[1].split('/')[0]] = (p, parts[0])\n",
    "        for li, loc in enumerate(LOCATIONS):\n",
    "            o = li * 16\n",
    "            if loc in um:\n",
    "                p, ut = um[loc]\n",
    "                if p in POWER_TO_IDX:\n",
    "                    ri = (POWER_TO_IDX[p] - pi) % NUM_POWERS\n",
    "                    f[o + ri] = 1.0\n",
    "                    f[o + 7] = 1.0 if ut == 'A' else 0.0\n",
    "            if loc in SUPPLY_CENTERS:\n",
    "                f[o + 15] = 1.0\n",
    "                for p, pc in centers.items():\n",
    "                    if pc and loc in pc and p in POWER_TO_IDX:\n",
    "                        f[o + 8 + (POWER_TO_IDX[p] - pi) % NUM_POWERS] = 1.0\n",
    "                        break\n",
    "        g = 1200\n",
    "        for p in POWERS:\n",
    "            ri = (POWER_TO_IDX[p] - pi) % NUM_POWERS\n",
    "            f[g + ri] = len(centers.get(p, []) or []) / VICTORY_CENTERS\n",
    "            f[g + 7 + ri] = len(units.get(p, []) or []) / 17.0\n",
    "        if phase:\n",
    "            try: f[g + 14] = (int(phase[1:5]) - 1901) / 20.0\n",
    "            except: pass\n",
    "            f[g + 15] = {'S': 0.0, 'F': 0.5, 'W': 1.0}.get(phase[0], 0.0)\n",
    "        return f\n",
    "\n",
    "state_encoder = StateEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionEncoder:\n",
    "    def __init__(self):\n",
    "        self.order_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx_to_order = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.vocab_size = 2\n",
    "    def build_vocab(self, games, max_vocab=15000):\n",
    "        counts = Counter()\n",
    "        for game in tqdm(games, desc='Building vocab'):\n",
    "            for phase in game.get('phases', []):\n",
    "                orders = phase.get('orders', {})\n",
    "                if orders:\n",
    "                    for po in orders.values():\n",
    "                        if po:\n",
    "                            for o in po:\n",
    "                                n = re.sub(r'/[A-Z]{2}', '', o.strip().upper())\n",
    "                                if len(n) >= 3: counts[n] += 1\n",
    "        for _ in range(20):\n",
    "            g = Game()\n",
    "            for _ in range(30):\n",
    "                if g.is_game_done: break\n",
    "                for loc, lo in g.get_all_possible_orders().items():\n",
    "                    for o in lo:\n",
    "                        n = re.sub(r'/[A-Z]{2}', '', o.strip().upper())\n",
    "                        if len(n) >= 3: counts[n] += 1\n",
    "                for p in POWERS:\n",
    "                    pw = g.get_power(p)\n",
    "                    pos = g.get_all_possible_orders()\n",
    "                    ords = [random.choice(pos[u.split()[-1].split('/')[0]]) for u in pw.units if u.split()[-1].split('/')[0] in pos and pos[u.split()[-1].split('/')[0]]]\n",
    "                    g.set_orders(p, ords)\n",
    "                g.process()\n",
    "        idx = 2\n",
    "        for o, _ in counts.most_common(max_vocab - 2):\n",
    "            self.order_to_idx[o] = idx\n",
    "            self.idx_to_order[idx] = o\n",
    "            idx += 1\n",
    "        self.vocab_size = len(self.order_to_idx)\n",
    "        print(f'Vocab: {self.vocab_size}')\n",
    "    def encode(self, o): return self.order_to_idx.get(re.sub(r'/[A-Z]{2}', '', o.strip().upper()), 1)\n",
    "    def decode(self, i): return self.idx_to_order.get(i, '<UNK>')\n",
    "    def get_valid(self, game, power):\n",
    "        valid, im = [], {}\n",
    "        pw = game.get_power(power)\n",
    "        pos = game.get_all_possible_orders()\n",
    "        for u in pw.units:\n",
    "            loc = u.split()[-1].split('/')[0]\n",
    "            if loc in pos:\n",
    "                for o in pos[loc]:\n",
    "                    i = self.encode(o)\n",
    "                    if i > 1: valid.append(i); im[i] = o\n",
    "        return valid if valid else [1], im\n",
    "\n",
    "action_encoder = ActionEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, ss, as_):\n",
    "        super().__init__()\n",
    "        self.action_size = as_\n",
    "        self.net = nn.Sequential(nn.Linear(ss, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.1),\n",
    "                                 nn.Linear(512, 512), nn.LayerNorm(512), nn.ReLU(), nn.Dropout(0.1),\n",
    "                                 nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU(), nn.Linear(256, as_))\n",
    "    def forward(self, x, mask=None):\n",
    "        logits = self.net(x)\n",
    "        if mask is not None: logits = logits.masked_fill(~mask.bool(), float('-inf'))\n",
    "        return logits\n",
    "    def get_probs(self, x, mask=None): return F.softmax(self.forward(x, mask), dim=-1)\n",
    "    def get_action(self, s, valid=None, det=False):\n",
    "        mask = None\n",
    "        if valid:\n",
    "            mask = torch.zeros(1, self.action_size, device=s.device)\n",
    "            mask[0, valid] = 1.0\n",
    "        probs = self.get_probs(s, mask)\n",
    "        action = probs.argmax(-1) if det else Categorical(probs).sample()\n",
    "        return action.item(), torch.log(probs[0, action] + 1e-10)\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, ss):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(ss, 512), nn.LayerNorm(512), nn.ReLU(), nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU(), nn.Linear(256, 1))\n",
    "    def forward(self, x): return self.net(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent classes\n",
    "class BaseAgent(ABC):\n",
    "    @abstractmethod\n",
    "    def get_orders(self, game, power): pass\n",
    "    @property\n",
    "    def name(self): return self.__class__.__name__\n",
    "\n",
    "class RandomAgent(BaseAgent):\n",
    "    \"\"\"Makes uniformly random legal moves.\"\"\"\n",
    "    def get_orders(self, game, power):\n",
    "        pw = game.get_power(power)\n",
    "        pos = game.get_all_possible_orders()\n",
    "        return [random.choice(pos[u.split()[-1].split('/')[0]]) for u in pw.units if u.split()[-1].split('/')[0] in pos and pos[u.split()[-1].split('/')[0]]]\n",
    "\n",
    "class PolicyAgent(BaseAgent):\n",
    "    \"\"\"Uses neural network policy.\"\"\"\n",
    "    def __init__(self, policy, se, ae, det=True, name=None):\n",
    "        self.policy, self.se, self.ae, self.det = policy, se, ae, det\n",
    "        self._name = name or 'PolicyAgent'\n",
    "    @property\n",
    "    def name(self): return self._name\n",
    "    def get_orders(self, game, power):\n",
    "        pw = game.get_power(power)\n",
    "        if not pw.units: return []\n",
    "        state = torch.FloatTensor(self.se.encode_game(game, power)).unsqueeze(0).to(device)\n",
    "        pos = game.get_all_possible_orders()\n",
    "        orders = []\n",
    "        self.policy.eval()\n",
    "        with torch.no_grad():\n",
    "            for u in pw.units:\n",
    "                loc = u.split()[-1].split('/')[0]\n",
    "                if loc in pos and pos[loc]:\n",
    "                    vi, im = [], {}\n",
    "                    for o in pos[loc]:\n",
    "                        i = self.ae.encode(o)\n",
    "                        if i > 1: vi.append(i); im[i] = o\n",
    "                    if vi:\n",
    "                        ai, _ = self.policy.get_action(state, vi, self.det)\n",
    "                        orders.append(im.get(ai, random.choice(pos[loc])))\n",
    "                    else: orders.append(random.choice(pos[loc]))\n",
    "        return orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopulationManager:\n",
    "    \"\"\"Manages diverse opponent population with PFSP sampling.\"\"\"\n",
    "    def __init__(self, p_power=0.5):\n",
    "        self.agents = {}\n",
    "        self.base_weights = {}\n",
    "        self.win_rates = defaultdict(list)\n",
    "        self.p_power = p_power\n",
    "        self.games_played = defaultdict(int)\n",
    "    \n",
    "    def add_agent(self, name, agent, weight=1.0):\n",
    "        self.agents[name] = agent\n",
    "        self.base_weights[name] = weight\n",
    "        print(f'  Added {name} (weight={weight})')\n",
    "    \n",
    "    def add_checkpoint(self, policy, se, ae, name, weight=0.5):\n",
    "        cp = PolicyNetwork(se.state_size, ae.vocab_size).to(device)\n",
    "        cp.load_state_dict(copy.deepcopy(policy.state_dict()))\n",
    "        cp.eval()\n",
    "        for p in cp.parameters(): p.requires_grad = False\n",
    "        self.add_agent(name, PolicyAgent(cp, se, ae, det=True, name=name), weight)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        w = {}\n",
    "        for n, bw in self.base_weights.items():\n",
    "            wr = np.mean(self.win_rates[n][-20:]) if self.win_rates[n] else 0.5\n",
    "            w[n] = bw * ((1 - wr) ** self.p_power)\n",
    "        total = sum(w.values())\n",
    "        return {k: v/total for k, v in w.items()} if total > 0 else w\n",
    "    \n",
    "    def sample(self):\n",
    "        w = self.get_weights()\n",
    "        names = list(w.keys())\n",
    "        chosen = np.random.choice(names, p=[w[n] for n in names])\n",
    "        return chosen, self.agents[chosen]\n",
    "    \n",
    "    def record(self, name, won):\n",
    "        self.win_rates[name].append(1.0 if won else 0.0)\n",
    "        self.games_played[name] += 1\n",
    "    \n",
    "    def print_stats(self):\n",
    "        print('\\n' + '='*50)\n",
    "        print('POPULATION STATS')\n",
    "        print('='*50)\n",
    "        for n in self.agents:\n",
    "            wr = np.mean(self.win_rates[n][-50:]) if self.win_rates[n] else 0\n",
    "            gp = self.games_played[n]\n",
    "            sw = self.get_weights().get(n, 0)\n",
    "            print(f'  {n:20s}: WR={wr:.1%}, Games={gp}, Weight={sw:.3f}')\n",
    "        print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, ss, as_, lr=3e-4, gamma=0.995, gae=0.98, clip=0.2, ent=0.02):\n",
    "        self.gamma, self.gae_lambda, self.clip, self.ent = gamma, gae, clip, ent\n",
    "        self.policy = PolicyNetwork(ss, as_).to(device)\n",
    "        self.value = ValueNetwork(ss).to(device)\n",
    "        self.p_opt = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.v_opt = optim.Adam(self.value.parameters(), lr=lr)\n",
    "        self.buffer = []\n",
    "    \n",
    "    def select_action(self, state, valid=None, det=False):\n",
    "        st = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            a, lp = self.policy.get_action(st, valid, det)\n",
    "            v = self.value(st).item()\n",
    "        return a, lp.item(), v\n",
    "    \n",
    "    def store(self, s, a, r, d, lp, v): self.buffer.append({'s': s, 'a': a, 'r': r, 'd': d, 'lp': lp, 'v': v})\n",
    "    \n",
    "    def update(self, epochs=4, bs=128):\n",
    "        if len(self.buffer) < bs: return {}\n",
    "        states = np.array([t['s'] for t in self.buffer])\n",
    "        actions = np.array([t['a'] for t in self.buffer])\n",
    "        rewards = [t['r'] for t in self.buffer]\n",
    "        dones = [t['d'] for t in self.buffer]\n",
    "        old_lps = np.array([t['lp'] for t in self.buffer])\n",
    "        values = [t['v'] for t in self.buffer] + [0]\n",
    "        \n",
    "        advs, rets = [], []\n",
    "        gae = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * values[t+1] * (1 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
    "            advs.insert(0, gae)\n",
    "            rets.insert(0, gae + values[t])\n",
    "        \n",
    "        st = torch.FloatTensor(states).to(device)\n",
    "        at = torch.LongTensor(actions).to(device)\n",
    "        olp = torch.FloatTensor(old_lps).to(device)\n",
    "        advt = torch.FloatTensor(advs).to(device)\n",
    "        rett = torch.FloatTensor(rets).to(device)\n",
    "        advt = (advt - advt.mean()) / (advt.std() + 1e-8)\n",
    "        \n",
    "        metrics = {'policy_loss': 0, 'value_loss': 0}\n",
    "        n_upd = 0\n",
    "        for _ in range(epochs):\n",
    "            idx = np.random.permutation(len(self.buffer))\n",
    "            for start in range(0, len(idx), bs):\n",
    "                b = idx[start:start+bs]\n",
    "                logits = self.policy(st[b])\n",
    "                probs = F.softmax(logits, -1)\n",
    "                dist = Categorical(probs)\n",
    "                new_lp = dist.log_prob(at[b])\n",
    "                ratio = torch.exp(new_lp - olp[b])\n",
    "                s1 = ratio * advt[b]\n",
    "                s2 = torch.clamp(ratio, 1-self.clip, 1+self.clip) * advt[b]\n",
    "                ploss = -torch.min(s1, s2).mean() - self.ent * dist.entropy().mean()\n",
    "                self.p_opt.zero_grad(); ploss.backward(); self.p_opt.step()\n",
    "                vloss = F.mse_loss(self.value(st[b]), rett[b])\n",
    "                self.v_opt.zero_grad(); vloss.backward(); self.v_opt.step()\n",
    "                metrics['policy_loss'] += ploss.item()\n",
    "                metrics['value_loss'] += vloss.item()\n",
    "                n_upd += 1\n",
    "        self.buffer = []\n",
    "        return {k: v/max(n_upd,1) for k,v in metrics.items()}\n",
    "    \n",
    "    def save(self, p): torch.save({'policy': self.policy.state_dict(), 'value': self.value.state_dict()}, p)\n",
    "    def load(self, p):\n",
    "        c = torch.load(p, map_location=device)\n",
    "        self.policy.load_state_dict(c['policy'])\n",
    "        self.value.load_state_dict(c['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardShaper:\n",
    "    def __init__(self, win=10.0, sc_gain=0.5, sc_loss=-0.3):\n",
    "        self.win, self.sc_gain, self.sc_loss = win, sc_gain, sc_loss\n",
    "        self.prev = {}\n",
    "    def reset(self, game): self.prev = {p: len(game.get_state()['centers'].get(p, [])) for p in POWERS}\n",
    "    def compute(self, game, done):\n",
    "        curr = {p: len(game.get_state()['centers'].get(p, [])) for p in POWERS}\n",
    "        winner = next((p for p in POWERS if curr[p] >= VICTORY_CENTERS), None)\n",
    "        rewards = {}\n",
    "        for p in POWERS:\n",
    "            if done and winner == p: rewards[p] = self.win\n",
    "            elif done and winner: rewards[p] = -self.win / 6\n",
    "            else:\n",
    "                d = curr[p] - self.prev.get(p, 0)\n",
    "                rewards[p] = self.sc_gain * max(d, 0) + self.sc_loss * max(-d, 0) + 0.01\n",
    "        self.prev = curr\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data & build vocab\n",
    "MAX_GAMES = 5000\n",
    "games = []\n",
    "with open(DATA_PATH, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= MAX_GAMES: break\n",
    "        games.append(json.loads(line))\n",
    "print(f'Loaded {len(games)} games')\n",
    "action_encoder.build_vocab(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BC agent for population\n",
    "class BCDataset(Dataset):\n",
    "    def __init__(self, games, se, ae):\n",
    "        self.samples = []\n",
    "        for g in tqdm(games, desc='BC Dataset'):\n",
    "            for ph in g.get('phases', []):\n",
    "                if not ph.get('name', '').endswith('M'): continue\n",
    "                st, ords = ph.get('state', {}), ph.get('orders', {})\n",
    "                if not ords: continue\n",
    "                for p in POWERS:\n",
    "                    po = ords.get(p, [])\n",
    "                    if not po: continue\n",
    "                    es = se.encode_json(st, ph['name'], p)\n",
    "                    for o in po:\n",
    "                        i = ae.encode(o)\n",
    "                        if i > 1: self.samples.append({'s': es, 'a': i})\n",
    "        print(f'BC samples: {len(self.samples)}')\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, i): return torch.FloatTensor(self.samples[i]['s']), torch.LongTensor([self.samples[i]['a']])\n",
    "\n",
    "bc_data = BCDataset(games, state_encoder, action_encoder)\n",
    "bc_loader = DataLoader(bc_data, batch_size=256, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_policy = PolicyNetwork(state_encoder.state_size, action_encoder.vocab_size).to(device)\n",
    "bc_opt = optim.AdamW(bc_policy.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print('Training BC for population...')\n",
    "for epoch in range(8):\n",
    "    bc_policy.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for states, actions in tqdm(bc_loader, desc=f'Epoch {epoch+1}', leave=False):\n",
    "        states, actions = states.to(device), actions.squeeze(1).to(device)\n",
    "        bc_opt.zero_grad()\n",
    "        loss = criterion(bc_policy(states), actions)\n",
    "        loss.backward()\n",
    "        bc_opt.step()\n",
    "        total_loss += loss.item()\n",
    "        correct += (bc_policy(states).argmax(1) == actions).sum().item()\n",
    "        total += actions.size(0)\n",
    "    print(f'Epoch {epoch+1}: Loss={total_loss/len(bc_loader):.4f}, Acc={correct/total:.4f}')\n",
    "\n",
    "bc_policy.eval()\n",
    "for p in bc_policy.parameters(): p.requires_grad = False\n",
    "print('BC trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize population\n",
    "population = PopulationManager(p_power=0.5)\n",
    "print('\\nInitializing population...')\n",
    "population.add_agent('Random', RandomAgent(), weight=0.15)\n",
    "population.add_agent('BC', PolicyAgent(bc_policy, state_encoder, action_encoder, det=True, name='BC'), weight=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'num_games': 800,\n",
    "    'max_length': 200,\n",
    "    'update_every': 10,\n",
    "    'checkpoint_every': 200,\n",
    "    'main_power': 'FRANCE',\n",
    "    'win_reward': 10.0,\n",
    "    'sc_gain': 0.5\n",
    "}\n",
    "\n",
    "main_agent = PPOAgent(state_encoder.state_size, action_encoder.vocab_size, lr=3e-4, ent=0.02)\n",
    "main_agent.policy.load_state_dict(bc_policy.state_dict())\n",
    "for p in main_agent.policy.parameters(): p.requires_grad = True\n",
    "\n",
    "reward_shaper = RewardShaper(CONFIG['win_reward'], CONFIG['sc_gain'])\n",
    "history = {'rewards': [], 'lengths': [], 'wins': defaultdict(int), 'games_vs': defaultdict(int), 'policy_loss': []}\n",
    "print('Config:', CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('POPULATION-BASED TRAINING')\n",
    "print('='*60)\n",
    "\n",
    "pbar = tqdm(range(CONFIG['num_games']), desc='PBT')\n",
    "for gn in pbar:\n",
    "    game = Game()\n",
    "    reward_shaper.reset(game)\n",
    "    other_powers = [p for p in POWERS if p != CONFIG['main_power']]\n",
    "    opps = {p: population.sample() for p in other_powers}\n",
    "    opp_types = set(n for n, _ in opps.values())\n",
    "    ep_reward, steps = 0, 0\n",
    "    \n",
    "    while not game.is_game_done and steps < CONFIG['max_length']:\n",
    "        for pwr in POWERS:\n",
    "            pw = game.get_power(pwr)\n",
    "            if not pw.units: continue\n",
    "            if pwr == CONFIG['main_power']:\n",
    "                state = state_encoder.encode_game(game, pwr)\n",
    "                pos = game.get_all_possible_orders()\n",
    "                orders = []\n",
    "                for u in pw.units:\n",
    "                    loc = u.split()[-1].split('/')[0]\n",
    "                    if loc in pos and pos[loc]:\n",
    "                        vi, im = action_encoder.get_valid(game, pwr)\n",
    "                        a, lp, v = main_agent.select_action(state, vi)\n",
    "                        orders.append(im.get(a, random.choice(pos[loc])))\n",
    "                        main_agent.store(state, a, 0, False, lp, v)\n",
    "                game.set_orders(pwr, orders)\n",
    "            else:\n",
    "                _, opp = opps[pwr]\n",
    "                game.set_orders(pwr, opp.get_orders(game, pwr))\n",
    "        \n",
    "        game.process()\n",
    "        steps += 1\n",
    "        done = game.is_game_done or steps >= CONFIG['max_length']\n",
    "        rewards = reward_shaper.compute(game, done)\n",
    "        mr = rewards[CONFIG['main_power']]\n",
    "        ep_reward += mr\n",
    "        for i in range(min(len(pw.units) if pw.units else 1, len(main_agent.buffer))):\n",
    "            idx = len(main_agent.buffer) - 1 - i\n",
    "            if idx >= 0: main_agent.buffer[idx]['r'] = mr; main_agent.buffer[idx]['d'] = done\n",
    "    \n",
    "    # Record\n",
    "    history['rewards'].append(ep_reward)\n",
    "    history['lengths'].append(steps)\n",
    "    state = game.get_state()\n",
    "    winner = next((p for p in POWERS if len(state['centers'].get(p, [])) >= VICTORY_CENTERS), None)\n",
    "    main_won = winner == CONFIG['main_power']\n",
    "    for ot in opp_types:\n",
    "        history['games_vs'][ot] += 1\n",
    "        population.record(ot, main_won)\n",
    "        if main_won: history['wins'][ot] += 1\n",
    "    \n",
    "    # Update\n",
    "    if (gn + 1) % CONFIG['update_every'] == 0:\n",
    "        m = main_agent.update(epochs=4, bs=128)\n",
    "        if m: history['policy_loss'].append(m['policy_loss'])\n",
    "    \n",
    "    # Checkpoint\n",
    "    if (gn + 1) % CONFIG['checkpoint_every'] == 0:\n",
    "        population.add_checkpoint(main_agent.policy, state_encoder, action_encoder, f'Ckpt_{gn+1}', 0.15)\n",
    "    \n",
    "    pbar.set_postfix({'r': f'{np.mean(history[\"rewards\"][-100:]):.1f}', 'pop': len(population.agents)})\n",
    "    if (gn + 1) % 200 == 0: population.print_stats()\n",
    "\n",
    "print('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "ax = axes[0, 0]\n",
    "r = history['rewards']\n",
    "ax.plot(r, alpha=0.3)\n",
    "if len(r) >= 50: ax.plot(range(49, len(r)), np.convolve(r, np.ones(50)/50, 'valid'), 'r', lw=2)\n",
    "ax.set_xlabel('Game'); ax.set_ylabel('Reward'); ax.set_title('Training Rewards'); ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "opps = list(history['games_vs'].keys())\n",
    "wrs = [history['wins'].get(o, 0) / max(history['games_vs'][o], 1) for o in opps]\n",
    "ax.bar(opps, wrs, color=plt.cm.Set2(range(len(opps))))\n",
    "ax.axhline(1/7, color='red', ls='--', label='Random (1/7)')\n",
    "ax.set_ylabel('Win Rate'); ax.set_title('Win Rate vs Opponents'); ax.legend(); ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "ax = axes[1, 0]\n",
    "sw = population.get_weights()\n",
    "ax.bar(list(sw.keys()), list(sw.values()), color=plt.cm.Set3(range(len(sw))))\n",
    "ax.set_ylabel('Weight'); ax.set_title('Sampling Weights'); ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "if history['policy_loss']: ax.plot(history['policy_loss'])\n",
    "ax.set_xlabel('Update'); ax.set_ylabel('Loss'); ax.set_title('Policy Loss'); ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pbt_results.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('SUMMARY')\n",
    "print('='*60)\n",
    "print(f'Games: {CONFIG[\"num_games\"]}')\n",
    "print(f'Population size: {len(population.agents)}')\n",
    "print(f'Avg reward (last 100): {np.mean(history[\"rewards\"][-100:]):.2f}')\n",
    "print('\\nWin rates:')\n",
    "for o in history['games_vs']:\n",
    "    wr = history['wins'].get(o, 0) / max(history['games_vs'][o], 1)\n",
    "    print(f'  vs {o}: {wr:.1%}')\n",
    "population.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_agent.save('pbt_agent.pt')\n",
    "with open('pbt_history.json', 'w') as f:\n",
    "    json.dump({'rewards': history['rewards'], 'lengths': history['lengths'], 'wins': dict(history['wins']), 'games_vs': dict(history['games_vs']), 'config': CONFIG}, f)\n",
    "print('Saved: pbt_agent.pt, pbt_history.json, pbt_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('pbt_agent.pt')\n",
    "files.download('pbt_history.json')\n",
    "files.download('pbt_results.png')\n",
    "print('Downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: RQ3 Answer\n",
    "\n",
    "**Question:** Does exposing the agent to a diverse population improve robustness?\n",
    "\n",
    "**Answer: YES**\n",
    "\n",
    "Population-Based Training improves robustness by:\n",
    "\n",
    "1. **Opponent Diversity**: Random + BC + Checkpoints provide varied strategies\n",
    "2. **PFSP Sampling**: Focuses on opponents we struggle against\n",
    "3. **Checkpoint Population**: Prevents forgetting past strategies\n",
    "4. **Measured Generalization**: Win rates vs different opponent types\n",
    "\n",
    "| Method | Diversity | Robustness |\n",
    "|--------|-----------|------------|\n",
    "| Self-Play | Low | Low |\n",
    "| HR-RL | Medium | Medium |\n",
    "| **PBT** | **High** | **High** |"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
