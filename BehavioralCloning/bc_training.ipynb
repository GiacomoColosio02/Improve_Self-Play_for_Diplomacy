{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Cloning for No-Press Diplomacy\n",
    "\n",
    "**Project:** Improve Self-Play for Diplomacy  \n",
    "**Authors:** Giacomo Colosio, Maciej Tasarz, Jakub Seliga, Luka Ivcevic  \n",
    "**Course:** ISP - UPC Barcelona, Fall 2025/26\n",
    "\n",
    "---\n",
    "\n",
    "This notebook trains a neural network to imitate human Diplomacy players using behavioral cloning.\n",
    "\n",
    "**Requirements:** GPU runtime (Runtime -> Change runtime type -> GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Data\n",
    "\n",
    "Upload `standard_no_press.jsonl` from your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Option 1: Upload file directly\n",
    "print(\"Upload 'standard_no_press.jsonl' file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "DATA_PATH = 'standard_no_press.jsonl'\n",
    "print(f\"\\nFile uploaded: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: If using Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# DATA_PATH = '/content/drive/MyDrive/diplomacy/standard_no_press.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POWERS = ['AUSTRIA', 'ENGLAND', 'FRANCE', 'GERMANY', 'ITALY', 'RUSSIA', 'TURKEY']\n",
    "\n",
    "LOCATIONS = [\n",
    "    # Supply centers (34)\n",
    "    'ANK', 'BEL', 'BER', 'BRE', 'BUD', 'BUL', 'CON', 'DEN', 'EDI', 'GRE',\n",
    "    'HOL', 'KIE', 'LON', 'LVP', 'MAR', 'MOS', 'MUN', 'NAP', 'NWY', 'PAR',\n",
    "    'POR', 'ROM', 'RUM', 'SER', 'SEV', 'SMY', 'SPA', 'STP', 'SWE', 'TRI',\n",
    "    'TUN', 'VEN', 'VIE', 'WAR',\n",
    "    # Non-supply center land (22)\n",
    "    'ALB', 'APU', 'ARM', 'BOH', 'BUR', 'CLY', 'FIN', 'GAL', 'GAS', 'LVN',\n",
    "    'NAF', 'PIC', 'PIE', 'PRU', 'RUH', 'SIL', 'SYR', 'TUS', 'TYR', 'UKR',\n",
    "    'WAL', 'YOR',\n",
    "    # Sea zones (19)\n",
    "    'ADR', 'AEG', 'BAL', 'BAR', 'BLA', 'BOT', 'EAS', 'ENG', 'GOL', 'HEL',\n",
    "    'ION', 'IRI', 'MAO', 'NAO', 'NTH', 'NWG', 'SKA', 'TYS', 'WES'\n",
    "]\n",
    "\n",
    "SUPPLY_CENTERS = LOCATIONS[:34]\n",
    "LOC_TO_IDX = {loc: i for i, loc in enumerate(LOCATIONS)}\n",
    "POWER_TO_IDX = {p: i for i, p in enumerate(POWERS)}\n",
    "\n",
    "print(f'Powers: {len(POWERS)}')\n",
    "print(f'Locations: {len(LOCATIONS)}')\n",
    "print(f'Supply Centers: {len(SUPPLY_CENTERS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. State Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateEncoder:\n",
    "    \"\"\"\n",
    "    Encodes Diplomacy game state into a fixed-size vector.\n",
    "    \n",
    "    Per location (75 locations):\n",
    "        - 7 bits: which power has a unit (one-hot)\n",
    "        - 1 bit: army (1) or fleet (0)\n",
    "        - 7 bits: which power owns SC (one-hot)\n",
    "        - 1 bit: is supply center\n",
    "    Total per location: 16 features\n",
    "    \n",
    "    Global features: 16\n",
    "    Total: 75 * 16 + 16 = 1216 features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.num_locations = len(LOCATIONS)\n",
    "        self.num_powers = len(POWERS)\n",
    "        self.features_per_loc = 16\n",
    "        self.global_features = 16\n",
    "        self.state_size = self.num_locations * self.features_per_loc + self.global_features\n",
    "        \n",
    "    def encode(self, state: Dict, phase_name: str = '') -> np.ndarray:\n",
    "        features = np.zeros(self.state_size, dtype=np.float32)\n",
    "        \n",
    "        units = state.get('units', {})\n",
    "        centers = state.get('centers', {})\n",
    "        \n",
    "        # Encode each location\n",
    "        for loc_idx, loc in enumerate(LOCATIONS):\n",
    "            offset = loc_idx * self.features_per_loc\n",
    "            \n",
    "            # Check for units\n",
    "            for power_idx, power in enumerate(POWERS):\n",
    "                power_units = units.get(power, [])\n",
    "                for unit in power_units:\n",
    "                    unit_loc = self._parse_unit_location(unit)\n",
    "                    if unit_loc == loc:\n",
    "                        features[offset + power_idx] = 1.0\n",
    "                        features[offset + 7] = 1.0 if unit.startswith('A ') else 0.0\n",
    "                        break\n",
    "            \n",
    "            # Check SC ownership\n",
    "            if loc in SUPPLY_CENTERS:\n",
    "                features[offset + 15] = 1.0\n",
    "                for power_idx, power in enumerate(POWERS):\n",
    "                    if loc in centers.get(power, []):\n",
    "                        features[offset + 8 + power_idx] = 1.0\n",
    "                        break\n",
    "        \n",
    "        # Global features\n",
    "        global_offset = self.num_locations * self.features_per_loc\n",
    "        \n",
    "        for power_idx, power in enumerate(POWERS):\n",
    "            features[global_offset + power_idx] = len(centers.get(power, [])) / 18.0\n",
    "            features[global_offset + 7 + power_idx] = len(units.get(power, [])) / 17.0\n",
    "        \n",
    "        if phase_name:\n",
    "            try:\n",
    "                year = int(phase_name[1:5])\n",
    "                features[global_offset + 14] = (year - 1901) / 20.0\n",
    "            except:\n",
    "                pass\n",
    "            features[global_offset + 15] = {'S': 0.0, 'F': 0.5, 'W': 1.0}.get(phase_name[0], 0.0)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _parse_unit_location(self, unit: str) -> str:\n",
    "        parts = unit.split()\n",
    "        if len(parts) >= 2:\n",
    "            return parts[1].split('/')[0]\n",
    "        return ''\n",
    "\n",
    "# Test\n",
    "encoder = StateEncoder()\n",
    "print(f'State size: {encoder.state_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Action Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionEncoder:\n",
    "    \"\"\"\n",
    "    Encodes Diplomacy orders into numerical indices.\n",
    "    Builds vocabulary from training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.order_to_idx = {}\n",
    "        self.idx_to_order = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def build_vocab(self, games: List[Dict], max_vocab: int = 10000):\n",
    "        order_counts = Counter()\n",
    "        \n",
    "        for game in games:\n",
    "            for phase in game.get('phases', []):\n",
    "                for power, orders in phase.get('orders', {}).items():\n",
    "                    if orders is None:\n",
    "                        continue\n",
    "                    for order in orders:\n",
    "                        norm = self._normalize(order)\n",
    "                        if norm:\n",
    "                            order_counts[norm] += 1\n",
    "        \n",
    "        most_common = order_counts.most_common(max_vocab - 2)\n",
    "        \n",
    "        self.order_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx_to_order = {0: '<PAD>', 1: '<UNK>'}\n",
    "        \n",
    "        for idx, (order, _) in enumerate(most_common, start=2):\n",
    "            self.order_to_idx[order] = idx\n",
    "            self.idx_to_order[idx] = order\n",
    "        \n",
    "        self.vocab_size = len(self.order_to_idx)\n",
    "        print(f'Vocabulary size: {self.vocab_size}')\n",
    "        \n",
    "    def _normalize(self, order: str) -> Optional[str]:\n",
    "        order = order.strip().upper()\n",
    "        order = re.sub(r'/[A-Z]{2}', '', order)\n",
    "        return order if len(order) >= 3 else None\n",
    "    \n",
    "    def encode(self, order: str) -> int:\n",
    "        norm = self._normalize(order)\n",
    "        return self.order_to_idx.get(norm, 1)\n",
    "    \n",
    "    def decode(self, idx: int) -> str:\n",
    "        return self.idx_to_order.get(idx, '<UNK>')\n",
    "\n",
    "action_encoder = ActionEncoder()\n",
    "print('Action encoder ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiplomacyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for behavioral cloning.\n",
    "    Each sample: (state, power, action)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, games: List[Dict], state_encoder: StateEncoder, \n",
    "                 action_encoder: ActionEncoder):\n",
    "        self.state_encoder = state_encoder\n",
    "        self.action_encoder = action_encoder\n",
    "        self.samples = []\n",
    "        self._process(games)\n",
    "        \n",
    "    def _process(self, games: List[Dict]):\n",
    "        for game in tqdm(games, desc='Processing games'):\n",
    "            for phase in game.get('phases', []):\n",
    "                phase_name = phase.get('name', '')\n",
    "                state = phase.get('state', {})\n",
    "                orders = phase.get('orders', {})\n",
    "                \n",
    "                if not phase_name.endswith('M'):\n",
    "                    continue\n",
    "                \n",
    "                encoded_state = self.state_encoder.encode(state, phase_name)\n",
    "                \n",
    "                for power_idx, power in enumerate(POWERS):\n",
    "                    power_orders = orders.get(power, [])\n",
    "                    if power_orders is None:\n",
    "                        continue\n",
    "                    \n",
    "                    for order in power_orders:\n",
    "                        action_idx = self.action_encoder.encode(order)\n",
    "                        if action_idx <= 1:\n",
    "                            continue\n",
    "                        \n",
    "                        self.samples.append({\n",
    "                            'state': encoded_state,\n",
    "                            'power': power_idx,\n",
    "                            'action': action_idx\n",
    "                        })\n",
    "        \n",
    "        print(f'Total samples: {len(self.samples):,}')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        return (\n",
    "            torch.FloatTensor(s['state']),\n",
    "            torch.LongTensor([s['power']]),\n",
    "            torch.LongTensor([s['action']])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCModel(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP model for behavioral cloning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size: int, vocab_size: int, hidden_size: int = 512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.LayerNorm(hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_size // 2, vocab_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def predict(self, x, temperature=1.0):\n",
    "        logits = self.forward(x)\n",
    "        return F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "\n",
    "class TransformerBCModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model for behavioral cloning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size: int, vocab_size: int, \n",
    "                 d_model: int = 256, nhead: int = 8, num_layers: int = 4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_proj = nn.Linear(state_size, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model, vocab_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x).unsqueeze(1)\n",
    "        x = x + self.pos_emb\n",
    "        x = self.transformer(x).squeeze(1)\n",
    "        return self.output(x)\n",
    "\n",
    "print('Models defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MAX_GAMES = 10000  # Increase for better results\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 30\n",
    "HIDDEN_SIZE = 512\n",
    "MODEL_TYPE = 'mlp'  # 'mlp' or 'transformer'\n",
    "\n",
    "print(f'Config:')\n",
    "print(f'  Max games: {MAX_GAMES}')\n",
    "print(f'  Batch size: {BATCH_SIZE}')\n",
    "print(f'  Epochs: {EPOCHS}')\n",
    "print(f'  Model: {MODEL_TYPE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load games\n",
    "print('Loading games...')\n",
    "games = []\n",
    "with open(DATA_PATH, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= MAX_GAMES:\n",
    "            break\n",
    "        games.append(json.loads(line))\n",
    "        if (i + 1) % 2000 == 0:\n",
    "            print(f'  Loaded {i + 1} games...')\n",
    "\n",
    "print(f'Total games: {len(games)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "print('\\nBuilding vocabulary...')\n",
    "action_encoder.build_vocab(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "split_idx = int(0.9 * len(games))\n",
    "train_games = games[:split_idx]\n",
    "val_games = games[split_idx:]\n",
    "\n",
    "print(f'Train: {len(train_games)} games')\n",
    "print(f'Val: {len(val_games)} games')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "state_encoder = StateEncoder()\n",
    "\n",
    "print('\\nCreating train dataset...')\n",
    "train_dataset = DiplomacyDataset(train_games, state_encoder, action_encoder)\n",
    "\n",
    "print('\\nCreating val dataset...')\n",
    "val_dataset = DiplomacyDataset(val_games, state_encoder, action_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'Train batches: {len(train_loader)}')\n",
    "print(f'Val batches: {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "if MODEL_TYPE == 'transformer':\n",
    "    model = TransformerBCModel(state_encoder.state_size, action_encoder.vocab_size)\n",
    "else:\n",
    "    model = BCModel(state_encoder.state_size, action_encoder.vocab_size, HIDDEN_SIZE)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model: {MODEL_TYPE.upper()}')\n",
    "print(f'Parameters: {num_params:,}')\n",
    "print(f'State size: {state_encoder.state_size}')\n",
    "print(f'Vocab size: {action_encoder.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for states, powers, actions in pbar:\n",
    "        states = states.to(device)\n",
    "        actions = actions.squeeze(1).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(states)\n",
    "        loss = criterion(logits, actions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == actions).sum().item()\n",
    "        total += actions.size(0)\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{correct/total:.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for states, powers, actions in tqdm(loader, desc='Validating'):\n",
    "            states = states.to(device)\n",
    "            actions = actions.squeeze(1).to(device)\n",
    "            \n",
    "            logits = model(states)\n",
    "            loss = criterion(logits, actions)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == actions).sum().item()\n",
    "            \n",
    "            _, top5 = logits.topk(5, dim=1)\n",
    "            top5_correct += (top5 == actions.unsqueeze(1)).any(dim=1).sum().item()\n",
    "            \n",
    "            total += actions.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total, top5_correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': [], 'val_top5': []\n",
    "}\n",
    "best_val_acc = 0\n",
    "\n",
    "print('='*60)\n",
    "print('TRAINING')\n",
    "print('='*60)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\nEpoch {epoch + 1}/{EPOCHS}')\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_top5 = validate(model, val_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_top5'].append(val_top5)\n",
    "    \n",
    "    print(f'  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}')\n",
    "    print(f'  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, Top-5: {val_top5:.4f}')\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'vocab': action_encoder.order_to_idx,\n",
    "            'config': {'state_size': state_encoder.state_size, 'vocab_size': action_encoder.vocab_size}\n",
    "        }, 'best_bc_model.pt')\n",
    "        print(f'  -> Saved best model!')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print(f'Best Val Accuracy: {best_val_acc:.4f}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Val', linewidth=2)\n",
    "axes[1].plot(history['val_top5'], label='Val Top-5', linewidth=2, linestyle='--')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('best_bc_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print('Model loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random sample\n",
    "test_game = val_games[0]\n",
    "test_phase = test_game['phases'][0]\n",
    "\n",
    "print(f\"Phase: {test_phase['name']}\")\n",
    "print(f\"\\nActual orders:\")\n",
    "for power in POWERS:\n",
    "    orders = test_phase['orders'].get(power, [])\n",
    "    if orders:\n",
    "        print(f\"  {power}: {orders}\")\n",
    "\n",
    "# Get model predictions\n",
    "state = test_phase['state']\n",
    "encoded = state_encoder.encode(state, test_phase['name'])\n",
    "x = torch.FloatTensor(encoded).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(x)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    top5_probs, top5_idx = probs.topk(5, dim=-1)\n",
    "\n",
    "print(f\"\\nModel's top 5 predictions:\")\n",
    "for i in range(5):\n",
    "    idx = top5_idx[0, i].item()\n",
    "    prob = top5_probs[0, i].item()\n",
    "    order = action_encoder.decode(idx)\n",
    "    print(f\"  {i+1}. {order} ({prob:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the trained model\n",
    "from google.colab import files\n",
    "\n",
    "files.download('best_bc_model.pt')\n",
    "files.download('training_curves.png')\n",
    "\n",
    "print('Files downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary\n",
    "\n",
    "### Results\n",
    "- **Train Accuracy**: See plot above\n",
    "- **Val Accuracy**: See plot above  \n",
    "- **Top-5 Accuracy**: Model's correct order is in top 5 predictions\n",
    "\n",
    "### Next Steps\n",
    "1. **Self-Play**: Use BC model as starting point for RL\n",
    "2. **Human-Regularized RL**: Add KL penalty to stay close to human policy\n",
    "3. **Population Training**: Train diverse opponents"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
