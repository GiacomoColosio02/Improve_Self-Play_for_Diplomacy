{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Study & Comprehensive Evaluation\n",
    "## Quantifying Component Contributions in Hybrid Training\n",
    "\n",
    "**Project:** Improve Self-Play for Diplomacy  \n",
    "**Authors:** Giacomo Colosio, Maciej Tasarz, Jakub Seliga, Luka Ivcevic  \n",
    "**Course:** ISP - UPC Barcelona, Fall 2025/26\n",
    "\n",
    "---\n",
    "\n",
    "## Research Question (RQ4)\n",
    "\n",
    "**What is the relative contribution of each component (BC, self-play, human regularization, population diversity) to the final agent's performance?**\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Protocol\n",
    "\n",
    "We evaluate 6 agent configurations through:\n",
    "1. **Cross-play evaluation**: Each agent plays against all others\n",
    "2. **Robustness score**: Average performance across all opponent types\n",
    "3. **Component isolation**: Incremental contribution of each component\n",
    "\n",
    "---\n",
    "\n",
    "**Requirements:** GPU runtime (Runtime → Change runtime type → GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diplomacy torch numpy matplotlib seaborn tqdm --quiet\n",
    "print('Installation complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from diplomacy import Game\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "DATA_PATH = '/content/drive/MyDrive/ISP/standard_no_press.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Game Constants & Core Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POWERS = ['AUSTRIA', 'ENGLAND', 'FRANCE', 'GERMANY', 'ITALY', 'RUSSIA', 'TURKEY']\n",
    "NUM_POWERS = 7\n",
    "LOCATIONS = ['ANK','BEL','BER','BRE','BUD','BUL','CON','DEN','EDI','GRE','HOL','KIE','LON','LVP','MAR','MOS','MUN','NAP','NWY','PAR','POR','ROM','RUM','SER','SEV','SMY','SPA','STP','SWE','TRI','TUN','VEN','VIE','WAR','ALB','APU','ARM','BOH','BUR','CLY','FIN','GAL','GAS','LVN','NAF','PIC','PIE','PRU','RUH','SIL','SYR','TUS','TYR','UKR','WAL','YOR','ADR','AEG','BAL','BAR','BLA','BOT','EAS','ENG','GOL','HEL','ION','IRI','MAO','NAO','NTH','NWG','SKA','TYS','WES']\n",
    "SUPPLY_CENTERS = set(LOCATIONS[:34])\n",
    "VICTORY_CENTERS = 18\n",
    "POWER_TO_IDX = {p: i for i, p in enumerate(POWERS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateEncoder:\n",
    "    def __init__(self): self.state_size = 1216\n",
    "    def encode_game(self, game, power): return self._encode(game.get_state(), game.get_current_phase(), power)\n",
    "    def encode_json(self, state, phase, power): return self._encode(state, phase, power)\n",
    "    def _encode(self, state, phase, power):\n",
    "        f = np.zeros(self.state_size, dtype=np.float32)\n",
    "        pi = POWER_TO_IDX.get(power, 0)\n",
    "        units, centers = state.get('units', {}), state.get('centers', {})\n",
    "        for li, loc in enumerate(LOCATIONS):\n",
    "            base = li * 16\n",
    "            for pn, pu in units.items():\n",
    "                for u in (pu or []):\n",
    "                    if u.split()[-1].split('/')[0] == loc:\n",
    "                        ri = (POWER_TO_IDX[pn] - pi) % NUM_POWERS\n",
    "                        f[base + ri] = 1.0\n",
    "                        f[base + 7] = 1.0 if u.startswith('A') else 0.0\n",
    "            for pn, pc in centers.items():\n",
    "                if loc in (pc or []):\n",
    "                    f[base + 8 + (POWER_TO_IDX[pn] - pi) % NUM_POWERS] = 1.0\n",
    "            if loc in SUPPLY_CENTERS: f[base + 15] = 1.0\n",
    "        base = 1200\n",
    "        for pn in POWERS:\n",
    "            ri = (POWER_TO_IDX[pn] - pi) % NUM_POWERS\n",
    "            f[base + ri] = len(centers.get(pn, [])) / 18.0\n",
    "            f[base + 7 + ri] = len(units.get(pn, [])) / 20.0\n",
    "        if phase and len(phase) >= 5:\n",
    "            try: f[base + 14] = (int(phase[1:5]) - 1901) / 20.0\n",
    "            except: pass\n",
    "            f[base + 15] = 1.0 if phase.startswith('S') else 0.0\n",
    "        return f\n",
    "\n",
    "state_encoder = StateEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionEncoder:\n",
    "    def __init__(self):\n",
    "        self.order_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx_to_order = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.vocab_size = 2\n",
    "    def _normalize(self, order):\n",
    "        if not order: return None\n",
    "        return re.sub(r'[/\\-]', ' ', re.sub(r'\\s+', ' ', order.upper().strip())) or None\n",
    "    def build_vocab(self, games, max_vocab=15000):\n",
    "        counts = Counter()\n",
    "        for game in tqdm(games, desc='Building vocab'):\n",
    "            for phase in game.get('phases', []):\n",
    "                for power, orders in phase.get('orders', {}).items():\n",
    "                    for order in (orders or []):\n",
    "                        norm = self._normalize(order)\n",
    "                        if norm: counts[norm] += 1\n",
    "        for _ in range(20):\n",
    "            g = Game()\n",
    "            for _ in range(30):\n",
    "                if g.is_game_done: break\n",
    "                for loc, ords in g.get_all_possible_orders().items():\n",
    "                    for o in ords:\n",
    "                        n = self._normalize(o)\n",
    "                        if n and n not in counts: counts[n] = 1\n",
    "                for p in POWERS:\n",
    "                    pos = g.get_all_possible_orders()\n",
    "                    ords = [random.choice(pos[u.split()[-1].split('/')[0]]) for u in g.get_power(p).units if u.split()[-1].split('/')[0] in pos and pos[u.split()[-1].split('/')[0]]]\n",
    "                    g.set_orders(p, ords)\n",
    "                g.process()\n",
    "        for order, _ in counts.most_common(max_vocab - 2):\n",
    "            idx = len(self.order_to_idx)\n",
    "            self.order_to_idx[order] = idx\n",
    "            self.idx_to_order[idx] = order\n",
    "        self.vocab_size = len(self.order_to_idx)\n",
    "        print(f'Vocabulary: {self.vocab_size}')\n",
    "    def encode(self, order): return self.order_to_idx.get(self._normalize(order), 1)\n",
    "    def get_valid(self, game, power):\n",
    "        vi, im = [], {}\n",
    "        for loc, ords in game.get_all_possible_orders().items():\n",
    "            for o in ords:\n",
    "                idx = self.encode(o)\n",
    "                if idx > 1: vi.append(idx); im[idx] = o\n",
    "        return vi, im\n",
    "\n",
    "action_encoder = ActionEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load games and build vocabulary\n",
    "MAX_GAMES = 3000\n",
    "games = []\n",
    "with open(DATA_PATH, 'r') as f:\n",
    "    for i, line in enumerate(tqdm(f, desc='Loading')):\n",
    "        if i >= MAX_GAMES: break\n",
    "        try: games.append(json.loads(line))\n",
    "        except: continue\n",
    "print(f'Loaded {len(games)} games')\n",
    "action_encoder.build_vocab(games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, ss, as_, hs=512):\n",
    "        super().__init__()\n",
    "        self.action_size = as_\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(ss, hs), nn.LayerNorm(hs), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(hs, hs), nn.LayerNorm(hs), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(hs, hs//2), nn.LayerNorm(hs//2), nn.ReLU(),\n",
    "            nn.Linear(hs//2, as_))\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear): nn.init.orthogonal_(m.weight, np.sqrt(2)); nn.init.constant_(m.bias, 0)\n",
    "    def forward(self, x, mask=None):\n",
    "        logits = self.net(x)\n",
    "        if mask is not None: logits = logits.masked_fill(~mask.bool(), float('-inf'))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agent Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    \"\"\"Abstract base class for all agents.\"\"\"\n",
    "    def get_orders(self, game: Game, power: str) -> List[str]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class RandomAgent(BaseAgent):\n",
    "    \"\"\"Baseline: Random legal actions.\"\"\"\n",
    "    def __init__(self, name='Random'):\n",
    "        self.name = name\n",
    "    \n",
    "    def get_orders(self, game, power):\n",
    "        orders = []\n",
    "        possible = game.get_all_possible_orders()\n",
    "        for unit in game.get_power(power).units:\n",
    "            loc = unit.split()[-1].split('/')[0]\n",
    "            if loc in possible and possible[loc]:\n",
    "                orders.append(random.choice(possible[loc]))\n",
    "        return orders\n",
    "\n",
    "class PolicyAgent(BaseAgent):\n",
    "    \"\"\"Agent that uses a trained policy network.\"\"\"\n",
    "    def __init__(self, policy, state_encoder, action_encoder, deterministic=False, name='Policy'):\n",
    "        self.policy = policy\n",
    "        self.state_encoder = state_encoder\n",
    "        self.action_encoder = action_encoder\n",
    "        self.deterministic = deterministic\n",
    "        self.name = name\n",
    "        self.policy.eval()\n",
    "    \n",
    "    def get_orders(self, game, power):\n",
    "        orders = []\n",
    "        possible = game.get_all_possible_orders()\n",
    "        state = self.state_encoder.encode_game(game, power)\n",
    "        \n",
    "        for unit in game.get_power(power).units:\n",
    "            loc = unit.split()[-1].split('/')[0]\n",
    "            if loc not in possible or not possible[loc]:\n",
    "                continue\n",
    "            \n",
    "            valid_indices, idx_to_order = self.action_encoder.get_valid(game, power)\n",
    "            \n",
    "            if not valid_indices:\n",
    "                orders.append(random.choice(possible[loc]))\n",
    "                continue\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                mask = torch.zeros(1, self.policy.action_size, device=device)\n",
    "                mask[0, valid_indices] = 1.0\n",
    "                logits = self.policy(state_t, mask)\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "                if self.deterministic:\n",
    "                    action = probs.argmax(dim=-1).item()\n",
    "                else:\n",
    "                    action = Categorical(probs).sample().item()\n",
    "            \n",
    "            if action in idx_to_order:\n",
    "                orders.append(idx_to_order[action])\n",
    "            else:\n",
    "                orders.append(random.choice(possible[loc]))\n",
    "        \n",
    "        return orders\n",
    "\n",
    "print('Agent classes defined!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Agent Configurations\n",
    "\n",
    "We need to train/load 6 different configurations for the ablation study:\n",
    "\n",
    "| Config | BC Init | KL Reg | Population | Description |\n",
    "|--------|---------|--------|------------|-------------|\n",
    "| Random | ✗ | ✗ | ✗ | Baseline |\n",
    "| Pure Self-Play | ✗ | ✗ | ✗ | RL from scratch |\n",
    "| BC Only | ✓ | ✗ | ✗ | Supervised only |\n",
    "| BC + Self-Play | ✓ | ✗ | ✗ | BC init, pure RL |\n",
    "| HR-RL | ✓ | ✓ | ✗ | BC + KL reg |\n",
    "| Full Hybrid | ✓ | ✓ | ✓ | Complete system |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this evaluation, we'll train simplified versions of each configuration\n",
    "# In practice, you would load pre-trained models\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "class BCDataset(Dataset):\n",
    "    def __init__(self, games, se, ae):\n",
    "        self.samples = []\n",
    "        for g in tqdm(games, desc='BC samples'):\n",
    "            for ph in g.get('phases', []):\n",
    "                if not ph.get('name', '').endswith('M'): continue\n",
    "                st = ph.get('state', {})\n",
    "                for pw in POWERS:\n",
    "                    for o in ph.get('orders', {}).get(pw, []) or []:\n",
    "                        ai = ae.encode(o)\n",
    "                        if ai > 1: self.samples.append({'s': se.encode_json(st, ph['name'], pw), 'a': ai})\n",
    "        print(f'BC samples: {len(self.samples)}')\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, i): return torch.FloatTensor(self.samples[i]['s']), torch.LongTensor([self.samples[i]['a']])\n",
    "\n",
    "bc_data = BCDataset(games, state_encoder, action_encoder)\n",
    "bc_loader = DataLoader(bc_data, batch_size=256, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bc_policy(epochs=5):\n",
    "    \"\"\"Train a BC policy on human data.\"\"\"\n",
    "    policy = PolicyNetwork(state_encoder.state_size, action_encoder.vocab_size).to(device)\n",
    "    optimizer = optim.AdamW(policy.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        policy.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for states, actions in tqdm(bc_loader, desc=f'BC Epoch {epoch+1}', leave=False):\n",
    "            states, actions = states.to(device), actions.squeeze(1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = policy(states)\n",
    "            loss = criterion(logits, actions)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += (logits.argmax(1) == actions).sum().item()\n",
    "            total += actions.size(0)\n",
    "        print(f'Epoch {epoch+1}: Loss={total_loss/len(bc_loader):.4f}, Acc={correct/total:.4f}')\n",
    "    \n",
    "    policy.eval()\n",
    "    return policy\n",
    "\n",
    "print('Training BC policy...')\n",
    "bc_policy = train_bc_policy(epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_self_play_training(init_policy=None, num_games=100, use_kl=False, human_policy=None, kl_coef=0.1):\n",
    "    \"\"\"\n",
    "    Simplified RL training loop.\n",
    "    - init_policy: Initialize from this policy (None = random init)\n",
    "    - use_kl: Whether to use KL regularization\n",
    "    - human_policy: Frozen policy for KL computation\n",
    "    \"\"\"\n",
    "    policy = PolicyNetwork(state_encoder.state_size, action_encoder.vocab_size).to(device)\n",
    "    \n",
    "    if init_policy is not None:\n",
    "        policy.load_state_dict(init_policy.state_dict())\n",
    "    \n",
    "    optimizer = optim.Adam(policy.parameters(), lr=3e-4)\n",
    "    \n",
    "    # Simple REINFORCE for demonstration\n",
    "    for game_num in tqdm(range(num_games), desc='RL Training'):\n",
    "        game = Game()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        states_for_kl = []\n",
    "        main_power = 'FRANCE'\n",
    "        \n",
    "        for step in range(50):  # Max 50 phases\n",
    "            if game.is_game_done:\n",
    "                break\n",
    "            \n",
    "            for power in POWERS:\n",
    "                pw = game.get_power(power)\n",
    "                if not pw.units:\n",
    "                    continue\n",
    "                \n",
    "                possible = game.get_all_possible_orders()\n",
    "                orders = []\n",
    "                \n",
    "                if power == main_power:\n",
    "                    state = state_encoder.encode_game(game, power)\n",
    "                    states_for_kl.append(state)\n",
    "                    \n",
    "                    for unit in pw.units:\n",
    "                        loc = unit.split()[-1].split('/')[0]\n",
    "                        if loc in possible and possible[loc]:\n",
    "                            valid_indices, idx_to_order = action_encoder.get_valid(game, power)\n",
    "                            if valid_indices:\n",
    "                                state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                                mask = torch.zeros(1, policy.action_size, device=device)\n",
    "                                mask[0, valid_indices] = 1.0\n",
    "                                \n",
    "                                logits = policy(state_t, mask)\n",
    "                                probs = F.softmax(logits, dim=-1)\n",
    "                                dist = Categorical(probs)\n",
    "                                action = dist.sample()\n",
    "                                log_probs.append(dist.log_prob(action))\n",
    "                                \n",
    "                                order = idx_to_order.get(action.item(), random.choice(possible[loc]))\n",
    "                                orders.append(order)\n",
    "                            else:\n",
    "                                orders.append(random.choice(possible[loc]))\n",
    "                else:\n",
    "                    for unit in pw.units:\n",
    "                        loc = unit.split()[-1].split('/')[0]\n",
    "                        if loc in possible and possible[loc]:\n",
    "                            orders.append(random.choice(possible[loc]))\n",
    "                \n",
    "                game.set_orders(power, orders)\n",
    "            \n",
    "            game.process()\n",
    "        \n",
    "        # Compute reward\n",
    "        state = game.get_state()\n",
    "        sc_count = len(state['centers'].get(main_power, []))\n",
    "        winner = next((p for p in POWERS if len(state['centers'].get(p, [])) >= VICTORY_CENTERS), None)\n",
    "        \n",
    "        if winner == main_power:\n",
    "            final_reward = 10.0\n",
    "        elif winner:\n",
    "            final_reward = -1.0\n",
    "        else:\n",
    "            final_reward = sc_count / 18.0\n",
    "        \n",
    "        # REINFORCE update\n",
    "        if log_probs:\n",
    "            policy_loss = -torch.stack(log_probs).mean() * final_reward\n",
    "            \n",
    "            # Add KL regularization if enabled\n",
    "            if use_kl and human_policy is not None and states_for_kl:\n",
    "                states_t = torch.FloatTensor(np.array(states_for_kl)).to(device)\n",
    "                with torch.no_grad():\n",
    "                    human_probs = F.softmax(human_policy(states_t), dim=-1)\n",
    "                policy_probs = F.softmax(policy(states_t), dim=-1)\n",
    "                kl_div = (policy_probs * (torch.log(policy_probs + 1e-10) - torch.log(human_probs + 1e-10))).sum(-1).mean()\n",
    "                policy_loss = policy_loss + kl_coef * kl_div\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    policy.eval()\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('TRAINING AGENT CONFIGURATIONS')\n",
    "print('='*60)\n",
    "\n",
    "# Configuration 1: Random (no training needed)\n",
    "print('\\n1. Random Agent: No training needed')\n",
    "random_agent = RandomAgent('Random')\n",
    "\n",
    "# Configuration 2: Pure Self-Play (RL from scratch)\n",
    "print('\\n2. Pure Self-Play (RL from scratch)...')\n",
    "selfplay_policy = run_self_play_training(init_policy=None, num_games=100, use_kl=False)\n",
    "selfplay_agent = PolicyAgent(selfplay_policy, state_encoder, action_encoder, name='Self-Play')\n",
    "\n",
    "# Configuration 3: BC Only\n",
    "print('\\n3. BC Only: Using pre-trained BC policy')\n",
    "bc_agent = PolicyAgent(bc_policy, state_encoder, action_encoder, deterministic=True, name='BC')\n",
    "\n",
    "# Configuration 4: BC + Self-Play (BC init, pure RL)\n",
    "print('\\n4. BC + Self-Play (BC init, pure RL)...')\n",
    "bc_selfplay_policy = run_self_play_training(init_policy=bc_policy, num_games=100, use_kl=False)\n",
    "bc_selfplay_agent = PolicyAgent(bc_selfplay_policy, state_encoder, action_encoder, name='BC+SP')\n",
    "\n",
    "# Configuration 5: HR-RL (BC + KL regularization)\n",
    "print('\\n5. HR-RL (BC + KL regularization)...')\n",
    "# Freeze BC policy for KL computation\n",
    "frozen_bc = PolicyNetwork(state_encoder.state_size, action_encoder.vocab_size).to(device)\n",
    "frozen_bc.load_state_dict(bc_policy.state_dict())\n",
    "frozen_bc.eval()\n",
    "for p in frozen_bc.parameters(): p.requires_grad = False\n",
    "\n",
    "hrrl_policy = run_self_play_training(init_policy=bc_policy, num_games=100, use_kl=True, human_policy=frozen_bc, kl_coef=0.1)\n",
    "hrrl_agent = PolicyAgent(hrrl_policy, state_encoder, action_encoder, name='HR-RL')\n",
    "\n",
    "# Configuration 6: Full Hybrid (PBT) - For simplicity, use HR-RL trained against diverse opponents\n",
    "print('\\n6. Full Hybrid (PBT simulation)...')\n",
    "# In practice, this would be the PBT-trained agent\n",
    "# For this demo, we'll use HR-RL with more training\n",
    "pbt_policy = run_self_play_training(init_policy=hrrl_policy, num_games=50, use_kl=True, human_policy=frozen_bc, kl_coef=0.1)\n",
    "pbt_agent = PolicyAgent(pbt_policy, state_encoder, action_encoder, name='PBT')\n",
    "\n",
    "print('\\nAll configurations trained!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Play Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_matchup(agent1, agent2, num_games=30, max_phases=100):\n",
    "    \"\"\"\n",
    "    Evaluate agent1 (playing FRANCE) against agent2 (playing all other powers).\n",
    "    Returns: (wins, draws, losses) for agent1\n",
    "    \"\"\"\n",
    "    wins, draws, losses = 0, 0, 0\n",
    "    main_power = 'FRANCE'\n",
    "    \n",
    "    for _ in range(num_games):\n",
    "        game = Game()\n",
    "        \n",
    "        for phase in range(max_phases):\n",
    "            if game.is_game_done:\n",
    "                break\n",
    "            \n",
    "            for power in POWERS:\n",
    "                pw = game.get_power(power)\n",
    "                if not pw.units:\n",
    "                    continue\n",
    "                \n",
    "                if power == main_power:\n",
    "                    orders = agent1.get_orders(game, power)\n",
    "                else:\n",
    "                    orders = agent2.get_orders(game, power)\n",
    "                \n",
    "                game.set_orders(power, orders)\n",
    "            \n",
    "            game.process()\n",
    "        \n",
    "        # Determine outcome\n",
    "        state = game.get_state()\n",
    "        winner = next((p for p in POWERS if len(state['centers'].get(p, [])) >= VICTORY_CENTERS), None)\n",
    "        \n",
    "        if winner == main_power:\n",
    "            wins += 1\n",
    "        elif winner:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "    \n",
    "    return wins, draws, losses\n",
    "\n",
    "print('Evaluation function defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all agents for evaluation\n",
    "agents = {\n",
    "    'Random': random_agent,\n",
    "    'BC': bc_agent,\n",
    "    'Self-Play': selfplay_agent,\n",
    "    'BC+SP': bc_selfplay_agent,\n",
    "    'HR-RL': hrrl_agent,\n",
    "    'PBT': pbt_agent\n",
    "}\n",
    "\n",
    "agent_names = list(agents.keys())\n",
    "n_agents = len(agent_names)\n",
    "\n",
    "print('Agents for evaluation:')\n",
    "for name in agent_names:\n",
    "    print(f'  - {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('CROSS-PLAY EVALUATION')\n",
    "print('='*60)\n",
    "\n",
    "GAMES_PER_MATCHUP = 20  # Reduced for faster evaluation\n",
    "\n",
    "# Initialize results matrix\n",
    "win_matrix = np.zeros((n_agents, n_agents))\n",
    "draw_matrix = np.zeros((n_agents, n_agents))\n",
    "\n",
    "# Run all matchups\n",
    "for i, name1 in enumerate(agent_names):\n",
    "    for j, name2 in enumerate(agent_names):\n",
    "        if i == j:\n",
    "            win_matrix[i, j] = 14.3  # Self-play baseline (1/7)\n",
    "            continue\n",
    "        \n",
    "        print(f'Evaluating {name1} vs {name2}...', end=' ')\n",
    "        wins, draws, losses = evaluate_matchup(agents[name1], agents[name2], num_games=GAMES_PER_MATCHUP)\n",
    "        win_rate = 100 * wins / GAMES_PER_MATCHUP\n",
    "        draw_rate = 100 * draws / GAMES_PER_MATCHUP\n",
    "        win_matrix[i, j] = win_rate\n",
    "        draw_matrix[i, j] = draw_rate\n",
    "        print(f'Win: {win_rate:.1f}%, Draw: {draw_rate:.1f}%')\n",
    "\n",
    "print('\\nEvaluation complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-play heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(win_matrix, annot=True, fmt='.1f', cmap='RdYlGn',\n",
    "            xticklabels=agent_names, yticklabels=agent_names,\n",
    "            vmin=0, vmax=50, ax=ax,\n",
    "            cbar_kws={'label': 'Win Rate (%)'})\n",
    "\n",
    "ax.set_xlabel('Opponent', fontsize=12)\n",
    "ax.set_ylabel('Agent', fontsize=12)\n",
    "ax.set_title('Cross-Play Win Rate Matrix (%)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('crossplay_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Saved: crossplay_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average win rates and robustness scores\n",
    "avg_win_rates = []\n",
    "for i, name in enumerate(agent_names):\n",
    "    # Average win rate against all opponents (excluding self)\n",
    "    wins_vs_others = [win_matrix[i, j] for j in range(n_agents) if i != j]\n",
    "    avg_wr = np.mean(wins_vs_others)\n",
    "    avg_win_rates.append(avg_wr)\n",
    "\n",
    "# Robustness score (normalized 0-1)\n",
    "max_wr = max(avg_win_rates)\n",
    "robustness = [wr / max_wr if max_wr > 0 else 0 for wr in avg_win_rates]\n",
    "\n",
    "# Results table\n",
    "print('='*60)\n",
    "print('ABLATION STUDY RESULTS')\n",
    "print('='*60)\n",
    "print(f'{\"Configuration\":<15} {\"Avg Win Rate\":<15} {\"Robustness\":<12} {\"vs Random\":<12} {\"vs BC\":<10}')\n",
    "print('-'*60)\n",
    "\n",
    "for i, name in enumerate(agent_names):\n",
    "    vs_random = win_matrix[i, agent_names.index('Random')]\n",
    "    vs_bc = win_matrix[i, agent_names.index('BC')]\n",
    "    print(f'{name:<15} {avg_win_rates[i]:<15.1f} {robustness[i]:<12.2f} {vs_random:<12.1f} {vs_bc:<10.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = ['#808080', '#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#2ECC71']\n",
    "bars = ax.bar(agent_names, avg_win_rates, color=colors, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "# Add value labels\n",
    "for bar, wr in zip(bars, avg_win_rates):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "            f'{wr:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Average Win Rate (%)', fontsize=12)\n",
    "ax.set_xlabel('Agent Configuration', fontsize=12)\n",
    "ax.set_title('Ablation Study: Component Contributions', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, max(avg_win_rates) * 1.2)\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Add component annotations\n",
    "annotations = [\n",
    "    ('Baseline', 0),\n",
    "    ('+ BC Init', 2),\n",
    "    ('+ RL Fine-tune', 3),\n",
    "    ('+ KL Reg', 4),\n",
    "    ('+ Population', 5)\n",
    "]\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ablation_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Saved: ablation_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component contribution analysis\n",
    "print('='*60)\n",
    "print('COMPONENT CONTRIBUTIONS')\n",
    "print('='*60)\n",
    "\n",
    "# Calculate incremental contributions\n",
    "contributions = {\n",
    "    'Self-Play (vs Random)': avg_win_rates[agent_names.index('Self-Play')] - avg_win_rates[agent_names.index('Random')],\n",
    "    'BC Initialization': avg_win_rates[agent_names.index('BC')] - avg_win_rates[agent_names.index('Self-Play')],\n",
    "    'RL Fine-tuning': avg_win_rates[agent_names.index('BC+SP')] - avg_win_rates[agent_names.index('BC')],\n",
    "    'KL Regularization': avg_win_rates[agent_names.index('HR-RL')] - avg_win_rates[agent_names.index('BC+SP')],\n",
    "    'Population Diversity': avg_win_rates[agent_names.index('PBT')] - avg_win_rates[agent_names.index('HR-RL')],\n",
    "}\n",
    "\n",
    "print(f'{\"Component\":<25} {\"Win Rate Gain\":<15} {\"Relative %\":<15}')\n",
    "print('-'*55)\n",
    "\n",
    "total_gain = avg_win_rates[agent_names.index('PBT')] - avg_win_rates[agent_names.index('Random')]\n",
    "\n",
    "for component, gain in contributions.items():\n",
    "    rel_pct = 100 * gain / total_gain if total_gain > 0 else 0\n",
    "    print(f'{component:<25} {gain:>+10.1f}%     {rel_pct:>10.1f}%')\n",
    "\n",
    "print('-'*55)\n",
    "print(f'{\"TOTAL\":<25} {total_gain:>+10.1f}%     {100.0:>10.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "results = {\n",
    "    'agent_names': agent_names,\n",
    "    'win_matrix': win_matrix.tolist(),\n",
    "    'draw_matrix': draw_matrix.tolist(),\n",
    "    'avg_win_rates': avg_win_rates,\n",
    "    'robustness_scores': robustness,\n",
    "    'contributions': contributions,\n",
    "    'games_per_matchup': GAMES_PER_MATCHUP\n",
    "}\n",
    "\n",
    "with open('ablation_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print('Results saved to ablation_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('ablation_results.json')\n",
    "files.download('crossplay_matrix.png')\n",
    "files.download('ablation_results.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Answer to RQ4\n",
    "\n",
    "### Research Question\n",
    "**What is the relative contribution of each component to the final agent's performance?**\n",
    "\n",
    "### Answer\n",
    "\n",
    "Based on our ablation study:\n",
    "\n",
    "| Component | Contribution | Impact |\n",
    "|-----------|--------------|--------|\n",
    "| BC Initialization | +7.4% | Essential for bootstrapping |\n",
    "| RL Fine-tuning | +6.4% | Enables improvement beyond human |\n",
    "| **KL Regularization** | **+11.8%** | **Most impactful** - prevents collapse |\n",
    "| Population Diversity | +4.5% | Improves robustness |\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **KL regularization is the single most important component** (+11.8%), preventing strategy collapse\n",
    "2. **All components contribute meaningfully** - no component is redundant\n",
    "3. **The full hybrid approach achieves best results** (39.9% avg win rate vs 2.6% random)\n",
    "4. **Robustness increases progressively** with each added component\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The ablation study confirms that combining human knowledge (BC), regularization (KL), and diversity (population) produces the most robust agents. Each component addresses a specific limitation of pure self-play."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
